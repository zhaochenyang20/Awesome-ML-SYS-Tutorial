# Tiny-LLM Day 1

é…ç¯å¢ƒç¨å¾®æŠ˜è…¾äº†ä¸€ä¼šå„¿ï¼Œå› ä¸ºæˆ‘æ–°çš„ mac å‘ç°è¦é…åˆ mlx éœ€è¦å®‰è£… XCoderï¼Œæäº†åŠå¤©ç»•ä¸è¿‡å»ï¼Œé‚å®‰è£…ã€‚ç„¶ååªèƒ½ç”¨ python 3.10 åˆ° 3.12 çš„ç‰ˆæœ¬ã€‚

æ•´ä½“ä½“éªŒæ„Ÿè§‰æ–‡æ¡£å†™çš„æ¯”è¾ƒ fly bitchï¼Œä¸çŸ¥æ˜¯ä¸æ˜¯è¿Ÿå…ˆç”ŸåŠ©æ•™åšå¤šäº†çš„ç¼˜æ•… ğŸ¤£

è¿™é‡Œå®ç°ä¸¤ä¸ªåŠŸèƒ½ï¼Œä¸€å…±å°±ä¸‰ä¸ªå‡½æ•°ï¼Œç®€å•æ¢³ç†ä¸‹ï¼š

## Scaled Dot-Product Attention

```python
def scaled_dot_product_attention_simple(
    query: mx.array,
    key: mx.array,
    value: mx.array,
    scale: float | None = None,
    mask: mx.array | None = None,
) -> mx.array:
    d_k = key.shape[-1]
    scale = scale or 1.0 / sqrt(d_k)
    
    # ä½¿ç”¨ä¹˜æ³•è€Œä¸æ˜¯é™¤æ³•
    score = (query @ mx.swapaxes(key, -2, -1)) * scale  # æ³¨æ„è¿™é‡Œç”¨ * è€Œä¸æ˜¯ /
    if mask is not None:
        score += mask
    
    attn = softmax(score, axis=-1) @ value
    return attn
```

è¿™ä¸ªå‡½æ•°è‡ªç„¶æ˜¯æ¯”è¾ƒç®€å•çš„ï¼Œå¿«é€Ÿè¿‡ä¸€ä¸‹ï¼š

1. æˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨çš„ attn å…¬å¼æ˜¯ $$ \text{attn} = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}) V $$ã€‚å…¶ä¸­ï¼Œç»´åº¦ä¿¡æ¯å€¼å¾—æ ¼å¤–æ³¨æ„ã€‚å…¬å¼ä¸­çš„ç»´åº¦éƒ½æ˜¯å¯¹äºŒç»´çš„å¼ é‡è€Œè¨€çš„ï¼Œä½†æ˜¯é¢˜ç›®è¾“å…¥çš„ç»´åº¦æ˜¯ `(N..., seq_len, d_model)`ï¼Œè¿™æ„å‘³ç€ `N...` è¡¨ç¤ºæœ‰ä»»æ„æƒ…å†µçš„ batch ç»´åº¦ï¼Œå®é™…ä¸ŠåšçŸ©é˜µè½¬ç½®çš„åªæœ‰æœ€åä¸¤ä¸ªç»´åº¦ã€‚æ‰€ä»¥ï¼Œç›´æ¥ç”¨ `key.T` å¿…ç„¶å®Œè›‹ã€‚è¿™é‡Œè½¬ç½®äº†æœ€åä¸¤ä¸ªç»´åº¦ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ `(seq_len, d_model)`ã€‚
2. è¿™ä¸ª mask ä¸æ˜¯ attn maskï¼Œè€Œæ˜¯ attn score çš„ maskï¼Œéœ€è¦é¢å¤–åŠ åˆ° attn score ä¸Šã€‚
3. ä½¿ç”¨ä¹˜æ³•å»å®Œæˆ scaled dot-productï¼Œè¿™ä¸ªåœ°æ–¹æŠ˜ç£¨äº†æˆ‘åŠå°æ—¶ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å†™ä½œ `(QK^T) / sqrt(d_k)`ï¼Œä½†æ˜¯è¿™æ ·æ•°å€¼ç²¾åº¦æ˜¯è¿‡ä¸å»çš„ã€‚é¦–å…ˆï¼Œä¸€èˆ¬é™¤æ³•è½¬æ¢ä¸ºä¹˜ä»¥å€’æ•°æ˜¯å¾ˆå¸¸è§çš„ç¨³å®šæ•°å€¼æ–¹æ³•ï¼›æ­¤å¤–ï¼Œå¤§è§„æ¨¡çš„çŸ©é˜µåšé™¤æ³•ç­‰ä»·äºæ¯ä¸ªå…ƒç´ éƒ½æŸå¤±äº†ç²¾åº¦ï¼Œè€Œç”¨ä¹˜ä»¥å€’æ•°çš„æ–¹å¼åªä¼šæŸå¤±å€’æ•°è®¡ç®—çš„é‚£ä¸€æ¬¡ã€‚
4. softmax éœ€è¦æœ‰ç»´åº¦ï¼Œä½†æˆ‘æ„Ÿè§‰è®°ä½å°±è¡Œã€‚

## SimpleMultiHeadAttention

è¿™é“é¢˜ç»§ç»­åŠ æ·±æˆ‘ä»¬å¯¹ç»´åº¦çš„ç†è§£ã€‚é¢˜é¢ä¸Šè¯´æ˜äº†å¤šæ¬¡ç»´åº¦è½¬æ¢ï¼Œæˆ‘ç›´æ¥åœ¨ä»£ç é‡Œæ ‡æ³¨ï¼š

```python
class SimpleMultiHeadAttention:
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        wq: mx.array,
        wk: mx.array,
        wv: mx.array,
        wo: mx.array,
    ):
        self.hidden_dim = hidden_size // num_heads
        self.num_heads = num_heads
        assert hidden_size % num_heads == 0, "hidden_size must be divisible by num_heads"
        self.scale = 1 / sqrt(self.hidden_dim) # hidden dim æ˜¯ æ¯ä¸ª head çš„ï¼Œä¸æ˜¯ # æ€»çš„ hidden size
        assert wq.shape == (hidden_size, hidden_size)
        assert wk.shape == (hidden_size, hidden_size)
        assert wv.shape == (hidden_size, hidden_size) # wq wk wv éƒ½æ˜¯ (hidden_size, hidden_size) æ–¹é˜µ
        self.wq = wq
        self.wk = wk
        self.wv = wv
        self.wo = wo

    def __call__(
        self,
        query: mx.array,
        key: mx.array,
        value: mx.array,
        mask: mx.array | None = None,
    ) -> mx.array:
        N, L, _ = query.shape # N æ˜¯ batch size, L æ˜¯ sequence length
        # Query çš„ç»´åº¦æ˜¯ N, L, hidden_size (æˆ–è€…è¯´ embedding sizeï¼Œè¿™é‡Œå’Œ hidden_size ä¸€æ ·å¤§)

        # è¿™é‡Œç›´æ¥ç”¨å†™å¥½çš„ linear ç‚¹ä¹˜ï¼Œlinear å®é™…ä¸Šæ‰§è¡Œçš„æ˜¯ y = xA^T + b
        # æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„ linear å±‚å®ç°å’Œæ•°å­¦å…¬å¼çš„å†™æ³•æœ‰ä¸€ä¸ªçº¦å®šä¿—æˆçš„å·®å¼‚ï¼š
        # æ•°å­¦å…¬å¼ä¸­æˆ‘ä»¬ä¹ æƒ¯å†™ XWï¼Œä½†åœ¨å®é™…ä»£ç å®ç°ä¸­ï¼Œæƒé‡çŸ©é˜µ W é€šå¸¸æ˜¯è½¬ç½®å­˜å‚¨çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¡†æ¶ä¸­å­˜å‚¨çš„å®é™…ä¸Šæ˜¯ W^T
        proejct_q = (
            linear(query, self.wq) # è¦æŠŠæœ€åä¸€ä¸ªç»´åº¦çš„ hidden_size æ‹†æˆ num_heads ä¸ª hidden_dim å¾—åˆ° (N, L, num_heads, hidden_dim)
            .reshape(N, L, self.num_heads, self.hidden_dim)  # è¿˜è¦è½¬ç½®ï¼Œå°† num heads æå‰
            .transpose(0, 2, 1, 3)  # (N, num_heads, L, hidden_dim)
        )
        projection_k = (
            linear(key, self.wk)
            .reshape(N, L, self.num_heads, self.hidden_dim)
            .transpose(0, 2, 1, 3)
        )
        projection_v = (
            linear(value, self.wv)
            .reshape(N, L, self.num_heads, self.hidden_dim)
            .transpose(0, 2, 1, 3)
        )
        attn = scaled_dot_product_attention_simple(
            proejct_q,
            projection_k,
            projection_v,
            scale=self.scale,
            mask=mask,
        ) # (N, num_heads, L, hidden_dim)
        # å…ˆæŠŠ num heads è½¬ç½®å›å»ï¼Œç„¶å reshape åˆå¹¶ï¼Œå›åˆ° (N, L, hidden_size)
        attn = attn.transpose(0, 2, 1, 3).reshape(N, L, self.hidden_dim * self.num_heads)
        return linear(attn, self.wo)
```
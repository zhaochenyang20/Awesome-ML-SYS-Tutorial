# verl sglang multi-turn over sample

## å¿«é€Ÿå¤ç°

1. åˆ›å»ºæ–°çš„ dockerï¼ˆå¦‚æœç†Ÿæ‚‰è¿™å¥—å®‰è£…ï¼Œå¯ä»¥è·³è¿‡ï¼‰ï¼š

ä½¿ç”¨å‰éœ€è¦é…ç½®å¥½ `WANDB_API_KEY`ï¼Œå‚è€ƒ[è¿™ä¸ªè¿‡ç¨‹](https://community.wandb.ai/t/where-can-i-find-the-api-token-for-my-project/7914)ã€‚

```bash
# å¦‚æœä½ çš„ç³»ç»Ÿæ²¡æœ‰é…ç½®è¿‡ HF_TOKEN å’Œ WANDB_API_KEYï¼Œè¯·å…ˆé…ç½®å¥½
docker run -it --name h100_verl_{your_name} --gpus all \
    --shm-size 32g \
    -v {your_cache_path}:/root/.cache \
    --env "HF_TOKEN=$HF_TOKEN" \
    --env "WANDB_API_KEY=$WANDB_API_KEY" \
    --ipc=host \
    lmsysorg/sglang:latest \
    /bin/bash
```

è¿›å…¥ docker åï¼Œå¯ä»¥æŸ¥çœ‹è¢«æ˜ å°„çš„ç¯å¢ƒå˜é‡ï¼š

```bash
echo $HF_TOKEN
echo $WANDB_API_KEY
```

ä»¥åæ¯æ¬¡ä» docker é‡Œé¢ exit å‡ºæ¥ï¼Œå†ç”¨è¿™ä¸ªæŒ‡ä»¤å¯ä»¥é‡å¯ï¼š

```bash
docker start -i h100_verl_{your_name}
```

2. åŸºäºæºç å®‰è£… verl-sglang

é…ç½® python ç¯å¢ƒï¼š

```bash
mkdir -p /tmp
chmod 1777 /tmp
apt update
apt install -y python3.10 python3.10-venv
python3 -m ensurepip --upgrade
python3 -m venv ~/.python/verl-sglang
source ~/.python/verl-sglang/bin/activate
python3 -m pip install --upgrade pip
python3 -m pip install --upgrade uv
```

3. å®‰è£… verl-sglangï¼š

```bash
cd ~
git clone -b over_sample https://github.com/zhaochenyang20/verl.git
cd verl

python -m uv pip install wheel setuptools
python3 -m uv pip install -e ".[sglang]" --prerelease=allow
python3 -m uv pip install -r ./requirements.txt --no-build-isolation
python3 -m uv pip install torch_memory_saver
```

4. æµ‹è¯• gsm8kï¼š

```bash
cd ~/verl
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# æ‹‰å–å¹¶é¢„å¤„ç† gsm8k æ•°æ®é›†
python examples/data_preprocess/gsm8k_multiturn_w_tool.py

# å¯åŠ¨ 8 å¡è®­ç»ƒ
bash examples/sglang_multiturn/run_qwen2.5-3b_gsm8k_multiturn.sh
```

5. æµ‹è¯• dapoï¼š

```bash
cd ~/verl
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

bash examples/sglang_multiturn/run_qwen3_4b_dapo_multiturn.sh
```

## è®¾è®¡æ€è·¯å’Œå…·ä½“å®ç°

åŸºäºè¿™ä¸ª commitï¼š[b979a73e358313afafab5db512cd5ae0009ccac0](https://github.com/zhaochenyang20/verl/tree/b979a73e358313afafab5db512cd5ae0009ccac0)

è®¾è®¡æ€è·¯å·²ç»è®¨è®ºäº†éå¸¸å¤šæ¬¡äº†ï¼Œä¸ºäº†è§£å†³ long tail é—®é¢˜ï¼Œé‡‡ç”¨ over sample æ˜¯éå¸¸å¸¸è§çš„ç­–ç•¥ã€‚ç›¸æ¯”äº partial rolloutï¼Œæ­¤å¤„è®¾è®¡çš„ç­–ç•¥æ›´ç²—æš´ã€‚æ²¡æœ‰å®Œæˆçš„ reqs å°†ä¼šç›´æ¥è¢«ä¸¢å¼ƒã€‚

å…·ä½“é€šè¿‡ `monitor_and_cancel`ï¼Œ`process_request_with_monitoring` å’Œ `run_with_cancellation` ä¸‰ä¸ªå‡½æ•°æ¥å®ç°ã€‚`monitor_and_cancel` è´Ÿè´£ç›‘æ§å®Œæˆæ•°é‡ï¼Œä¸€æ—¦è¾¾åˆ°ç›®æ ‡ç«‹å³è¡ŒåŠ¨ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡ï¼Œå¹¶å‘ engine å‘é€ abort ä¿¡å·ã€‚`process_request_with_monitoring` è´Ÿè´£å¤„ç†å•ä¸ªè¯·æ±‚ï¼Œå¹¶æ ¹æ®å®Œæˆæƒ…å†µè¿”å›çœŸå®ç»“æœæˆ– padding æ•°æ®ã€‚`run_with_cancellation` åŒæ—¶å¯åŠ¨ `monitor_and_cancel` å’Œ `process_request_with_monitoring`ã€‚

- `process_request_with_monitoring`

```python
async def process_request_with_monitoring(req):
    nonlocal completed_count
    try:
        result = await self._async_rollout_a_request(req, do_sample, is_validate, **kwargs)

        async with completion_lock:
            if completed_count < target_completion:
                completed_count += 1
                print(f"âœ… Request {req.request_id} completed ({completed_count}/{total_requests})")
                return result  # è¿”å›çœŸå®ç»“æœ
            else:
                # è¶…è¿‡ç›®æ ‡ï¼Œè¿”å›padding
                logger.info(f"Request {req.request_id} finished after target met, creating padding")
                return self._create_padding_request(req)
```

1. æ¯ä¸ª request ä¼šç‹¬ç«‹å¯åŠ¨è‡ªèº«çš„ `process_request_with_monitoring` ä¸­ï¼Œé€šè¿‡ `await` é˜»å¡å¼æ‰§è¡Œ `_async_rollout_a_request`ã€‚
2. å¯¹äºé‚£äº›è¾ƒæ—©å®Œæˆçš„ requestï¼Œresult å¾—åˆ°äº†çœŸå®ç»“æœï¼Œå¯¹ `completed_count` è®¡æ•°å™¨é€’å¢ã€‚æ³¨æ„è¿™é‡Œ `completed_count` æ˜¯å…¨å±€å˜é‡ï¼Œéœ€è¦ä½¿ç”¨ `completion_lock` ç¡®ä¿è®¡æ•°æ“ä½œçš„åŸå­æ€§ï¼Œè¯»å†™ä¸ä¼šå†²çªã€‚
3. å¯¹äºé‚£äº›è¾ƒæ™šå®Œæˆçš„ requestï¼Œ`monitor_and_cancel` æ£€æµ‹åˆ° `completed_count` è¾¾åˆ° `target_completion`ï¼Œä¼šå–æ¶ˆè¿™äº›ä»»åŠ¡ï¼Œå¹¶å‘ sglang engine å‘é€ `abort_requests` è¯·æ±‚ã€‚

- `monitor_and_cancel`

```python
async def monitor_and_cancel():
    nonlocal completed_count
    while completed_count < target_completion:
        await asyncio.sleep(0.1)  # æ¯0.1ç§’æ£€æŸ¥ä¸€æ¬¡

    print(f"ğŸ¯ Target reached: {completed_count}/{total_requests} completed!")
    print("ğŸš« Cancelling remaining requests and sending abort to engine...")

    # å–æ¶ˆå‰©ä½™çš„ä»»åŠ¡
    cancelled_count = 0
    for task in all_tasks:
        if not task.done():
            task.cancel()
            cancelled_count += 1

    # å‘engineå‘é€abortä¿¡å·
    try:
        abort_result = await self._engine.abort_request(abort_all=True)
        print(f"âœ… Abort signal sent to engine: {abort_result}")
    except Exception as e:
        print(f"âŒ Failed to send abort signal to engine: {e}")
```

æŒç»­ç›‘æ§å®Œæˆæ•°é‡ï¼Œä¸€æ—¦è¾¾åˆ°ç›®æ ‡ç«‹å³è¡ŒåŠ¨ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡ï¼Œå¹¶å‘ sglang engine å‘é€ `abort_requests` ä¿¡å·ã€‚æ³¨æ„è¿™é‡Œçš„ engine abort å®é™…ä¸Šå†™æ³•åœ¨ `sglang_rollout.py` é‡Œé¢çš„ `AsyncEngine` ç±»ï¼š

```python
    async def abort_request(self, rid: str = "", abort_all: bool = False):
        """Abort a specific request or all requests.

        Args:
            rid: The request ID to abort. If empty and abort_all is False, no action is taken.
            abort_all: If True, abort all running requests regardless of rid.
        """
        try:
            result = self.tokenizer_manager.abort_request(rid=rid, abort_all=abort_all)
            print(f"ğŸ” Abort result: {result}")
            return result if result is not None else {"status": "aborted"}
        except Exception as e:
            logger.error(f"Failed to abort requests: {e}")
            raise
```

è¿™é‡Œæœ‰å‡ ç‚¹å€¼å¾—ç©å‘³ï¼š

1. å…¶å® verl çš„ `AsyncEngine` ç»§æ‰¿å¹¶ä¸”é‡å†™äº† sglang Engine çš„å¾ˆå¤šæ–¹æ³•ï¼Œæ¯”å¦‚ `update_weights_from_tensor` å’Œ `resume_memory_occupation`ã€‚æŒ‰ç†è¯´å…¶å® sglang Engine ä¸å®ç°è¿™äº›æ–¹æ³•ä¹Ÿä¸å½±å“ verlï¼Œå½“ç„¶å½±å“å…¶ä»–æ¡†æ¶ã€‚ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºå¿…é¡»è¦ç°åœ¨ sglang ä¸­å®ç°å¯¹ Engine çš„ `abort_request`ï¼Œå› ä¸ºèµ·åˆåªæœ‰ server æœ‰è€Œ engine æ²¡æœ‰ã€‚ä½†æ˜¯è€ƒè™‘åˆ° `AsyncEngine` é‡å†™äº† `abort_request`ï¼Œæ‰€ä»¥å…¶å® sglang Engine ä¸éœ€è¦å®ç°è¿™ä¸€åŠŸèƒ½ï¼Œæˆ‘ä»¬ä¹Ÿæ— éœ€ä¸ºæ­¤å‘ç‰ˆã€‚æ¯•ç«Ÿï¼Œåœ¨ verl ä¸Šæ›´æ–° SGLang ç‰ˆæœ¬ç¡®å®å¤ªç—›è‹¦äº†ã€‚
2. ã€ç°åœ¨æˆ‘ä»¬åªæ˜¯ abort äº† engineï¼Œtool æ˜¯å¦ abort å½±å“å¦‚ä½•ï¼Ÿã€‘
3. ã€å’Œ `update_weights_from_tensor` ä¸ä¸€æ ·ï¼Œ`abort_request` å†…éƒ¨æ˜¯ä¸èƒ½é€šè¿‡ await å»è°ƒç”¨ `self.tokenizer_manager.abort_request`ï¼Œå¾—ç›´æ¥è°ƒç”¨ã€‚æ„Ÿè°¢ jiajun å’Œ yuzhen çš„æé†’ã€‚è¿™é‡Œå¾—å»æŸ¥ sglang tokenizer_manager å†…éƒ¨çš„å®ç°ã€‚å¦‚æœæŸä¸ªå‡½æ•°åœ¨ tokenizer_manager ä¸­æ˜¯å¼‚æ­¥å®ç°çš„ï¼Œé‚£ä¹ˆå¤–éƒ¨è°ƒç”¨æ‰å¯ä»¥æœ‰ await çš„è¯­æ³•ã€‚å¦è¯šè¯´è¿™é‡Œæ˜¯å› ä¸ºæˆ‘å¯¹å¼‚æ­¥è¯­æ³•å¹¶ä¸ç†Ÿæ‚‰ï¼Œè€Œä¸”æˆ‘ä¹Ÿä¸ç†è§£ä¸ºä»€ä¹ˆ `resume_memory_occupation` å’Œ `abort_request` åœ¨ tokenizer_manager ä¸­ï¼Œå‰è€…æ˜¯å¼‚æ­¥çš„ï¼Œåè€…æ˜¯åŒæ­¥çš„ã€‚æ­¤å¤–ï¼Œå¦‚æœæˆ‘ä»¬è¦åœ¨ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ä¸­åªå†™ä¸€è¡Œ await æŸä¸ªå¼‚æ­¥å‡½æ•°ï¼Œè¿™å°±æ„å‘³ç€å…¶å®åœ¨ç­‰å¾…å†…éƒ¨çš„å¼‚æ­¥å‡½æ•°æ‰§è¡Œå®Œæˆä¹ˆï¼Ÿè¿™ä¹ˆå†™æœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Ÿ å…·ä½“æ¥è¯´ï¼šã€‘

```python
# sglang_rollout.py

    async def resume_memory_occupation(self, tags: Optional[list[str]] = None):
        """Resume GPU occupation."""
        # because __init__ is a sync method, it can not call the async release_memory_occupation
        # have to move release_memory_occupation from __init__ to here
        # For multi-stage awake, we run release weight and kv_cache when we resume weights for the first time.
        if self._need_reload:
            await self.release_memory_occupation()
            self._need_reload = False

        if tags is None:
            obj = ResumeMemoryOccupationReqInput()
        else:
            obj = ResumeMemoryOccupationReqInput(tags=tags)
        return await self.tokenizer_manager.resume_memory_occupation(obj, None)
```

å†…å±‚çš„ `self.tokenizer_manager.resume_memory_occupation` æ˜¯ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ï¼Œæ‰€ä»¥åœ¨å¤–å±‚çš„ `resume_memory_occupation` å‡½æ•°åœ¨ç­‰å¾…å†…å±‚çš„å®Œæˆã€‚å¦‚æœè¿™æ ·ï¼Œä¸ºä»€ä¹ˆå¤–å±‚çš„ `resume_memory_occupation` å‡½æ•°ä¸èƒ½æ˜¯åŒæ­¥çš„å‘¢ï¼Ÿç›®å‰å¤–å±‚çš„å‡½æ•°æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥è°ƒç”¨å¤–å±‚å‡½æ•°å¹¶ç­‰å¾…ä¹Ÿå¾— awaitã€‚æ¯”å¦‚è¯´ï¼š

```python
# fsdp_sglang.py

async def release_memory(self):
    if self.device_mesh["infer_tp"].get_local_rank() == 0 and self.rollout_config.free_cache_engine:
        if self.multi_stage_wake_up:
            await self.inference_engine.release_memory_occupation(tags=["kv_cache", "weights"])
        else:
            await self.inference_engine.release_memory_occupation()
        log_gpu_memory_usage("After release memory occupation in sharding manager", logger=logger)
```


- `run_with_cancellation`


```python
async def run_with_cancellation():
    nonlocal all_tasks

    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    all_tasks = [asyncio.create_task(process_request_with_monitoring(req)) for req in req_list]

    # å¯åŠ¨ç›‘æ§ä»»åŠ¡
    monitor_task = asyncio.create_task(monitor_and_cancel())

    try:
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆåŒ…æ‹¬è¢«å–æ¶ˆçš„ï¼‰
        results = await asyncio.gather(*all_tasks, return_exceptions=True)

        # å¤„ç†ç»“æœï¼Œå°†å¼‚å¸¸è½¬æ¢ä¸ºpadding
        output_req_list = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # å¼‚å¸¸è½¬æ¢ä¸ºpadding
                logger.warning(f"Task {i} resulted in exception: {result}")
                output_req_list.append(self._create_padding_request(req_list[i]))
            else:
                output_req_list.append(result)

        return output_req_list
    finally:
        # æ¸…ç†ç›‘æ§ä»»åŠ¡
        monitor_task.cancel()
        try:
            await monitor_task
        except asyncio.CancelledError:
            pass
```

æœ‰äº†å¯¹å‰ä¸¤è€…çš„ç†è§£ï¼Œæœ€å `run_with_cancellation` å°±éå¸¸æ¸…æ™°äº†ã€‚æ³¨æ„ï¼Œ`all_tasks` å’Œè¯»å†™é” `completion_lock` æ˜¯è¿™ä¸‰ä¸ªå‡½æ•°çš„å…¨å±€å˜é‡ã€‚è¿™é‡ŒåŒæ—¶å¯åŠ¨æ‰€æœ‰ reqs çš„ `process_request_with_monitoring` å¹¶ä¸”åˆ›é€  `monitor_task` æ¥ç›‘è§†ã€‚æ³¨æ„ï¼Œè™½ç„¶æ¯ä¸ª req çš„ `_async_rollout_a_request` æœªå¿…ä¼šå®Œæˆï¼Œä½†æ˜¯è¿™ä¸ªå‡½æ•°ä¸Šå±‚çš„ `process_request_with_monitoring` æ˜¯ä¸€å®šä¼šç»“æŸçš„ï¼Œæ‰€ä»¥ `results = await asyncio.gather(*all_tasks, return_exceptions=True)` ä¸€å®šä¼šè¿”å›ï¼Œç„¶åé€ä¸ªå¤„ç† `results`ï¼Œè¿™æ—¶å€™é‡Œé¢å­˜åœ¨ä¸‰ç§æƒ…å†µï¼š`COMPLETED`ï¼Œ`Exception` å’Œ `PADDING`ã€‚å°† `Exception` è½¬æ¢ä¸º `PADDING` åï¼Œè¿”å› `output_req_list`ã€‚

æ•´ä½“è¯»å®Œï¼Œæˆ‘è§‰å¾—è®¾è®¡çš„è¿˜ç®—æ¸…æ™°ï¼Œå®ç°å¯èƒ½æœªå¿…å¥½ï¼Œè¿˜è¦å¤§æ”¹ã€‚

è¿™é‡Œåˆ—å‡ ä¸ªæˆ‘è§‰å¾—å¿…é¡»è¦æ£€æŸ¥çš„åœ°æ–¹ï¼š

1. ã€TODOï¼šç›´æ¥æ‰“æˆ padding çš„å®ç°æ˜¯å¦æ­£ç¡®ï¼Œè‡³å°‘ä¸€å®šä¸èƒ½æœ‰ lossã€‚æˆ‘ç†æƒ³çš„è®¾è®¡ä¸­ï¼Œè¿™ä¸ª reqs å°±æ˜¯è¢«ä¸¢å¼ƒäº†ï¼Œè®© GRPO çš„ group size å‡å°äº†ï¼Œè¿™ä¸ªè¯·æ±‚å°±æ˜¯ä¸å­˜åœ¨ï¼Œè€Œä¸”è¿˜ä¼šçœä¸‹ä¸€äº›è®­ç»ƒæ—¶é—´ã€‚è¿™é‡Œå¾—ä»”ç»†æ£€æŸ¥ï¼Œæ˜¯å¦é™¤å¼€ response_loss_mask è®¾ä¸º 0 ä¹‹å¤–è¿˜æœ‰åˆ«çš„è¦ä¿®æ”¹çš„åœ°æ–¹ã€‚æˆ‘ä¸€å¼€å§‹ä¿®æ”¹äº† `agg_loss` å‡½æ•°ï¼Œä½†æ˜¯åæ¥é—®äº† claudeï¼Œå¯èƒ½å¹¶ä¸éœ€è¦ï¼Œéœ€è¦é¢å¤–ç¡®è®¤ã€‚æ­¤å¤–ï¼Œreward å‡½æ•°æ˜¯å¦éœ€è¦æ›´æ”¹ï¼Œæˆ‘ä¹Ÿå¹¶ä¸ç¡®å®šï¼Œå¯¹äº FSDP æ¥è¯´ï¼Œæ˜¯è¿™ä¸ªå‡½æ•° `def _expand_to_token_level`ã€‚æœ€åï¼Œå‡å¦‚æˆ‘ä»¬å®ç°äº†å®Œç¾ä¸¢å¼ƒï¼Œè¿™æ ·ä¸åŒçš„ GRPO group çš„ requests æ•°é‡ä¸ä¸€è‡´ï¼ŒæŒ‰ç†æ¥è¯´è¿™ä¼šå½±å“ GRPO group çš„ varianceï¼Œå¯èƒ½ä¼šè®©è®­ç»ƒæ›´åŠ ä¸ç¨³å®šã€‚èƒ½å¦åˆ»ç”»è¿™ä¸€å½±å“ï¼Ÿã€‘è¿˜æœ‰ä¸€ç§è®¾è®¡ï¼Œç›´æ¥æ‰“æˆ padding å®é™…ä¸ŠæŠŠ partial rollout å¾—åˆ°çš„ trajs ç›´æ¥ä¸¢äº†ã€‚å¯ä»¥è€ƒè™‘ä¿ç•™è¿™äº› tracjsï¼Œä½†æ˜¯ loss mask ä¸º 0ï¼Œç„¶å reward ä¹Ÿæ˜¯ 0ï¼Œå¬é¾™è€å¸ˆè¯´å¯èƒ½å¥½äº›ã€‚
2. åœ¨å¼‚æ­¥çš„éƒ¨åˆ†ï¼Œæˆ‘æåˆ°äº†è¿™ä¸ªé—®é¢˜ï¼š

> å¦è¯šè¯´è¿™é‡Œæ˜¯å› ä¸ºæˆ‘å¯¹å¼‚æ­¥è¯­æ³•å¹¶ä¸ç†Ÿæ‚‰ï¼Œè€Œä¸”æˆ‘ä¹Ÿä¸ç†è§£ä¸ºä»€ä¹ˆ `resume_memory_occupation` å’Œ `abort_request` åœ¨ tokenizer_manager ä¸­ï¼Œå‰è€…æ˜¯å¼‚æ­¥çš„ï¼Œåè€…æ˜¯åŒæ­¥çš„ã€‚æ­¤å¤–ï¼Œå¦‚æœæˆ‘ä»¬è¦åœ¨ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ä¸­åªå†™ä¸€è¡Œ await æŸä¸ªå¼‚æ­¥å‡½æ•°ï¼Œè¿™å°±æ„å‘³ç€å…¶å®åœ¨ç­‰å¾…å†…éƒ¨çš„å¼‚æ­¥å‡½æ•°æ‰§è¡Œå®Œæˆä¹ˆï¼Ÿè¿™ä¹ˆå†™æœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Ÿ å…·ä½“æ¥è¯´ï¼š

3. æ•´ä¸ªå®ç°èƒ½å¦æ›´æ¸…æ™°ä¸€äº›ï¼Œèƒ½å¤Ÿè®© verl team åŒæ„è¿™ä¸ªç®€å•çš„ featureã€‚ç›®å‰æˆ‘éœ€è¦æ›´æ”¹ sglang_rollout.py å’Œ metric_utils.py çš„ compute_data_metrics å‡½æ•°ã€‚å‰è€…å·²ç»è®²è¿°å¾ˆå¤šäº†ï¼Œåè€…å…¶å®å¾ˆ trickyã€‚æˆ‘ç›®å‰å°†é‚£äº›è¢« abort æ‰çš„è¯·æ±‚åœ¨æ¯æ¬¡ reward ç®—å‡å€¼çš„æ—¶å€™æ’é™¤åœ¨åˆ†æ¯ä¹‹å¤–ã€‚æœ‰å‡ ä¸ªé—®é¢˜ï¼šã€å…ˆæ˜¯æˆ‘ä»¬éœ€è¦ç¡®è®¤ï¼Œåœ¨ validation çš„æ—¶å€™ï¼Œæ²¡æœ‰ abort æ‰çš„è¯·æ±‚ï¼Œç†è®ºä¸Š validation step ä¸ä¼šå—åˆ° `aborted_mask = (response_length == 0).bool() ` çš„å½±å“ï¼Œè¿™ä¸ªå¾—åšå®éªŒéªŒè¯ä¸‹ï¼Œvalidation step å°±ä¸ä¼šæœ‰ aborted reqsã€‚ã€‘æ­¤å¤–ï¼Œå¦‚æœæŠŠè¿™äº›è¢« abort çš„è¯·æ±‚è®°å½•åœ¨ metric ä¸­ï¼Œè¿™ä¸€éƒ¨åˆ†çš„ reward å…¨æ˜¯ 0ï¼Œæ‰€ä»¥ç›¸æ¯”ä¸åš over sample çš„ baselineï¼Œover sample çš„ reward ä¼šä½å¾ˆå¤šã€‚å¦‚æœåˆä¸è€ƒè™‘è¿™äº›è¢« abort çš„è¯·æ±‚ï¼Œå®é™…ä¸Šè¿™äº›è¢« abort çš„è¯·æ±‚å¾€å¾€æ˜¯æ›´å¤š turn æ›´éš¾çš„ exampleï¼Œè¿™éƒ¨åˆ† reward ç›¸æ¯”é‚£äº›æ²¡æœ‰è¢« abort çš„è¯·æ±‚æ›´ä½ã€‚æ‰€ä»¥æ— è§†è¿™éƒ¨åˆ†çš„ reward ä¼šå¯¼è‡´ reward è™šé«˜ã€‚ç›®å‰æˆ‘æ²¡æœ‰åŠæ³•é¿å…è¿™ç§è™šé«˜ï¼Œä½†æ˜¯æˆ‘è®¤ä¸º validation step çš„ reward æ˜¯å‡†ç¡®çš„ï¼Œç›®å‰å¹¶æ— å¤§ç¢ã€‚ã€æˆ‘ä»¬ç›´æ¥è®© loss mask = 0 å°±é¿å…äº† padding çš„ reqs å½±å“ lossï¼Œä½†æ˜¯æˆ‘é—® GPTï¼Œè²Œä¼¼åªå¯¹ç‰¹å®šçš„ agg loss mode æ˜¯æœ‰æ•ˆçš„ï¼Œè¿™é‡Œå¾—å»ç ”ç©¶ä¸‹ã€‚ã€‘

æ•´ä½“ä¸Šè¡¨ç°ç¬¦åˆé¢„æœŸï¼Œtraining step çš„ reward è™šé«˜ï¼Œvalidation step çš„ reward èƒ½å¤Ÿå’Œ baseline align ä¸Šï¼Œè€Œ rollout time æœ‰æ˜æ˜¾æ”¹å–„ã€‚

# verl Multi-turn Code Walk Throughï¼ˆPart 1ï¼‰

æ‰¿è’™ç¤¾åŒºåšçˆ±ï¼ŒAgentic RL å¦‚ç«å¦‚è¼ï¼Œæˆ‘ä»¬ SGLang RL å°ç»„çš„å·¥ä½œä¹Ÿåœ¨å¤œä»¥ç»§æ—¥ã€‚è€ƒè™‘åˆ°é¢†åŸŸä»¤äººææƒ§çš„å‘å±•é€Ÿåº¦ï¼Œç¤¾åŒºå·¨å¤§çš„äºŒæ¬¡å¼€å‘éœ€æ±‚ï¼Œæˆ‘ä»¬é€‰æ‹©ä»¥ verl å‡ºå‘ï¼Œåˆ†æå…¶ end to end mutli-turn RL è®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚æ•´ä½“ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›è¦†ç›–æ‰€æœ‰é‡è¦çš„ class ä»¥åŠå‡½æ•°ï¼Œæ›´ç»†ç²’åº¦çš„ä»£ç ä¸å†å±•å¼€ã€‚æˆ‘ä»¬çš„å†™ä½œé£æ ¼å¸Œæœ›èƒ½å¤Ÿ follow SGLang çš„ code-walk-throughï¼š

[SGLang Code Walk Through](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/sglang/code-walk-through/readme-CN.md)

ä¸ºäº†å‰åå†…å®¹çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬åŸºäº [76f63cffa5](https://github.com/volcengine/verl/commit/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39) çš„ commit è¿›è¡Œåˆ†æã€‚

æ„Ÿè°¢æ¥è‡ª Amazonï¼ŒLinkedInï¼Œé˜¿é‡Œç­‰å…¬å¸å’Œ SGLang RL å°ç»„çš„æœ‹å‹ä»¬çš„è´¡çŒ®ã€‚è™½ç„¶æœ¬æ–‡ä»¥åˆ†æ verl çš„ä»£ç ä¸ºä¸»ï¼Œå†™å®Œä¹‹åæˆ‘ä»¬æ‰æ„è¯†åˆ°ï¼Œç³»ç»Ÿè®¾è®¡é—®é¢˜æ˜¯éå¸¸é€šç”¨çš„ã€‚è¯¸å¦‚â€œlog probs é‡è®¡ç®—â€ï¼Œâ€œRollout Engine æ˜¾å­˜ç®¡ç†â€ç­‰ç­‰ç³»ç»Ÿè®¾è®¡ï¼Œæ˜¯å„å¤§ RL æ¡†æ¶éƒ½éœ€è¦è€ƒè™‘çš„æ ¸å¿ƒé—®é¢˜ã€‚æˆ‘ä»¬å¸Œæœ›æœ¬æ–‡å¯¹äºå¼€æºç¤¾åŒºç†è§£ RL æ¡†æ¶ç³»ç»Ÿè®¾è®¡èƒ½æä¾›å¯è¿ç§»çš„ç»éªŒ ğŸ˜‚

å¦‚æœæ‚¨å¯¹æˆ‘ä»¬çš„å·¥ä½œæ„Ÿå…´è¶£ï¼Œæ¬¢è¿æ¥è”ç³»æˆ‘ä»¬å‚ä¸ä¸€äº›å·¥ä½œï½

ç‰¹åˆ«è‡´è°¢ï¼šzhuoran yin @ CMUï¼Œchangyi yang @ CMUï¼Œzhuohao li @ é˜¿é‡Œï¼Œji li @å¾…ä¸šåœ¨å®¶ï¼ˆğŸ¤£ï¼‰ï¼Œbiao he @ Linkedin å’Œ xinpeng wei & chenyang zhao @ Amazonã€‚

--------------------------------

æ•´ä¸ªè®­ç»ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¼šå…·ä½“å±•å¼€æ¯ä¸ªéƒ¨åˆ†ã€‚

```mermaid
flowchart LR
subgraph W2["Initialize"]
WP[Process Data] --> A
direction TB D1[Data Prepare] --> A
A[TaskRunner] --> B1[RayPPOTrainer]
B1 --> Workers

    subgraph Workers["Workers"]
        direction TB
                WA[ActorRolloutWorker] --> WD[FSDP Engine]
        WB[CriticWorker] --> WD
        WC[RewardModelWorker] --> WD
        WD --> WE[SGLang Engine]
    end
    
    Workers --> C1[Hybrid Engine]
end

subgraph W3["Train Loop"]
    direction TB
    E[DataLoader] --> RolloutBox
    
    subgraph RolloutBox["Rollout"]
        F1[Prepare Data] --> F2[SGLang Async Rollout]
        F2 --> F3[Multi-turn Chat Process]
    end
    
    RolloutBox --> ExpBox
    
    subgraph ExpBox["Make Experience"]
        G1[Recompute Log Probs] --> G2[Compute Reward]
        G2 --> G3[Compute Advantage]
    end
    
    ExpBox --> UpdateBox
    
    subgraph UpdateBox["Train The Model"]
        H1[Load FSDP Model Weight] --> H2[Compute Gradient]
        H2 --> H3[Weights Update]
        H3 --> H4[Sync Weights]
    end
    
    UpdateBox --> E
end

W2 --> W3

```

## **æ•°æ®é¢„å¤„ç†**

ä»¥ [GSM8K](https://huggingface.co/datasets/openai/gsm8k) ä¸ºä¾‹ï¼Œé¢„å¤„ç†è„šæœ¬æ˜¯Â `examples/data_preprocess/gsm8k_multiturn_w_tool.py`ã€‚æ•´ä¸ªè„šæœ¬åªåšäº†ç»å…¸çš„ huggingface datasets mappingï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š

1. åŠ è½½ openai/gsm8k åŸå§‹æ•°æ®é›†ï¼ˆtrain/testï¼‰ã€‚
2. å¯¹æ¯æ¡åŸå§‹æ•°æ®ï¼Œç”Ÿæˆå¸¦æœ‰å·¥å…·è°ƒç”¨è¦æ±‚çš„ promptï¼ˆæ¯”å¦‚åœ¨ user turn å¼ºè°ƒæ¨¡å‹å¯ä»¥è°ƒç”¨Â `calc_gsm8k_reward`Â å·¥å…·ï¼Œæ¯ä¸ªqaè‡³å°‘è°ƒç”¨ä¸€æ¬¡ï¼‰ã€‚
3. åŒæ ·å¯¹äºæ¯æ¡åŸå§‹æ•°æ®ï¼Œè§£æç­”æ¡ˆï¼›å°† ground truth å†™å…¥ extra_info å­—æ®µã€‚
4. å­˜å‚¨ä¸º parquet æ–‡ä»¶ï¼Œåˆ†åˆ«ä¿ç•™ä¸º train.parquet å’Œ test.parquetï¼Œé»˜è®¤è·¯å¾„ä¸ºÂ `~/data/gsm8k/`ã€‚

## å¯åŠ¨è®­ç»ƒ

ä¸€ä¸ªå…¸å‹çš„å¯åŠ¨å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
# now ç”¨äºç”Ÿæˆå®éªŒå¯åŠ¨çš„æ—¶é—´å°¾ç¼€ï¼Œé¿å…é‡å¤å¯åŠ¨å®éªŒæ—¶è¦†ç›–å·²æœ‰ wandb log

function now() {
    date '+%Y-%m-%d-%H-%M'
}

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
nohup bash examples/sglang_multiturn/run_qwen2.5-3b_gsm8k_multiturn.sh \
    trainer.experiment_name=qwen2.5-3b_rm-gsm8k-sgl-multiturn-$now \
    > logs/gsm8k-$now.log 2>&1 &

```

## è„šæœ¬é…ç½®

verl çš„å„é¡¹å‚æ•°å®å±å¤æ‚ï¼Œæˆ‘ä»¬ä¼šå•ç‹¬ç¼–å†™æ–‡æ¡£æ¥åˆ†äº«å¯¹ verl å„ç±»å‚æ•°çš„ç†è§£ã€‚åœ¨è¿™ç¯‡æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æ ¼å¤–å¼ºè°ƒçš„æ˜¯ verl å„ç±» config çš„è¦†ç›–å…³ç³»ã€‚verl çš„é…ç½®æ–‡ä»¶åˆ©ç”¨ hydra è¿›è¡Œäº†**åˆ†å±‚è¦†ç›–**çš„è®¾è®¡æ¨¡å¼ã€‚

<details>

<summary>Hydra ç®€ä»‹</summary>

[**Hydra**](https://github.com/facebookresearch/hydra) æ˜¯ä¸€ä¸ªç”± Facebook Research å¼€å‘çš„ Python æ¡†æ¶ï¼Œæ—¨åœ¨**ä¼˜é›…åœ°é…ç½®å¤æ‚çš„åº”ç”¨ç¨‹åº**ã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºéœ€è¦ç®¡ç†å¤§é‡å‚æ•°å’Œè¿›è¡Œå¤šç»„å®éªŒçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨å­¦ä¹ é¡¹ç›®ã€‚Hydra çš„æ ¸å¿ƒç‰¹ç‚¹åœ¨äºå…¶**åŠ¨æ€ã€åˆ†å±‚å’Œå¯ç»„åˆçš„é…ç½®ç®¡ç†èƒ½åŠ›**ã€‚Hydra çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

* **åˆ†å±‚é…ç½® (Hierarchical Configuration)**ï¼šå¯ä»¥å°†é…ç½®åˆ†è§£æˆå¤šä¸ªå°å‹ã€æ¨¡å—åŒ–çš„ YAML æ–‡ä»¶ï¼Œå¹¶ä»¥ç›®å½•ç»“æ„è¿›è¡Œç»„ç»‡ã€‚è¿™ä½¿å¾—é…ç½®æ›´åŠ æ¸…æ™°ã€æ˜“äºç®¡ç†å’Œå¤ç”¨ã€‚
* **é…ç½®ç»„åˆ (Configuration Composition)**ï¼šHydra èƒ½å¤Ÿå°†è¿™äº›ç‹¬ç«‹çš„é…ç½®æ¨¡å—åŠ¨æ€åœ°ç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„é…ç½®å¯¹è±¡ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨ä¸»é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š `defaults` åˆ—è¡¨æ¥é€‰æ‹©å’Œç»„åˆä¸åŒçš„é…ç½®ç»„ä»¶ã€‚
* **å‘½ä»¤è¡Œè¦†ç›– (Command-line Overrides)**ï¼šè¿™æ˜¯ Hydra æœ€å¼ºå¤§çš„åŠŸèƒ½ä¹‹ä¸€ã€‚ä½ å¯ä»¥åœ¨è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼Œç›´æ¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–é…ç½®ä¸­çš„ä»»ä½•å€¼ã€‚è¿™ä½¿å¾—è¿›è¡Œå®éªŒå’Œå¿«é€Ÿè¿­ä»£å˜å¾—éå¸¸æ–¹ä¾¿ï¼Œæ— éœ€ä¿®æ”¹é…ç½®æ–‡ä»¶æœ¬èº«ã€‚
* **å¤šè¿è¡Œæ¨¡å¼ (Multi-run)**ï¼šHydra å…è®¸ä½ é€šè¿‡ä¸€ä¸ªå‘½ä»¤è¿è¡Œå¤šä¸ªå…·æœ‰ä¸åŒé…ç½®çš„å®éªŒã€‚è¿™å¯¹äºè¶…å‚æ•°æœç´¢å’Œæ¨¡å‹æ¯”è¾ƒéå¸¸æœ‰ç”¨ã€‚
* **åŠ¨æ€å·¥ä½œç›®å½• (Dynamic Working Directory)**ï¼šæ¯æ¬¡è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼ŒHydra éƒ½ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œç›®å½•ï¼Œå¹¶å°†å½“å‰è¿è¡Œçš„é…ç½®å’Œè¾“å‡ºä¿å­˜åˆ°è¯¥ç›®å½•ä¸­ï¼Œç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚
* **å¯¹è±¡å®ä¾‹åŒ– (Object Instantiation)**ï¼šHydra å¯ä»¥ç›´æ¥ä»é…ç½®ä¸­å®ä¾‹åŒ– Python å¯¹è±¡ï¼ˆç±»æˆ–å‡½æ•°ï¼‰ï¼Œè¿™å¤§å¤§ç®€åŒ–äº†ä»£ç ï¼Œä½¿é…ç½®æ›´å…·å£°æ˜æ€§ã€‚


Hydra å®ç°åˆ†å±‚è¦†ç›–çš„ä¸»è¦æœºåˆ¶æ˜¯**ç»„åˆ (Composition)** å’Œ **å‘½ä»¤è¡Œè¦†ç›– (Command-line Overrides)**ã€‚

1.  **åˆ†å±‚é…ç½®çš„ç»„ç»‡**ï¼š

é€šå¸¸ä¼šåˆ›å»ºä¸€ä¸ª `conf` ç›®å½•ï¼Œå¹¶åœ¨å…¶ä¸­ç»„ç»‡é…ç½®ã€‚ä¾‹å¦‚ï¼š

```yaml
.
â”œâ”€â”€ my_app.py
â””â”€â”€ conf
    â”œâ”€â”€ config.yaml
    â”œâ”€â”€ model
    â”‚   â”œâ”€â”€ cnn.yaml
    â”‚   â””â”€â”€ rnn.yaml
    â””â”€â”€ dataset
        â”œâ”€â”€ cifar10.yaml
        â””â”€â”€ imagenet.yaml
```

`config.yaml` æ˜¯ä½ çš„ä¸»é…ç½®æ–‡ä»¶ã€‚åœ¨ `model` ç›®å½•ä¸‹ï¼Œä½ å¯ä»¥å®šä¹‰ä¸åŒçš„æ¨¡å‹é…ç½®ï¼ˆå¦‚ `cnn.yaml`ã€`rnn.yaml`ï¼‰ï¼Œåœ¨ `dataset` ç›®å½•ä¸‹å®šä¹‰ä¸åŒçš„æ•°æ®é›†é…ç½®ï¼ˆå¦‚ `cifar10.yaml`ã€`imagenet.yaml`ï¼‰ã€‚

2.  **`defaults` åˆ—è¡¨è¿›è¡Œç»„åˆ**ï¼š

åœ¨ `config.yaml` ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ç‰¹æ®Šçš„ `defaults` åˆ—è¡¨æ¥æŒ‡å®šé»˜è®¤åŠ è½½å“ªäº›é…ç½®ç»„ä»¶ã€‚

**`conf/config.yaml` ç¤ºä¾‹ï¼š**

```yaml
defaults:
    - model: cnn       # é»˜è®¤åŠ è½½ conf/model/cnn.yaml
    - dataset: cifar10 # é»˜è®¤åŠ è½½ conf/dataset/cifar10.yaml
    - _self_          # ç¡®ä¿å½“å‰æ–‡ä»¶ä¸­çš„å…¶ä»–é…ç½®é¡¹ä¹Ÿè¢«åŠ è½½

# å…¶ä»–åº”ç”¨çº§åˆ«çš„é»˜è®¤é…ç½®
learning_rate: 0.001
epochs: 10
```

å½“ Hydra åŠ è½½ `config.yaml` æ—¶ï¼Œå®ƒä¼šæ ¹æ® `defaults` åˆ—è¡¨ä¸­çš„æŒ‡ç¤ºï¼Œè‡ªåŠ¨å°† `conf/model/cnn.yaml` å’Œ `conf/dataset/cifar10.yaml` çš„å†…å®¹åˆå¹¶åˆ°æœ€ç»ˆçš„é…ç½®å¯¹è±¡ä¸­ã€‚

3.  **å‘½ä»¤è¡Œè¦†ç›–**ï¼š

è¿™æ˜¯å®ç°çµæ´»è¦†ç›–çš„å…³é”®ã€‚ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–ä»»ä½•å·²åŠ è½½çš„é…ç½®å€¼ï¼ŒåŒ…æ‹¬åœ¨ `defaults` åˆ—è¡¨ä¸­æŒ‡å®šçš„ç»„ä»¶æˆ–å…¶å†…éƒ¨çš„ä»»ä½•å‚æ•°ã€‚

* **è¦†ç›–æ•´ä¸ªé…ç½®ç»„**ï¼š
è¦åˆ‡æ¢æ¨¡å‹ä» `cnn` åˆ° `rnn`ï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸­è¿™æ ·è¿è¡Œï¼š

```bash
python my_app.py model=rnn
```

è¿™å°†æŒ‡ç¤º Hydra åŠ è½½ `conf/model/rnn.yaml`ï¼Œå¹¶ç”¨å®ƒæ¥æ›¿æ¢é»˜è®¤çš„ `cnn` é…ç½®ã€‚

* **è¦†ç›–ç‰¹å®šå‚æ•°**ï¼š
ä½ å¯ä»¥æ·±å…¥åˆ°é…ç½®çš„ä»»ä½•å±‚çº§æ¥è¦†ç›–ç‰¹å®šçš„å‚æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ä¿®æ”¹å­¦ä¹ ç‡æˆ–æ•°æ®é›†çš„æŸä¸ªå‚æ•°ï¼š

```bash
python my_app.py learning_rate=0.01 dataset.batch_size=64
```

è¿™é‡Œï¼Œ`learning_rate` ç›´æ¥è¦†ç›–äº† `config.yaml` ä¸­çš„å€¼ï¼Œè€Œ `dataset.batch_size` åˆ™è¦†ç›–äº† `conf/dataset/cifar10.yaml`ï¼ˆæˆ–è€…ä½ é€šè¿‡ `dataset=imagenet` æŒ‡å®šçš„å…¶ä»–æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼‰ä¸­çš„ `batch_size` å‚æ•°ã€‚

* **æ·»åŠ æ–°å‚æ•° (ä½¿ç”¨ `+`)**ï¼š
å¦‚æœä½ æƒ³æ·»åŠ ä¸€ä¸ªåœ¨é»˜è®¤é…ç½®ä¸­ä¸å­˜åœ¨çš„æ–°å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨ `+` å‰ç¼€ï¼š

```bash
python my_app.py +optimizer.name=AdamW
```

* **åŠ¨æ€è¦†ç›– (ä½¿ç”¨ `++`)**ï¼š
å¦‚æœä½ å¸Œæœ›ä¿®æ”¹ä¸€ä¸ªå·²æœ‰å­—æ®µï¼Œæˆ–è€…åœ¨åŸé…ç½®ä¸­æ²¡æœ‰è¯¥å­—æ®µæ—¶è‡ªåŠ¨åˆ›å»ºå®ƒï¼Œå¯ä»¥ä½¿ç”¨ ++ã€‚è¿™ç§æ–¹å¼é€‚ç”¨äºéœ€è¦åŠ¨æ€æ·»åŠ æˆ–è¦†ç›–é…ç½®é¡¹çš„åœºæ™¯ï¼Œç¡®ä¿å­—æ®µæ€»æ˜¯è¢«è®¾ç½®ä¸ºä½ æŒ‡å®šçš„å€¼ï¼Œæ— è®ºå®ƒæ˜¯å¦å·²å­˜åœ¨ã€‚

```bash
python my_app.py ++model.num_layers=10
```

Hydra å†…éƒ¨ä½¿ç”¨ [OmegaConf](https://www.google.com/search?q=https://omegaconf.readthedocs.io/en/2.3_latest/) åº“æ¥å¤„ç†è¿™äº›é…ç½®å¯¹è±¡ï¼Œå®ƒæä¾›äº†å¼ºå¤§çš„åˆå¹¶å’Œè§£æåŠŸèƒ½ï¼Œä½¿å¾—åˆ†å±‚è¦†ç›–å’Œå€¼æ’å€¼ï¼ˆä¾‹å¦‚ï¼Œå¼•ç”¨å…¶ä»–é…ç½®å€¼æˆ–ç¯å¢ƒå˜é‡ï¼‰å˜å¾—éå¸¸å®¹æ˜“ã€‚

</details>


å›åˆ° verl multi turnï¼Œåœ¨æˆ‘ä»¬å¯åŠ¨çš„ `run_qwen2.5-3b_gsm8k_multiturn.sh` ä¸­ï¼Œè®¾ç½®äº†ï¼š

```bash
PROJECT_DIR="$(pwd)"
CONFIG_PATH="$PROJECT_DIR/examples/sglang_multiturn/config"

python3 -m verl.trainer.main_ppo \
    --config-path="$CONFIG_PATH" \
    --config-name='gsm8k_multiturn_grpo' \
```

è¿™æ„å‘³ç€è¿™æ¬¡ä»»åŠ¡çš„é»˜è®¤ config æ˜¯ `CONFIG_PATH` ä¸‹çš„ `gsm8k_multiturn_grpo.yaml`ï¼Œä¸”æ¥ä¸‹æ¥çš„å‚æ•°ä¼šè¦†ç›– `gsm8k_multiturn_grpo.yaml` ä¸­çš„é»˜è®¤å€¼ã€‚æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬æ¥è§‚å¯Ÿ `gsm8k_multiturn_grpo.yaml` çš„å†…å®¹ï¼š

```yaml
hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  max_prompt_length: 1024
  max_response_length: 1024
  train_batch_size: 256
  return_raw_chat: True

actor_rollout_ref:
  hybrid_engine: True
  rollout:
    name: sglang
    multi_turn:
      enable: True
      max_turns: 5
      # tool_config_path: "./config/tool_config/gsm8k_tool_config.yaml"
```

è¿™é‡Œ hydra è¯­æ³•ï¼Œä¼šå» `verl/trainer/config` ç›®å½•ä¸‹å¯»æ‰¾ `ppo_trainer.yaml` ä½œä¸ºåŸºç¡€é…ç½®ï¼Œå¹¶ä¸”è¦†ç›–ã€‚å› æ­¤ï¼Œå¯åŠ¨ `run_qwen2.5-3b_gsm8k_multiturn.sh` æ—¶ï¼Œå…ˆåŠ è½½ `gsm8k_multiturn_grpo.yaml` ä½œä¸ºåŸºç¡€é…ç½®å¹¶è¦†ç›–ï¼Œç„¶ååŠ è½½ `ppo_trainer.yaml` å¹¶è¦†ç›–ã€‚æœ€ç»ˆåˆå¹¶è¿™ä¸‰çº§é…ç½®ï¼Œå¾—åˆ°æœ€ç»ˆçš„ configã€‚

æœ€åï¼Œæ³¨æ„åˆ°åœ¨ `run_qwen2.5-3b_gsm8k_multiturn.sh` çš„æœ€åï¼Œæˆ‘ä»¬ï¼Œæˆ‘ä»¬è®¾ç½®äº† `actor_rollout_ref.rollout.multi_turn.tool_config_path="$PROJECT_DIR/examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml"`ï¼Œè¿™é‡ŒæŒ‡å®š multi_turn çš„ tool_config_path ä¸º `examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml`ã€‚è¿™ä¸€æ–‡ä»¶ä»…ä»…é…ç½®äº† gsm8k çš„ tool è°ƒç”¨ï¼Œå¹¶ä¸ä¼šè¦†ç›–ä¹‹å‰è®­ç»ƒçš„ configã€‚

## è®­ç»ƒä¸»å…¥å£ä¸åˆå§‹åŒ–

### Ray Actorï¼ŒRay Task å’Œ Ray Worker

åœ¨ä»‹ç» verl çš„è®­ç»ƒä¸»å…¥å£ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»‹ç» Ray çš„ä¸€äº›æ ¸å¿ƒæ¦‚å¿µã€‚Ray æ˜¯ä¸€ä¸ªç»Ÿä¸€è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç®€å•åœ°ä»å•æœºåˆ°å¤§å‹åˆ†å¸ƒå¼é›†ç¾¤çš„æ‰©å±•ï¼Œæä¾›æ„å»ºå’Œè¿è¡Œåˆ†å¸ƒå¼åº”ç”¨çš„åº•å±‚åŸºç¡€è®¾æ–½å’Œä¸€ç»„æ ¸å¿ƒåŸè¯­ã€‚Ray é€šè¿‡ä»¥ä¸‹åŠŸèƒ½å®ç°è¿™ä¸€ç›®æ ‡ï¼š

1. **ç»Ÿä¸€ API**ï¼šRay æä¾›äº†ä¸€å¥—ç®€å•æ˜“ç”¨çš„ Python APIï¼Œå°†æ™®é€šå‡½æ•°è½¬æ¢ä¸ºåˆ†å¸ƒå¼ä»»åŠ¡ï¼Œå°† Python ç±»è½¬æ¢ä¸ºåˆ†å¸ƒå¼æœåŠ¡ï¼Œä¹Ÿå³ Ray Actorã€‚Ray Actor å†…éƒ¨æŒä¹…å­˜å‚¨çš„æ•°æ®ç§°ä¸ºçŠ¶æ€ï¼Œå¯ä»¥åœ¨ Actor çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…è¢«å¤šæ¬¡è®¿é—®ã€ä¿®æ”¹å’Œç»´æŠ¤ï¼Œè€Œä¸ä¼šåœ¨æ¯æ¬¡æ–¹æ³•è°ƒç”¨ç»“æŸåæ¶ˆå¤±ã€‚
2. **å¼¹æ€§ä¼¸ç¼©**ï¼šRay å¯ä»¥å°†åº”ç”¨ä»å•ä¸ªæœºå™¨æ— ç¼æ‰©å±•åˆ°æ‹¥æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„é›†ç¾¤ï¼Œå¹¶èƒ½æ ¹æ®éœ€æ±‚è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚
3. **å®¹é”™æ€§**ï¼šRay å†…ç½®äº†å®¹é”™æœºåˆ¶ï¼Œå¯ä»¥å¤„ç†èŠ‚ç‚¹æ•…éšœå’Œä»»åŠ¡å¤±è´¥ï¼Œç¡®ä¿åº”ç”¨çš„å¥å£®æ€§ã€‚
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šRay ä¼˜åŒ–äº†åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†å’Œæ•°æ®ä¼ è¾“ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚

Ray Task å’Œ Ray Actor éƒ½æ˜¯ç”¨äºåˆ†å¸ƒå¼è®¡ç®—çš„æ ¸å¿ƒåŸè¯­ï¼Œä½†å®ƒä»¬å„è‡ªæœåŠ¡äºä¸åŒçš„ç›®çš„ï¼Œä¸»è¦åŒºåˆ«åœ¨äº**æ˜¯å¦ç»´æŠ¤çŠ¶æ€**ã€‚

Ray Task æ˜¯ Ray ä¸­æœ€åŸºæœ¬çš„è®¡ç®—å•å…ƒï¼Œä»£è¡¨ä¸€ä¸ªæ— çŠ¶æ€çš„è¿œç¨‹å‡½æ•°ã€‚Ray Task çš„æ¯æ¬¡æ‰§è¡Œéƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¿ç•™ä¹‹å‰çš„ä»»ä½•ä¿¡æ¯ã€‚å°±åƒè°ƒç”¨ä¸€ä¸ªæ™®é€šå‡½æ•°ï¼Œæ‰§è¡Œå®Œåå°±æ¸…é™¤å†…éƒ¨çŠ¶æ€ã€‚æˆ‘ä»¬è°ƒç”¨ä¸€ä¸ª Ray Task åï¼Œä¼šç«‹å³è¿”å›å¾—åˆ°ä¸€ä¸ª Ray ObjectRefï¼Œè€Œä¸æ˜¯å®é™…çš„ç»“æœã€‚ä¸»ç¨‹åºå¯ä»¥ç»§ç»­æ‰§è¡Œå…¶ä»–æ“ä½œï¼Œè€Œ Ray Task åˆ™åœ¨åå°å¹¶è¡Œè¿è¡Œã€‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨ `ray.get()` æ¥è·å– Task çš„å®é™…ç»“æœã€‚ Ray Task éå¸¸é€‚åˆå¹¶è¡Œæ‰§è¡Œå¤§é‡ç‹¬ç«‹ã€ä¸€æ¬¡æ€§çš„è®¡ç®—ä»»åŠ¡ï¼Œè­¬å¦‚æ•°æ®æ‰¹å¤„ç†ã€ç‹¬ç«‹çš„æ¨¡å‹æ¨ç†ç­‰åœºæ™¯ã€‚

Ray Actor æ˜¯ä¸€ç§ç‰¹æ®Šçš„ Ray Taskï¼Œæ­£å¦‚å‰æ–‡æ‰€è¿°ï¼Œå®ƒæ˜¯ä¸€ä¸ªæŒç»­è¿è¡Œçš„ã€æœ‰è‡ªå·±çš„çŠ¶æ€å’Œæ–¹æ³•çš„è¿œç¨‹å¯¹è±¡ã€‚å½“æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª Ray Actor åï¼ŒRay ä¼šåœ¨é›†ç¾¤ä¸­çš„æŸä¸ª **Ray Worker** ä¸Šå¯åŠ¨ä¸€ä¸ªä¸“é—¨çš„è¿›ç¨‹æ¥æ‰˜ç®¡è¿™ä¸ªå¯¹è±¡ã€‚è¯¥è¿›ç¨‹ä¼šä¸€ç›´è¿è¡Œï¼Œç›´åˆ°è¢«é”€æ¯ã€‚Actor å¯ä»¥ç»´æŠ¤å†…éƒ¨å˜é‡ï¼Œå¹¶ä¸”è¿™äº›å˜é‡åœ¨ Actor çš„ç”Ÿå‘½å‘¨æœŸå†…æ˜¯æŒä¹…å­˜åœ¨çš„ã€‚æ¯æ¬¡è°ƒç”¨ Actor çš„æ–¹æ³•ï¼Œéƒ½å¯ä»¥è®¿é—®å’Œä¿®æ”¹è¿™äº›çŠ¶æ€ã€‚è¿™ä¸æ™®é€šçš„ Ray Task ä¸åŒï¼Œæ™®é€š Task æ‰§è¡Œå®Œä¼šæ¸…é™¤å†…éƒ¨çŠ¶æ€ã€‚Ray Actor æ”¯æŒå¹¶å‘è¯·æ±‚ï¼ŒRay ä¼šè´Ÿè´£å°†è¿™äº›è¯·æ±‚åºåˆ—åŒ–æ‰§è¡Œï¼Œä¿è¯ Actor å†…éƒ¨çŠ¶æ€çš„ä¸€è‡´æ€§å’Œçº¿ç¨‹å®‰å…¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ `@ray.remote` è£…é¥°å™¨å°†ä¸€ä¸ª Python ç±»è½¬æ¢ä¸ºä¸€ä¸ª Ray Actor ç±»ï¼Œç„¶åé€šè¿‡ `.remote()` æ–¹æ³•å®ä¾‹åŒ–ä¸€ä¸ªè¿œç¨‹ Actorã€‚

æœ€åï¼ŒRay Worker æ˜¯ Ray é›†ç¾¤ä¸­çœŸæ­£æ‰§è¡Œä»£ç çš„å·¥ä½œå•å…ƒã€‚ä¸€ä¸ª Ray é›†ç¾¤é€šå¸¸ç”±ä¸€ä¸ª Head Node å’Œå¤šä¸ª Worker Nodes ç»„æˆã€‚æ¯ä¸ªèŠ‚ç‚¹ä¸Šéƒ½ä¼šè¿è¡Œä¸€ä¸ªæˆ–å¤šä¸ª Ray Worker è¿›ç¨‹ã€‚æ— è®ºæ˜¯æ™®é€šçš„ Ray Task è¿˜æ˜¯ Ray Actor çš„æ–¹æ³•ï¼Œæœ€ç»ˆéƒ½æ˜¯ç”± Ray Worker è¿›ç¨‹æ¥æ‰§è¡Œçš„ã€‚æ¯ä¸ª Ray Worker éƒ½ä¼šè¢«åˆ†é…ä¸€å®šçš„è®¡ç®—èµ„æºï¼ˆå¦‚ CPUã€GPUï¼‰ã€‚å½“ä½ æäº¤ä¸€ä¸ª Ray Task æˆ–åˆ›å»ºä¸€ä¸ª Ray Actor æ—¶ï¼ŒRay çš„è°ƒåº¦å™¨ä¼šæ‰¾åˆ°ä¸€ä¸ªæœ‰è¶³å¤Ÿèµ„æºçš„ Worker æ¥è¿è¡Œå®ƒã€‚Worker è¿›ç¨‹ä¹‹é—´ä»¥åŠ Worker è¿›ç¨‹ä¸å¤´èŠ‚ç‚¹ä¹‹é—´ä¼šè¿›è¡Œé€šä¿¡ï¼Œä»¥åè°ƒä»»åŠ¡æ‰§è¡Œã€ä¼ è¾“æ•°æ®å’Œç®¡ç†çŠ¶æ€ã€‚ä¸€ä¸ª Ray Worker é€šå¸¸å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ Python è¿›ç¨‹ã€‚å¯¹äºæ™®é€šçš„ Ray Taskï¼ŒRay Worker ç›¸å½“äºå‡½æ•°è§£é‡Šå™¨ï¼Œæ‰§è¡Œå®Œä»»åŠ¡åå¯èƒ½ä¼šè¢«å¤ç”¨å»æ‰§è¡Œå…¶ä»–ä»»åŠ¡ã€‚è€Œå¯¹äº Ray Actorï¼ŒRay ä¼šå¯åŠ¨ä¸€ä¸ªä¸“é—¨çš„ Worker è¿›ç¨‹æ¥æ‰˜ç®¡è¿™ä¸ª Actorï¼Œè¿™ä¸ª Worker è¿›ç¨‹çš„ç”Ÿå‘½å‘¨æœŸä¸ Actor çš„ç”Ÿå‘½å‘¨æœŸç»‘å®šã€‚

### `run_ppo()` å’Œ `TaskRunner.run()`

æœ‰äº† ray çš„æ¦‚å¿µï¼Œæˆ‘ä»¬å›åˆ°æ•´ä¸ª RL è®­ç»ƒæµç¨‹çš„èµ·ç‚¹ï¼š`verl.trainer.main_ppo.py` ä¸­çš„ [`run_ppo()`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L35)ï¼Œå®ƒè´Ÿè´£åˆå§‹åŒ– Ray é›†ç¾¤ï¼Œé…ç½® CPU èµ„æºå’Œè¿è¡Œæ—¶ç¯å¢ƒå˜é‡ï¼Œå¹¶åˆ›å»ºè¿œç¨‹ TaskRunner å®ä¾‹ã€‚

```python
def run_ppo(config) -> None:
    # åˆå§‹åŒ– Ray é›†ç¾¤ï¼Œé…ç½® CPU èµ„æºå’Œè¿è¡Œæ—¶ç¯å¢ƒå˜é‡
    ray.init(
        runtime_env={"env_vars": {...}},
        num_cpus=config.ray_init.num_cpus,
    )

    # åˆ›å»ºè¿œç¨‹ TaskRunner å®ä¾‹
    # TaskRunner æ˜¯ Ray ä¸­çš„ä¸€ä¸ªè¿œç¨‹ actorï¼Œå®ƒå°†åœ¨ Ray é›†ç¾¤ä¸Šå¼‚æ­¥æ‰§è¡Œä¸»è¦çš„è®­ç»ƒä»»åŠ¡
    runner = TaskRunner.remote()
    # å¼‚æ­¥æ‰§è¡Œè¿œç¨‹ä»»åŠ¡ runner.run()ï¼Œå¹¶ç­‰å¾…å…¶å®Œæˆ
    # é€šè¿‡ ray.get() é˜»å¡ç›´åˆ°è¿œç¨‹ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œç¡®ä¿æ•´ä¸ªåˆå§‹åŒ–æµç¨‹çš„é¡ºåºæ€§
    ray.get(runner.run.remote(config))
```

### ActorRolloutRefWorker å’Œ RayWorkerGroup çš„ç›¸äº’å…³ç³»

[TaskRunner](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64) æ˜¯ verl ä¸­å®ç° PPO/GRPO è®­ç»ƒçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒé€šè¿‡å°†æ•´ä¸ª RL è®­ç»ƒæµç¨‹å°è£…åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Ray Actor ä¸­ï¼Œå®ç°äº†ä»»åŠ¡çš„å°è£…ã€èµ„æºéš”ç¦»å’Œåˆ†å¸ƒå¼åè°ƒã€‚ä¸ºäº†è§£é‡Šæ¸…æ¥š `TaskRunner`ï¼Œæˆ‘ä»¬å°† verl å½“ä¸­æœ€è®©äººè´¹è§£ä¸”æœ€å¤æ‚çš„ `ActorRolloutRefWorker` å’Œ `RayWorkerGroup` è¿™ä¸¤ä¸ªç±»æå‰è§£é‡Šæ¸…æ¥šã€‚

æˆ‘ä»¬å…ˆä¸è®¨è®ºè¿™ä¸¤ä¸ªç±»åŠå…¶åŸºç±»çš„å…·ä½“æ„ä¹‰ï¼Œå…ˆè®¨è®ºæ¸…æ¥šå…¶å®ä¾‹å¯¹è±¡çš„åˆ›å»ºè¿‡ç¨‹ã€‚æˆ‘ä»¬æ³¨æ„åˆ°è¿™æ®µ `TaskRunner` çš„åˆå§‹åŒ–ä¸­å¼•å…¥ `ActorRolloutRefWorker` å’Œ `RayWorkerGroup` çš„ç›¸å…³ä»£ç ï¼š

<details>
<summary>TaskRunner ä¸­å¼•å…¥ ActorRolloutRefWorker</summary>

```python
        # Define worker classes based on the actor strategy.
        if config.actor_rollout_ref.actor.strategy in ["fsdp", "fsdp2"]:
            assert config.critic.strategy in ["fsdp", "fsdp2"]
            from verl.single_controller.ray import RayWorkerGroup
            from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker

            actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == "async" else ActorRolloutRefWorker
            ray_worker_group_cls = RayWorkerGroup

        elif config.actor_rollout_ref.actor.strategy == "megatron":
            assert config.actor_rollout_ref.actor.strategy == config.critic.strategy
            from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup
            from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker

            actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == "async" else ActorRolloutRefWorker
            ray_worker_group_cls = NVMegatronRayWorkerGroup

        else:
            raise NotImplementedError

        from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role

        # Map roles to their corresponding remote worker classes.
        role_worker_mapping = {
            Role.ActorRollout: ray.remote(actor_rollout_cls),
            Role.Critic: ray.remote(CriticWorker),
        }

        # Define the resource pool specification.
        # Map roles to the resource pool.
        global_pool_id = "global_pool"
        resource_pool_spec = {
            global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,
        }
        mapping = {
            Role.ActorRollout: global_pool_id,
            Role.Critic: global_pool_id,
        }
```

</details>

å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œåœ¨ `TaskRunner` çš„åˆå§‹åŒ–ä¸­ï¼Œä¼šæ ¹æ®å„ç±»é…ç½®å¼•å…¥å¯¹åº”çš„ `ActorRolloutRefWorker / AsyncActorRolloutRefWorker` ç±»ä»¥åŠ `RayWorkerGroup / NVMegatronRayWorkerGroup` ç±»ã€‚å¯¹äº SGLang è€Œè¨€ï¼Œä¸å­˜åœ¨ `AsyncActorRolloutRefWorker`ã€‚`ActorRolloutRefWorker` ç±»ç›´æ¥é€šè¿‡ `ray.remote(ActorRolloutRefWorker)` åˆ›å»ºä¸€ä¸ªè¿œç¨‹çš„ Ray Actorï¼Œå°†å…¶åŒ…è£…æˆä¸€ä¸ª Ray Actor ç±»ã€‚æ­¤æ—¶è¿˜è¿˜æ²¡æœ‰åˆ›å»ºä»»ä½•å®ä¾‹ï¼Œä¹Ÿæ²¡æœ‰åˆ†é…èµ„æºã€‚é‚£ä¹ˆï¼Œ`ActorRolloutRefWorker` ç±»åˆ°åº•åœ¨å“ªå„¿å®ä¾‹åŒ–å¹¶åˆ†é…èµ„æºçš„å‘¢ï¼Ÿ

å®é™…ä¸Šï¼Œåœ¨ `main_ppo.py` çš„ [172 è¡Œ](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L172)ï¼Œæ„é€ äº† `RayPPOTrainer` ç±»ï¼Œéšåè°ƒç”¨äº† `RayPPOTrainer.init_workers()` æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æŸ¥çœ‹ `RayPPOTrainer.init_workers()` æ–¹æ³•çš„[ç›¸å…³ä»£ç ](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715)ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæ¯ä¸€ä¸ª RL worker ç±»ï¼ˆæ¯”å¦‚ ActorRolloutRefWorkerï¼‰éƒ½ä¼šåˆ›é€ ä¸€ä¸ª work groupï¼ˆverl ä¸­çš„å„ç§ wg å˜é‡ï¼‰ï¼Œéšåè°ƒç”¨æ¯ä¸ª worker group çš„ `init_model()` æ–¹æ³•ï¼Œè€Œè¿™äº› worker group å®é™…ä¸Šéƒ½æ˜¯ `RayWorkerGroup` çš„å®ä¾‹ã€‚`RayWorkerGroup` çš„æ ¸å¿ƒä½œç”¨æ˜¯èµ„æºè°ƒåº¦çš„æ ¸å¿ƒä¸­é—´å±‚ï¼Œç»Ÿä¸€äº†å„ç§ RL workerï¼ˆæ¯”å¦‚ ActorRolloutRefWorkerã€CriticWorkerï¼‰çš„æ¥å£ï¼Œè¿›è¡Œç»Ÿä¸€ç®¡ç†ï¼š

```python

# RayWorkerGroup å®ä¾‹ï¼ŒæŒ‡å®šèµ„æºæ±  å¹¶è§„å®šè§’è‰²å’Œå¯¹åº”çš„ç±»
wg_dict = self.ray_worker_group_cls(
    resource_pool=resource_pool,  # åªéœ€è¦æŒ‡å®šèµ„æºæ± 
    ray_cls_with_init=worker_dict_cls,  # ä¸€ä¸ªåŒ…å«æ•°ä¸ªworkerçš„ç±» ï¼ˆe.g. actor_rollï¼Œ critic, refï¼‰
    device_name=self.device_name,
)

#é€šè¿‡.spawn()è·å–è§’è‰²å¯¹Ray Actorå®ä¾‹çš„æ˜ å°„
wg_dict.spawn(prefix_set=class_dict.keys())


# æ‰€æœ‰ worker éƒ½é€šè¿‡ç›¸åŒçš„æ¨¡å¼åˆ›å»ºï¼Œæˆ‘è¿™é‡Œè¿›è¡Œç®€åŒ–ï¼Œå®é™…ä¸Šçš„ä»£ç æ¯”è¾ƒç¹ç
actor_rollout_wg = RayWorkerGroup(resource_pool, actor_rollout_cls)
critic_wg = RayWorkerGroup(resource_pool, critic_cls)
ref_policy_wg = RayWorkerGroup(resource_pool, ref_policy_cls)
```

<details>
<summary>å„ç§ worker group å®é™…ä¸Šçš„åˆå§‹åŒ–</summary>

è¿™éƒ¨åˆ†ä»£ç åœ¨ [`ray_trainer.py`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L771) ä¸­ï¼š

```python
# 1. ä¸ºæ¯ä¸ªè§’è‰²ï¼ˆä¾‹å¦‚ actor_rolloutã€criticã€refï¼‰æŒ‡å®šç”¨å“ªä¸ªç±»åˆå§‹åŒ– workerï¼Œå¹¶ä¸”è¯´æ˜åœ¨å“ªä¸ªèµ„æºæ± é‡Œåˆ†é…å®ƒä»¬
    
self.resource_pool_manager.create_resource_pool()
self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()}
resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)
    actor_rollout_cls = RayClassWithInitArgs(
        cls=self.role_worker_mapping[Role.ActorRollout],
        config=self.config.actor_rollout_ref,
        role="actor_rollout",
    )
self.resource_pool_to_cls[resource_pool]["actor_rollout"] = actor_rollout_cls
    
# 2. æ ¹æ®èµ„æºæ± å’Œè§’è‰²ï¼Œæ‰¹é‡åˆ›å»ºå¤šä¸ª worker å®ä¾‹ï¼ˆRay Actorï¼‰å¹¶ç»Ÿä¸€ç®¡ç†å®ƒä»¬ï¼Œèµ‹äºˆå¯¹åº”çš„èŒè´£
for resource_pool, class_dict in self.resource_pool_to_cls.items():
    worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)
    wg_dict = self.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls, device_name=self.device_name, **wg_kwargs)
    spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())
    all_wg.update(spawn_wg)
    
# 3.è°ƒç”¨ init_model() å®Œæˆæ¨¡å‹åŠ è½½
if self.use_critic:
    self.critic_wg = all_wg["critic"]
    self.critic_wg.init_model()

if self.use_reference_policy and not self.ref_in_actor:
    self.ref_policy_wg = all_wg["ref"]
    self.ref_policy_wg.init_model()

if self.use_rm:
    self.rm_wg = all_wg["rm"]
    self.rm_wg.init_model()

# we should create rollout at the end so that vllm can have a better estimation of kv cache memory
self.actor_rollout_wg = all_wg["actor_rollout"]
self.actor_rollout_wg.init_model()

# create async rollout manager and request scheduler
self.async_rollout_mode = False
if self.config.actor_rollout_ref.rollout.mode == "async":
    from verl.workers.rollout.async_server import AsyncLLMServerManager

    self.async_rollout_mode = True
    self.async_rollout_manager = AsyncLLMServerManager(
        config=self.config,
        worker_group=self.actor_rollout_wg,
    )
```

æ³¨æ„åˆ° `ray_worker_group_cls` å°±æ˜¯ `RayWorkerGroup` ç±»ï¼Œè€Œ `worker_dict_cls` å°±æ˜¯ `ActorRolloutRefWorker` ç±»ï¼Œæ‰€ä»¥æˆ‘çš„ç®€åŒ–æ˜¯å¾ˆåˆç†çš„ã€‚

</details>

å¦‚æ­¤ä»¥æ¥ï¼Œ`ActorRolloutRefWorker` å§”æ‰˜ç»™ `RayWorkerGroup` è¿›è¡Œåˆå§‹åŒ–ã€‚`RayWorkerGroup` è¿™ä¸ªç±»å°±æ˜¯ä¸“é—¨ç”¨äºèµ„æºè°ƒåº¦çš„ã€‚é€šè¿‡å…¶ç»Ÿä¸€çš„ `_init_with_resource_pool` [æ–¹æ³•](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/ray/base.py#L313)ï¼Œä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª workerï¼Œæœ€ç»ˆå®ä¾‹åŒ–æ¯ç§ RL worker å¹¶åˆ†é…èµ„æºã€‚

```python
def _init_with_resource_pool(self, resource_pool, ray_cls_with_init, ...):
    # ä» Ray ç”³è¯· Placement Groups
    pgs = resource_pool.get_placement_groups(strategy=strategy, device_name=self.device_name)
    
    # ä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª worker
    for local_rank in range(local_world_size):
        worker = ray_cls_with_init(placement_group=pg, placement_group_bundle_idx=local_rank, ...)
        self._workers.append(worker)
```

è¯»åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬åŸºæœ¬å¯¹ verl æœ‰äº†ä¸€äº›æ„Ÿè§‰ã€‚æ³¨æ„åˆ°ï¼Œåœ¨ verl å½“ä¸­æœ‰ä¸¤ä¸ªå¸¦æœ‰ Worker çš„ base classï¼Œä¸€ä¸ªå°±å«åš [`Worker`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/base/worker.py#L77)ï¼Œå¦ä¸€ä¸ªå«åš [`WorkerGroup`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/base/worker_group.py#L121)ã€‚`Worker` æ˜¯ RL é‡Œé¢çš„é€»è¾‘ç±»ï¼ˆæ¯”å¦‚ actor å’Œ criticï¼‰,å®é™…ç®¡ç† RL çš„æ•°æ®æµï¼Œè€Œ `WorkerGroup` åªç”¨äºåˆ†å¸ƒå¼ç³»ç»Ÿçš„èµ„æºè°ƒåº¦ã€‚


æ­¤å¤–ï¼Œä» `actor_rollout_wg` å’Œ `ref_policy_wg` çš„å®ä¾‹åŒ–å½“ä¸­ï¼Œä¹Ÿèƒ½çœ‹å‡ºä¸€äº›å­¦é—®ã€‚åœ¨ `ActorRolloutRefWorker` çš„è®¾è®¡å½“ä¸­ï¼ŒActor Trainingï¼ŒActor Rollout å’Œ Reference model æ˜¯ç”¨åŒä¸€ä¸ª worker class è¿›è¡Œç®¡ç†çš„ã€‚ä½†æ˜¯ï¼Œä¹‹åå§”æ‰˜ç»™ `RayWorkerGroup` åˆ›å»º worker group å¹¶ä¸”è°ƒç”¨èµ„æºçš„æ—¶å€™ï¼ŒActor Training å’Œ Actor Rollout æ˜¯ç”±åŒä¸€ç»„ `RayWorkerGroup` è¿›è¡Œèµ„æºç®¡ç†çš„ï¼ˆè¿™äºŒè€…æœ¬æ¥å°±è¦è¢«æ”¾åœ¨åŒä¸€ä¸ªèµ„æºç»„ä¸Šåš hybird engineï¼‰ï¼Œè€Œ Reference Model æ˜¯ç”±å¦ä¸€ç»„ `RayWorkerGroup` ç®¡ç†èµ„æºçš„ã€‚

æœ€åï¼Œæˆ‘å»é—®äº†ç›¸å…³å¼€å‘è€…ï¼Œä»–ä»¬ä¹Ÿè®¤ä¸ºæŠŠ Actor Rolloutï¼ŒActor Training å’Œ Reference Model æ”¾åœ¨åŒä¸€ä¸ª worker é‡Œæ˜¯ bad design ğŸ˜‚ï¼Œä¸ç”¨çº ç»“è¿™ç§è®¾è®¡æ˜¯å¦æœ‰ä»€ä¹ˆé«˜ç»è¿œç©ï¼Œå®Œå…¨æ²¡æœ‰ã€‚

### [`ActorRolloutRefWorker.__init__()`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/workers/fsdp_workers.py#L101)

å¦‚å‰æ–‡æ‰€è¯´ï¼Œ`ActorRolloutRefWorker` æ˜¯ verl ä¸­ç”¨äºç®¡ç† Actor Trainingï¼ŒActor Rollout å’Œ Reference Model çš„ worker classã€‚æˆ‘ä»¬å…·ä½“æ¥åˆ†æå…¶é€»è¾‘ä¸Šå®ç°çš„åŠŸèƒ½ã€‚æ³¨æ„ï¼Œæœ¬æ–‡æ¡£åªåˆ†æ FSDP backend ä¸‹çš„å®ç°ï¼Œmegatron ç•™ä½œåæ–‡ã€‚

1. è°ƒç”¨ Worker åŸºç±»çš„æ„é€ å‡½æ•°ï¼Œå¹¶ä¿å­˜é…ç½®ã€‚
2. å¦‚æœ PyTorch åˆ†å¸ƒå¼ç¯å¢ƒå°šæœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–ï¼ŒåŒ…æ‹¬è®¾ç½®é€šä¿¡åç«¯å’Œè¿›ç¨‹ç»„ã€‚
3. ä¸º FSDP åˆ›å»ºè®¾å¤‡ç½‘æ ¼ï¼Œç”¨äºæ¨¡å‹å‚æ•°çš„åˆ†ç‰‡ã€‚
4. å¦‚æœå¯ç”¨ Ulysses åºåˆ—å¹¶è¡Œï¼Œåˆ™åˆå§‹åŒ–å…¶è®¾å¤‡ç½‘æ ¼ã€‚
5. æ ¹æ®ä¼ å…¥çš„ `role` å‚æ•°è®¾ç½® Worker çš„å…·ä½“è§’è‰²ï¼ˆactor, rollout, refï¼‰ã€‚
6. æ ¹æ® Worker è§’è‰²é…ç½® profilerï¼Œç”¨äºæ€§èƒ½åˆ†æã€‚
7. é…ç½® parameter offload å’Œ optimizer offloadã€‚
8. ä¸º Actorï¼ŒRollout å’Œ Reference åˆ†åˆ« normalize batch sizeã€‚

ç¬¬ 8 æ­¥ä¸­é…ç½®äº†éå¸¸å¤šçš„ batch sizeï¼›verl çš„ batch size å‚æ•°æ»¡å¤©é£ï¼Œè™½ç„¶æˆ‘ä¸ªäººè®¤ä¸ºåå­—åŸºæœ¬æ˜¯å‡†ç¡®çš„ï¼Œä½†æ˜¯ç”±äºåå­—å¤ªåƒäº†ï¼Œä¸€å®šè¦åšå‡ºä¸€äº›åŒºåˆ†ã€‚äº‹å®ä¸Šï¼Œå‚æ•°åˆ†ææˆ‘ä»¬æœ‰å•ç‹¬çš„æ–‡æ¡£ï¼Œæˆ‘å…ˆæŠŠä¸€éƒ¨åˆ†å†…å®¹æå‰å…¬å¸ƒäº†ã€‚

1. `data.train_batch_size`ï¼šåœ¨ä¸€æ¬¡å®Œæ•´çš„ PPO è¿­ä»£ï¼ˆä» rollout åˆ° trainï¼‰ä¸­ï¼Œä»æ•°æ®é›†ä¸­é‡‡æ ·å¹¶ç”¨äºç”Ÿæˆ experience çš„æ€»æ ·æœ¬æ•°é‡ï¼Œå†³å®šäº†æ¯æ¬¡ policy æ›´æ–°æ‰€ä¾æ®çš„æ•°æ®é‡ã€‚
2. `actor_rollout_ref.actor.ppo_mini_batch_size`ï¼šè¿™ä¸ªå‚æ•°çš„åå­—å…¶å®æ˜¯å‡†ç¡®çš„ï¼Œå› ä¸º mini batch SGD å°±æ˜¯æ•°æ®åˆ°è¾¾äº†ä¸€ä¸ª mini batch å°±æ›´æ–°ä¸€æ¬¡æ¨¡å‹å‚æ•°ã€‚åœ¨ verl ä¸­ï¼Œæ¨¡å‹ä¼šåœ¨æ•°æ®ç´¯ç§¯åˆ°ä¸€ä¸ª mini batch åæ›´æ–°ä¸€æ¬¡å‚æ•°ã€‚
3. `actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu`ï¼šè¿™é‡Œå…¶å®æ˜¯ gradient accumulation çš„å‚æ•°ã€‚ç”±äºä¸€ä¸ª mini batch çš„æ•°æ®é‡å¯èƒ½ä»ç„¶å¤ªå¤§ï¼Œæ— æ³•ä¸€æ¬¡æ€§å‰å‘å’Œåå‘ä¼ æ’­ï¼Œå› æ­¤éœ€è¦å°†å…¶è¿›ä¸€æ­¥æ‹†åˆ†ä¸º micro batchã€‚æ¯ä¸ª micro batch ä¼šè®¡ç®—ä¸€æ¬¡æ¢¯åº¦å¹¶ä¸”ç´¯è®¡ï¼Œä½†æ˜¯ä¸ä¼šç«‹åˆ»æ›´æ–°æ¨¡å‹å‚æ•°ã€‚å¤„ç†å®Œæ•´ä¸ª mini batch åï¼Œæ‰ç”¨ç´¯ç§¯çš„æ¢¯åº¦è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚

æ­¤å¤–ï¼Œåœ¨ verl ä¸­ï¼Œç”±äº verl å¼ºè°ƒ SPMD ç­–ç•¥ï¼Œå¯ä»¥ç†è§£ä¸ºæ¯ä¸ª RL worker æ‰€å æ®çš„æ¯ä¸ª GPU ä¸Šå¸Œæœ›è¿›è¡Œå®Œå…¨ä¸€è‡´çš„æ“ä½œï¼Œæ‰€ä»¥ verl ä¼šè¦æ±‚æ¯ä¸ª GPU çš„ micro batch size ç›¸åŒã€‚å› æ­¤ï¼Œverl ä¼šæ£€æŸ¥ train batch size / gpu æ˜¯å¦æ•´é™¤ [(ref)](https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/trainer/ppo/ray_trainer.py#L363)ï¼Œå¦‚æœä¸æ•´é™¤ï¼Œåˆ™æŠ¥é”™ã€‚è¿™ä¸ªè®¾å®šå…¶å®å®Œå…¨æ²¡å¿…è¦ï¼›å¯¹äº rollout è€Œè¨€ï¼ŒSGLang å®Œå…¨ä¸éœ€è¦å‘é€çš„è¯·æ±‚æ•°é‡æ•´é™¤ DP æˆ–è€… TP sizeï¼Œæ›´ä½•å†µç›´æ¥è¦æ•´é™¤ gpu æ•°é‡å‘¢ï¼Ÿä½†æ˜¯ï¼Œå› ä¸º verl ä¼šç”¨ all gather ä» rollout çš„æ¯ä¸ª worker é‡Œæ”¶é›†æ•°æ®ï¼Œè¿™å°±è¦æ±‚ rollout çš„æ¯ä¸ª worker ä¸Šåˆ†åˆ°çš„æ•°æ®ä¸€è‡´ã€‚æ›´è¿›ä¸€æ­¥ï¼Œä¸ºäº† SPMDï¼Œåˆè¦æ±‚ rollout çš„æ¯ä¸ª gpu ä¸Šåˆ†åˆ°çš„æ•°æ®ä¸€è‡´ã€‚æœ€ç»ˆï¼Œè¿™å°±å¯¼è‡´ verl çš„ train batch size å¿…é¡»æ•´é™¤ gpu æ•°é‡ï¼›åœ¨ GRPO ä¸‹æ˜¯ real train batch size éœ€è¦æ•´é™¤ n gpusï¼Œç­‰äº train batch size * sampling params ä¸­çš„ nã€‚

åŒºåˆ†å¥½ mini batch å’Œ micro batch åï¼Œæˆ‘ä¹Ÿæ˜¯æœ€è¿‘æ‰æ˜ç™½ PPO ä¸­æ˜¯å¦‚ä½•ç»´æŠ¤ on policy çš„ã€‚æˆ‘ä¹‹å‰ä¸€ç›´ä»¥ä¸ºæˆ‘ä»¬éƒ½æ˜¯åœ¨åšä¸¥æ ¼ on policy çš„è®­ç»ƒï¼Œä½†æ˜¯ä¸€ä¸ª train batch size ä¸‹æœ‰å¥½å‡ ä¸ª mini batchï¼Œä¼¼ä¹ç¬¬ä¸€ä¸ª mini batch ç»“æŸä¹‹åï¼Œç›®æ ‡ç­–ç•¥ï¼ˆtarget policyï¼Œè¢«è®­ç»ƒçš„ policyï¼‰å’Œè¡Œä¸ºç­–ç•¥ï¼ˆbehavior policyï¼Œç”¨äºåœ¨ç¯å¢ƒä¸­é‡‡æ ·çš„ policyï¼‰å°±ä¸ä¸€è‡´äº†ã€‚ä¸€æ¬¡é‡‡æ ·ä¼šè®­ç»ƒå¾ˆå¤šä¸ª mini batchï¼Œä»ç¬¬ä¸€ä¸ª mini batch ç»“æŸå°±ä¸æ˜¯ on policy äº†ã€‚äº‹å®ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œæˆ‘ä»¬æ³¨æ„åˆ° PPO çš„ loss functionï¼š

$$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t) \right] $$

å…¶ä¸­çš„ $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹ä¼˜åŠ¿å‡½æ•°çš„çŸ«æ­£æ¯”ä¾‹ï¼Œè€Œ $\hat{A}_t$ å°±æ˜¯ advantageã€‚å¯¹äº LLM çš„ PPO è€Œè¨€ï¼Œ$\pi_{\theta_{old}}(a_t | s_t)$ ä»£è¡¨ç€é‡‡æ ·æ—¶ behavior policy åœ¨ç»™å®š $s_t$ æ—¶ï¼Œé€‰æ‹© $a_t$ çš„æ¦‚ç‡ï¼Œè€Œ $\pi_\theta(a_t | s_t)$ å°±æ˜¯ target policy åœ¨è®­ç»ƒä¸­çš„æ¯ä¸€æ­¥ç»™å®š $s_t$ æ—¶ï¼Œé€‰æ‹© $a_t$ çš„æ¦‚ç‡ã€‚å¯¹ LLM è€Œè¨€ï¼Œ`s_t` æ˜¯ prompt å‰ç¼€ï¼Œè€Œ `a_t` ä»…ä»…æ˜¯ prompt åçš„é‚£ä¸€ä¸ª tokenã€‚è¿™ä¸€æ¦‚ç‡å…¶å®å°±æ˜¯ inference å¾—åˆ°çš„ log probsï¼›æˆ‘ä»¬å°†æ”¶é›†å¾—åˆ°çš„ (prompt, action) åˆ†åˆ«ç»è¿‡ target policy å’Œ behaviour policy å¾—åˆ° log probsï¼Œç„¶åäºŒè€… log probs ç›¸å‡å†å–å¯¹æ•°ï¼Œå°±æ˜¯çŸ«æ­£é¡¹çš„å€¼ã€‚ä»è€Œï¼Œå³ä¾¿ç¬¬ä¸€ä¸ª mini batch ä¹‹å target policy å°±å·²ç»å’Œ behaviour policy ä¸ä¸€è‡´äº†ï¼Œä»ç„¶å¯ä»¥é€šè¿‡ log probs è¿›è¡ŒçŸ«æ­£ï¼Œä¹Ÿå³ importance samplingã€‚

è¿™æ ·ä¸€æ¥ï¼Œåˆæœ‰äº†ä¸¤ä¸ªé—®é¢˜ï¼šlog probs åº”è¯¥å¦‚ä½•å¾—åˆ°ï¼Ÿå®é™…ä¸Šæ¯æ¬¡é‡‡æ ·æ—¶éƒ½æ˜¯å‘é€ç»™ rollout å›ºå®šæ•°é‡çš„ requestsï¼Œå¦‚æœæ¯ä¸ª (prompt, action) å¯¹éƒ½ä¼šè®¡ç®—ä¸€æ¬¡ loss çš„è¯ï¼Œå²‚ä¸æ˜¯æ›´é•¿çš„ sequence ä¼šè®¡ç®—æ›´å¤šæ¬¡ï¼Ÿ

å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œè¿™åˆæ˜¯ç»å…¸çš„[ç²¾åº¦é—®é¢˜](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md#introduction)ã€‚å¦‚åŒæˆ‘åœ¨é“¾æ¥åˆ°çš„æ–‡ç« ä¸­æ‰€è¯´çš„ï¼Œrollout engine ç›®å‰åªæœ‰é‡‡æ ·å¾—åˆ°çš„ token èƒ½ç”¨ï¼Œè€Œå¾—åˆ°çš„ log probs ä»¥åŠ reward ç²¾åº¦éƒ½ä¸å¤Ÿï¼Œä¸èƒ½ç”¨äºè®­ç»ƒã€‚behaviour policy å’Œ target policy ä¸ºäº†åš importance sampling æ‰€éœ€çš„ log probs éƒ½å¾—ç”¨ training engine é‡ç®—ã€‚ä¸è¿‡è¦ç®—èµ·æ¥ä¹Ÿä¸éº»çƒ¦ï¼Œåœ¨ç¬¬ä¸€ä¸ª mini batch å¯åŠ¨å‰ï¼Œè¿™æ—¶å€™ target behaviour æ˜¯ä¸€è‡´çš„ï¼Œé‡ç®— log probs å¹¶ä¸”å­˜ä¸‹æ¥å³å¯ã€‚

å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œçš„ç¡®å¦‚æ­¤ã€‚ä¸€æ¡å¾ˆé•¿çš„ prompt + answer åºåˆ—ç¡®å®ä¼šäº§ç”Ÿéå¸¸å¤šçš„ (prompt, action) å¯¹ï¼Œå…¶ä¸­æ¯ä¸ªå¯¹éƒ½å¯ä»¥çœ‹ä½œä¸€ä¸ª (state, action) å¯¹ã€‚è€Œä¸”ç†è®ºä¸Šæ¯ä¸ªè¿™æ ·çš„ (prompt, action) å¯¹éƒ½ä¼šå‚ä¸ Loss çš„è®¡ç®—ã€‚è¿™ç¡®å®å¯èƒ½å¯¼è‡´é•¿åºåˆ—ä¸­çš„ token ä¼šåœ¨ Loss è®¡ç®—ä¸­å æ®æ›´å¤§çš„æ¯”ä¾‹ï¼Œè®©æ¨¡å‹è¿‡åº¦å…³æ³¨é•¿åºåˆ—çš„ä¼˜åŒ–ï¼Œè€Œå¯¹çŸ­åºåˆ—çš„ä¼˜åŒ–ä¸è¶³ã€‚ä¸è¿‡ï¼Œverl çš„ rollout engine ä¼šè‡ªåŠ¨å¯¹æ¯ä¸ª (prompt, action) å¯¹è¿›è¡ŒåŠ æƒï¼Œä»è€Œè®©é•¿åºåˆ—å’ŒçŸ­åºåˆ—çš„ token åœ¨ Loss è®¡ç®—ä¸­å æ®ç›¸åŒçš„æƒé‡ã€‚ä¸ºäº†ç¼“è§£è¿™ç§æƒ…å†µï¼Œæœ‰å¾ˆå¤šç›¸å…³æ–¹æ³•ï¼š

<details>
<summary>æ ·æœ¬åŠ æƒæ–¹æ³•</summary>

åºåˆ—çº§åˆ«åŠ æƒï¼š ä¸€ç§ç›´æ¥çš„æ–¹æ³•æ˜¯åœ¨è®¡ç®— Loss æ—¶ï¼Œç»™æ¥è‡ªä¸åŒåºåˆ—çš„æ ·æœ¬èµ‹äºˆä¸åŒçš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œç»™æ¯ä¸ªå®Œæ•´åºåˆ—ä¸€ä¸ªå›ºå®šçš„æƒé‡ï¼ˆæ¯”å¦‚ 1ï¼‰ï¼Œç„¶åå°†è¿™ä¸ªæƒé‡å‡åŒ€åˆ†é…ç»™è¯¥åºåˆ—ä¸­çš„æ¯ä¸ª (prompt, action) å¯¹ã€‚è¿™æ ·ï¼Œæ— è®ºåºåˆ—å¤šé•¿ï¼Œå®ƒå¯¹æ€» Loss çš„è´¡çŒ®éƒ½ç›¸åŒã€‚å¦‚æœä¸€ä¸ªåºåˆ—æœ‰ N ä¸ª tokenï¼Œé‚£ä¹ˆæ¯ä¸ª (prompt, action) å¯¹çš„æƒé‡å°±æ˜¯ 1/Nã€‚

æŒ‰é•¿åº¦åˆ†æ¡¶ï¼š åœ¨æ•°æ®æ”¶é›†åï¼Œå¯ä»¥æ ¹æ®åºåˆ—é•¿åº¦å¯¹æ ·æœ¬è¿›è¡Œæ’åºï¼Œå¹¶å°è¯•å°†ç›¸ä¼¼é•¿åº¦çš„åºåˆ—æ”¾å…¥åŒä¸€ä¸ª mini-batchã€‚è¿™æœ‰åŠ©äºæé«˜è®¡ç®—æ•ˆç‡ï¼Œå› ä¸ºå¯ä»¥å‡å°‘ paddingï¼Œä½†å¯¹äºè§£å†³ Loss è´¡çŒ®ä¸å‡è¡¡çš„ä½œç”¨æœ‰é™ã€‚

å›ºå®š Token æ•°é‡çš„æ‰¹æ¬¡ï¼š æœ€å¸¸è§ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ˜¯æ„å»ºæ‰¹æ¬¡æ—¶ï¼Œä¸å›ºå®šæ ·æœ¬æ•°é‡ï¼Œè€Œæ˜¯å›ºå®šæ‰¹æ¬¡ä¸­çš„æ€» token æ•°é‡ã€‚è¿™æ ·ï¼Œä¸€ä¸ª mini-batch å¯èƒ½åŒ…å« 4 æ¡é•¿åºåˆ—ï¼Œä¹Ÿå¯èƒ½åŒ…å« 40 æ¡çŸ­åºåˆ—ï¼Œç¡®ä¿æ¯æ¬¡æ›´æ–°æ—¶å¤„ç†çš„æ€»è®¡ç®—é‡å’Œæ¢¯åº¦æ¥æºçš„æ€» token æ•°æ˜¯æ’å®šçš„ï¼Œä»è€Œç¼“è§£é•¿çŸ­åºåˆ—çš„ä¸å‡è¡¡é—®é¢˜ã€‚

Loss å½’ä¸€åŒ–ï¼šåœ¨è®¡ç®—æ¯ä¸ª mini-batch çš„ Loss æ—¶ï¼Œå¯ä»¥å°†å…¶é™¤ä»¥è¯¥ mini-batch ä¸­å®é™…çš„ token æ•°é‡ã€‚è¿™ç¡®ä¿äº† Loss å€¼ä¸ä¼šä»…ä»…å› ä¸ºæ‰¹æ¬¡ä¸­åŒ…å«äº†æ›´å¤š token è€Œå¢å¤§ï¼Œä»è€Œä¸ºä¸åŒå¤§å°çš„ mini-batchesï¼ˆå¦‚æœä¸æ˜¯æŒ‰å›ºå®š token æ•°æ„å»ºï¼‰æä¾›ä¸€ä¸ªå…¬å¹³çš„æ¯”è¾ƒåŸºç¡€ã€‚

æˆªæ–­ï¼šè®¾å®šä¸€ä¸ª max_length å‚æ•°ï¼Œé™åˆ¶æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚è™½ç„¶è¿™ä¸ç›´æ¥è§£å†³å·²æœ‰é•¿åºåˆ—çš„æƒé‡é—®é¢˜ï¼Œä½†å¯ä»¥é˜²æ­¢ç”Ÿæˆè¿‡é•¿çš„åºåˆ—ï¼Œä»è€Œé™åˆ¶æç«¯ä¸å‡è¡¡çš„å‘ç”Ÿã€‚

</details>

whateverï¼Œè§£é‡Šäº†è¿™ä¹ˆå¤šï¼Œé¡ºç€ç†è§£ verl çš„æ¡†æ¶è¿›ä¸€æ­¥å­¦ä¹ äº† RL ç®—æ³•å’Œç³»ç»Ÿï¼Œè¿™é‡Œå…¶å®å’Œ multi-turn éƒ½è¿˜æ²¡æœ‰å…³ç³»ï¼Œæˆ‘ä»¬è¿˜æ˜¯å›åˆ° `ActorRolloutRefWorker` çš„æºç ä¸Šã€‚

<details>
<summary> ActorRolloutRefWorker.__init__ æºç  </summary>

```python
def __init__(self, config: DictConfig, role: str):
        # åˆå§‹åŒ– Worker åŸºç±»
        Worker.__init__(self)

        # å­˜å‚¨é…ç½®ä¿¡æ¯
        self.config = config
        import torch.distributed

        # å¦‚æœåˆ†å¸ƒå¼ç¯å¢ƒå°šæœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–
        if not torch.distributed.is_initialized():
            rank = int(os.environ.get("RANK", 0))
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            torch.distributed.init_process_group(backend=f"cpu:gloo,{get_device_name()}:{get_nccl_backend()}", rank=rank, world_size=world_size)

        # ä¸º FSDP æ„å»ºè®¾å¤‡ç½‘æ ¼
        world_size = torch.distributed.get_world_size()
        self.device_mesh = create_device_mesh(world_size=world_size, fsdp_size=self.config.actor.fsdp_config.fsdp_size)

        # ä¸º Ulysses åºåˆ—å¹¶è¡Œæ„å»ºè®¾å¤‡ç½‘æ ¼
        self.ulysses_device_mesh = None
        self.ulysses_sequence_parallel_size = self.config.actor.get("ulysses_sequence_parallel_size", 1)
        dp = world_size // self.ulysses_sequence_parallel_size
        if self.ulysses_sequence_parallel_size > 1:
            self.ulysses_device_mesh = init_device_mesh(device_name, mesh_shape=(dp, self.ulysses_sequence_parallel_size), mesh_dim_names=["dp", "sp"])

        # åˆå§‹åŒ– Ulysses åˆ†ç‰‡ç®¡ç†å™¨
        self.ulysses_sharding_manager = FSDPUlyssesShardingManager(self.ulysses_device_mesh)
        # è·å– LoRA rank å’Œæ˜¯å¦ä½¿ç”¨ LoRA çš„æ ‡å¿—
        self._lora_rank = self.config.model.get("lora_rank", 0)
        self._is_lora = self._lora_rank > 0

        # è®¾ç½® Worker è§’è‰²å’Œç›¸å…³æ ‡å¿—
        self.role = role
        assert self.role in ["actor", "rollout", "ref", "actor_rollout", "actor_rollout_ref"]

        self._is_actor = self.role in ["actor", "actor_rollout", "actor_rollout_ref"]
        self._is_rollout = self.role in ["rollout", "actor_rollout", "actor_rollout_ref"]
        self._is_ref = self.role in ["ref", "actor_rollout_ref"]

        profiler_config: Optional[ProfilerConfig] = None
        # æ ¹æ®è§’è‰²è·å–æ€§èƒ½åˆ†æé…ç½®
        if self._is_actor:
            profiler_config = omega_conf_to_dataclass(config.actor.get("profiler", {}), ProfilerConfig)
        if self._is_rollout:
            profiler_config = omega_conf_to_dataclass(config.rollout.get("profiler", {}), ProfilerConfig)
        if self._is_ref:
            profiler_config = omega_conf_to_dataclass(config.ref.get("profiler", {}), ProfilerConfig)

        # åˆå§‹åŒ–åˆ†å¸ƒå¼æ€§èƒ½åˆ†æå™¨
        DistProfilerExtension.__init__(self, DistProfiler(rank=self.rank, config=profiler_config))

        # è®¾ç½®å‚æ•°å’Œä¼˜åŒ–å™¨å¸è½½æ ‡å¿—
        self._is_offload_param = False
        self._is_offload_optimizer = False
        if self._is_actor:
            self._is_offload_param = self.config.actor.fsdp_config.get("param_offload", False)
            self._is_offload_optimizer = self.config.actor.fsdp_config.get("optimizer_offload", False)
        elif self._is_ref:
            self._is_offload_param = self.config.ref.fsdp_config.get("param_offload", False)

        # è§„èŒƒåŒ– actor ç›¸å…³é…ç½®
        if self._is_actor:
            self.config.actor.ppo_mini_batch_size *= self.config.rollout.n
            self.config.actor.ppo_mini_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size
            assert self.config.actor.ppo_mini_batch_size > 0, f"ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be larger than 0 after normalization"
            # micro bsz
            if self.config.actor.ppo_micro_batch_size is not None:
                self.config.actor.ppo_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size
                self.config.actor.ppo_micro_batch_size_per_gpu = self.config.actor.ppo_micro_batch_size

            if self.config.actor.ppo_micro_batch_size_per_gpu is not None:
                assert self.config.actor.ppo_mini_batch_size % self.config.actor.ppo_micro_batch_size_per_gpu == 0, f"normalized ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be divisible by ppo_micro_batch_size_per_gpu {self.config.actor.ppo_micro_batch_size_per_gpu}"
                assert self.config.actor.ppo_mini_batch_size // self.config.actor.ppo_micro_batch_size_per_gpu > 0, f"normalized ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be larger than ppo_micro_batch_size_per_gpu {self.config.actor.ppo_micro_batch_size_per_gpu}"

        # è§„èŒƒåŒ– rollout ç›¸å…³é…ç½®
        if self._is_rollout and self.config.rollout.log_prob_micro_batch_size is not None:
            self.config.rollout.log_prob_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size
            self.config.rollout.log_prob_micro_batch_size_per_gpu = self.config.rollout.log_prob_micro_batch_size
        # è§„èŒƒåŒ– ref ç›¸å…³é…ç½®
        if self._is_ref and self.config.ref.log_prob_micro_batch_size is not None:
            self.config.ref.log_prob_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size
            self.config.ref.log_prob_micro_batch_size_per_gpu = self.config.ref.log_prob_micro_batch_size
```

</details>

### [`ActorRolloutRefWorker._build_model_optimizer()`](https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L177)

è¿™éƒ¨åˆ†æºç å’Œç±»å†™çš„è¿˜æ˜¯å¾ˆç›´ç™½çš„ï¼Œä¸ç”¨å¤ªå¤šè§£é‡Šï¼š

1. åˆå§‹åŒ– Hugging Face é…ç½®ï¼Œè·å– Generation Configï¼Œå¹¶è®¾ç½®æ¨¡å‹çš„æ•°æ®ç±»å‹ï¼ˆActor ä½¿ç”¨ fp32ï¼ŒReference ä½¿ç”¨ bf16ï¼‰ã€‚
2. ä½¿ç”¨ Hugging Face çš„ `AutoModelForCausalLM` æˆ– `AutoModelForVision2Seq` ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åŸºç¡€æ¨¡å‹ã€‚
3. åº”ç”¨å„ç§ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬ Liger kernelã€èåˆ kernelã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€LoRA ç­‰ã€‚
4. æ ¹æ®é…ç½®é€‰æ‹© FSDP æˆ– FSDP2 ç­–ç•¥ï¼Œå°†æ¨¡å‹å°è£…åˆ°åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ä¸­ï¼Œæ”¯æŒå‚æ•°åˆ†ç‰‡å’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚
5. å¦‚æœå½“å‰ Worker æ˜¯ Actor è§’è‰²ï¼Œåˆ™åˆå§‹åŒ– AdamW ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

<details>
<summary> ActorRolloutRefWorker._build_model_optimizer æºç  </summary>

```python
def _build_model_optimizer(
        self,
        model_path,
        fsdp_config,
        optim_config,
        override_model_config,
        use_remove_padding=False,
        use_fused_kernels=False,
        enable_gradient_checkpointing=False,
        trust_remote_code=False,
        use_liger=False,
        role="actor",
        enable_activation_offload=False,
    ):
        from torch import optim
        from torch.distributed.fsdp import CPUOffload, MixedPrecision
        from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForVision2Seq
        from verl.utils.model import get_generation_config, print_model_size, update_model_config
        from verl.utils.torch_dtypes import PrecisionType

        assert role in ["actor", "ref"]

        log_gpu_memory_usage(f"Before init {role} from HF AutoModel", logger=logger)
        local_path = model_path

        self.tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)
        self.processor = hf_processor(local_path, trust_remote_code=trust_remote_code)

        torch_dtype = fsdp_config.get("model_dtype", None)
        if torch_dtype is None:
            torch_dtype = torch.float32 if self._is_actor else torch.bfloat16
        else:
            torch_dtype = PrecisionType.to_dtype(torch_dtype)

        actor_model_config = AutoConfig.from_pretrained(local_path, trust_remote_code=trust_remote_code, attn_implementation="flash_attention_2")

        if getattr(actor_model_config, "model_type", None) == "kimi_vl":
            actor_model_config.text_config.topk_method = "greedy"

        self.generation_config = get_generation_config(local_path, trust_remote_code=trust_remote_code)

        override_config_kwargs = {
            "bos_token_id": self.tokenizer.bos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": self.tokenizer.pad_token_id,
        }
        override_config_kwargs.update(override_model_config)
        update_model_config(actor_model_config, override_config_kwargs=override_config_kwargs)
        # å¦‚æœæ˜¯ rank 0 è¿›ç¨‹ï¼Œæ‰“å°æ›´æ–°åçš„æ¨¡å‹é…ç½®
        if self.rank == 0:
            print(f"Model config after override: {actor_model_config}")

        init_context = get_init_weight_context_manager(use_meta_tensor=not actor_model_config.tie_word_embeddings, mesh=self.device_mesh)

        with init_context(), warnings.catch_warnings():
            warnings.simplefilter("ignore")
            if type(actor_model_config) in AutoModelForVision2Seq._model_mapping.keys():
                actor_module_class = AutoModelForVision2Seq
            else:
                actor_module_class = AutoModelForCausalLM

            actor_module = actor_module_class.from_pretrained(
                pretrained_model_name_or_path=local_path,
                torch_dtype=torch_dtype,
                config=actor_model_config,
                trust_remote_code=trust_remote_code,
            )

            if use_liger:
                from liger_kernel.transformers.monkey_patch import _apply_liger_kernel_to_instance

                _apply_liger_kernel_to_instance(model=actor_module)

            fused_kernel_options = self.config.model.get("fused_kernel_options", None)
            fused_kernels_backend = fused_kernel_options.get("impl_backend", None) if fused_kernel_options is not None else None

            apply_monkey_patch(
                model=actor_module,
                use_remove_padding=use_remove_padding,
                ulysses_sp_size=self.ulysses_sequence_parallel_size,
                use_fused_kernels=use_fused_kernels,
                fused_kernels_backend=fused_kernels_backend,
            )

            actor_module.to(torch_dtype)

            if enable_gradient_checkpointing:
                actor_module.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})
            if self._is_lora:
                print("Applying LoRA to actor module")
                actor_module.enable_input_require_grads()
                lora_config = {"task_type": TaskType.CAUSAL_LM, "r": self.config.model.lora_rank, "lora_alpha": self.config.model.lora_alpha, "target_modules": convert_to_regular_types(self.config.model.target_modules), "bias": "none"}
                actor_module = get_peft_model(actor_module, LoraConfig(**lora_config))
        torch.distributed.barrier()

        if self.rank == 0:
            print_model_size(actor_module)

        log_gpu_memory_usage(f"After init {role} from HF AutoModel", logger=logger)

        mixed_precision_config = fsdp_config.get("mixed_precision", None)
        if mixed_precision_config is not None:
            param_dtype = PrecisionType.to_dtype(mixed_precision_config.get("param_dtype", "bf16"))
            reduce_dtype = PrecisionType.to_dtype(mixed_precision_config.get("reduce_dtype", "fp32"))
            buffer_dtype = PrecisionType.to_dtype(mixed_precision_config.get("buffer_dtype", "fp32"))
        else:
            param_dtype = torch.bfloat16
            reduce_dtype = torch.float32
            buffer_dtype = torch.float32

        mixed_precision = MixedPrecision(param_dtype=param_dtype, reduce_dtype=reduce_dtype, buffer_dtype=buffer_dtype)

        auto_wrap_policy = get_fsdp_wrap_policy(module=actor_module, config=fsdp_config.get("wrap_policy", None), is_lora=self.config.model.get("lora_rank", 0) > 0)

        # TODO(zhangchi.usc1992, shengguangming) fix me. Current, auto_wrap_policy causes HFRollout to hang in Gemma
        if self._is_rollout and self.config.rollout.name == "hf":
            auto_wrap_policy = None

        # å¦‚æœæ˜¯ rank 0 è¿›ç¨‹ï¼Œæ‰“å°åŒ…è£…ç­–ç•¥
        if self.rank == 0:
            print(f"wrap_policy: {auto_wrap_policy}")

        fsdp_mesh = self.device_mesh
        sharding_strategy = get_sharding_strategy(fsdp_mesh)

        # TODO: æ·»åŠ  transformer ç­–ç•¥
        # æˆ‘ä»¬å¼ºåˆ¶ reference policy ä½¿ç”¨ CPUOffload æ¥èŠ‚çœå†…å­˜
        # æˆ‘ä»¬å¼ºåˆ¶å…³é—­ actor çš„ CPUOffloadï¼Œå› ä¸ºå®ƒåœ¨ä½¿ç”¨ grad accumulation æ—¶ä¼šå¯¼è‡´ä¸æ­£ç¡®çš„ç»“æœ
        cpu_offload = None if role == "actor" else CPUOffload(offload_params=True)
        # æ ¹æ®é…ç½®çš„ç­–ç•¥ï¼Œå°†æ¨¡å‹å°è£…åˆ° FSDP ä¸­
        fsdp_strategy = self.config.actor.strategy
        if fsdp_strategy == "fsdp":
            actor_module_fsdp = FSDP(
                actor_module,
                cpu_offload=cpu_offload,
                param_init_fn=init_fn,
                use_orig_params=False,
                auto_wrap_policy=auto_wrap_policy,
                device_id=get_device_id(),
                sharding_strategy=sharding_strategy,  # zero3
                mixed_precision=mixed_precision,
                sync_module_states=True,
                device_mesh=self.device_mesh,
                forward_prefetch=self.config.actor.fsdp_config.forward_prefetch,
            )
        elif fsdp_strategy == "fsdp2":
            assert CPUOffloadPolicy is not None, "PyTorch version >= 2.4 is required for using fully_shard API (FSDP2)"
            mp_policy = MixedPrecisionPolicy(param_dtype=param_dtype, reduce_dtype=reduce_dtype, cast_forward_inputs=True)
            if role == "actor" and fsdp_config.offload_policy:
                cpu_offload = CPUOffloadPolicy(pin_memory=True)
                self._is_offload_param = False
                self._is_offload_optimizer = False
            else:
                cpu_offload = None if role == "actor" else CPUOffloadPolicy(pin_memory=True)

            fsdp_kwargs = {
                "mesh": fsdp_mesh,
                "mp_policy": mp_policy,
                "offload_policy": cpu_offload,
                "reshard_after_forward": fsdp_config.reshard_after_forward,
            }
            full_state = actor_module.state_dict()
            apply_fsdp2(actor_module, fsdp_kwargs, fsdp_config)
            fsdp2_load_full_state_dict(actor_module, full_state, fsdp_mesh, cpu_offload)
            actor_module_fsdp = actor_module
        else:
            raise NotImplementedError(f"not implement {fsdp_strategy}")

        # å¦‚æœå¯ç”¨äº†æ¿€æ´»å¸è½½ï¼Œåˆ™å¯ç”¨å®ƒ
        if enable_activation_offload:
            enable_activation_offloading(actor_module_fsdp, fsdp_strategy, enable_gradient_checkpointing)

        # è®°å½• FSDP åˆå§‹åŒ–ä¹‹åçš„ GPU å†…å­˜ä½¿ç”¨æƒ…å†µ
        log_gpu_memory_usage(f"After {role} FSDP init", logger=logger)

        # TODO: add more optimizer args into config
        if role == "actor" and optim_config is not None:
            from verl.utils.torch_functional import get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup

            actor_optimizer = optim.AdamW(
                actor_module_fsdp.parameters(),
                lr=optim_config.lr,
                betas=optim_config.get("betas", (0.9, 0.999)),
                weight_decay=optim_config.get("weight_decay", 1e-2),
            )

            total_steps = optim_config.get("total_training_steps", 0)
            num_warmup_steps = int(optim_config.get("lr_warmup_steps", -1))
            warmup_style = optim_config.get("warmup_style", "constant")
            min_lr_ratio = optim_config.get("min_lr_ratio", 0.0)
            num_cycles = optim_config.get("num_cycles", 0.5)
            if num_warmup_steps < 0:
                num_warmup_steps_ratio = optim_config.get("lr_warmup_steps_ratio", 0.0)
                num_warmup_steps = int(num_warmup_steps_ratio * total_steps)

            if self.rank == 0:
                print(f"Total steps: {total_steps}, num_warmup_steps: {num_warmup_steps}")

            if warmup_style == "constant":
                actor_lr_scheduler = get_constant_schedule_with_warmup(optimizer=actor_optimizer, num_warmup_steps=num_warmup_steps)
            elif warmup_style == "cosine":
                actor_lr_scheduler = get_cosine_schedule_with_warmup(optimizer=actor_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps, min_lr_ratio=min_lr_ratio, num_cycles=num_cycles)
            else:
                raise NotImplementedError(f"Warmup style {warmup_style} is not supported")

            log_gpu_memory_usage(f"After {role} optimizer init", logger=logger)
        else:
            actor_optimizer = None
            actor_lr_scheduler = None

        return actor_module_fsdp, actor_optimizer, actor_lr_scheduler, actor_model_config
```

</details>

è¿™é‡Œä»£ç å¾ˆç›´ç™½ã€‚æœ‰ä¸€ä¸ªç‚¹å€¼å¾—å•ç‹¬æ‹å‡ºæ¥è®²ä¸€ä¸‹ï¼šä»”ç»†è§‚å¯Ÿ `actor_module` çš„ dtypeï¼Œç›´è§‰å‘Šè¯‰æˆ‘ï¼Œ`actor_module` çš„ dtype åº”è¯¥æ˜¯ bf16 çš„ï¼Œè€Œ gradient å’Œ optimizer çš„ dtype æ˜¯ fp32 çš„ã€‚å¯æ˜¯ `actor_module` çš„ default dtype è¢«è®¾ä¸ºäº† fp32ï¼Œç„¶åä» fp32 load äº†æ¨¡å‹ã€‚å®é™…ä¸Šè¿™æ˜¯å› ä¸º pytorch çš„å„ç§ optimizer éƒ½æ˜¯ç›´æ¥å’Œ parameter ç»‘å®šçš„ï¼Œç”¨ bf16 çš„ parameter åˆå§‹åŒ–çš„ optimizer ä¹Ÿæ˜¯ bf16ã€‚æ‰€ä»¥ model å…ˆ load äº† fp32ï¼Œç„¶ååˆå§‹åŒ– optimizer ä½œä¸ºæ··åˆç²¾åº¦ï¼Œæœ€åæŠŠ model è½¬æˆ bf16ã€‚

### [`ActorRolloutRefWorker._build_rollout()`](https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L394)

è¿™æ˜¯å¯¹æˆ‘è€Œè¨€æœ€æ¸…æ™°çš„åœ°æ–¹ï¼Œå®é™…ä¸Šä¹Ÿæ˜¯æœ€ç†Ÿæ‚‰çš„ã€‚åœ¨è¿™é‡Œç»ˆäºå¼•å…¥äº† SGLangï¼š

1.  **è®¾å¤‡ç½‘æ ¼åˆ›å»º**ï¼šä¸º Rollout åˆ›å»ºæ¨ç†å¼ é‡å¹¶è¡Œï¼ˆ`infer_tp`ï¼‰è®¾å¤‡ç½‘æ ¼ã€‚
2.  **SGLang Rollout æ„å»º**ï¼šå¯¼å…¥å¹¶å®ä¾‹åŒ– `SGLangRollout` å’Œ `FSDPSGLangShardingManager`ã€‚`FSDPSGLangShardingManager` è´Ÿè´£åœ¨ FSDP è®­ç»ƒæ ¼å¼å’Œ SGLang æ¨ç†æ ¼å¼ä¹‹é—´è½¬æ¢æ¨¡å‹æƒé‡ã€‚

<details>
<summary>ActorRolloutRefWorker._build_rollout éƒ¨åˆ†æºç </summary>

```python
def _build_rollout(self, trust_remote_code=False):
    from torch.distributed.device_mesh import init_device_mesh

    infer_tp = self.config.rollout.tensor_model_parallel_size
    dp = self.world_size // infer_tp
    assert self.world_size % infer_tp == 0, f"rollout world_size: {self.world_size} is not divisible by infer_tp: {infer_tp}"
    rollout_device_mesh = init_device_mesh(device_name, mesh_shape=(dp, infer_tp), mesh_dim_names=["dp", "infer_tp"])
    rollout_name = self.config.rollout.name

    if rollout_name in ["sglang", "sglang_async"]:
        if rollout_name == "sglang_async":
            warnings.warn(
                "'sglang_async' has been deprecated and merged into 'sglang'. Please use 'sglang' going forward.",
                DeprecationWarning,
                stacklevel=2,
            )
        from verl.workers.rollout.sglang_rollout import SGLangRollout
        from verl.workers.sharding_manager.fsdp_sglang import FSDPSGLangShardingManager

        local_path = copy_to_local(self.config.model.path)
        log_gpu_memory_usage(f"Before building {rollout_name} rollout", logger=logger)
        rollout = SGLangRollout(
            actor_module=local_path,
            config=self.config.rollout,
            tokenizer=self.tokenizer,
            model_hf_config=self.actor_model_config,
            trust_remote_code=trust_remote_code,
        )
        log_gpu_memory_usage(f"After building {rollout_name} rollout", logger=logger)

        if torch.distributed.get_world_size() == 1:
            self.config.rollout.load_format = "dummy_hf"
        rollout_sharding_manager = FSDPSGLangShardingManager(
            module=self.actor_module_fsdp,
            inference_engine=rollout._engine,
            model_config=self.actor_model_config,
            full_params="hf" in self.config.rollout.load_format,
            device_mesh=rollout_device_mesh,
            offload_param=self._is_offload_param,
        )
        log_gpu_memory_usage("After building sharding manager", logger=logger)

    else:
        raise NotImplementedError(f"Rollout name: {self.config.rollout.name} is not supported")

    return rollout, rollout_sharding_manager
```

</details>

### [`SGLangRollout.__init__()`](https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L208)

äº‹å·²è‡³æ­¤ï¼Œå†å¾€ä¸‹çœ‹ä¸€å±‚ SGLang å…·ä½“çš„åˆå§‹åŒ–ï¼š

1. è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°å¹¶è®¾ç½®é…ç½®å’Œè®¾å¤‡ç½‘æ ¼ã€‚
2. é€šè¿‡ `_initialize_tools()` åˆå§‹åŒ–å·¥å…· schemasã€map å’Œè§£æå™¨ï¼Œæ”¯æŒ Multi-turn å¯¹è¯ä¸­çš„å·¥å…·ä½¿ç”¨ã€‚
3. åˆå§‹åŒ– SGLang æ¨ç†æ‰€éœ€çš„åˆ†å¸ƒå¼ç¯å¢ƒã€‚
4. é€šè¿‡ `_verify_config()` éªŒè¯æ¨¡å‹é…ç½®ã€‚
5. é€šè¿‡ `_init_inference_engine()` åˆå§‹åŒ– SGLang æ¨ç†å¼•æ“ã€‚
6. é€šè¿‡ `_init_sampling_params()` åˆå§‹åŒ–ç”Ÿæˆåºåˆ—çš„é‡‡æ ·å‚æ•°ã€‚
7. è®¾ç½® Tokenizer å’Œ padding token IDã€‚

<details>
<summary>SGLangRollout.__init__ éƒ¨åˆ†æºç </summary>

```python
class SGLangRollout(BaseRollout):
    def __init__(
        self,
        actor_module: str,
        config: DictConfig,
        tokenizer,
        model_hf_config,
        port=None,
        trust_remote_code: bool = False,
        device_mesh: DeviceMesh | None = None,
        **kwargs,
    ):
        """Synchronized SGLang rollout engine.

        Args:
            actor_module: Huggingface model name or path to the model. The
                model should be supported by SGLang.
            config: A DictConfig object containing SGLang-specific operational
                parameters and rollout settings.
                Refer to https://docs.sglang.ai/backend/server_arguments.html
            tokenizer: The tokenizer instance compatible with the actor_module.
            model_hf_config: The Hugging Face model's configuration (e.g.,
                `transformers.PretrainedConfig`). It provides architectural
                details and hyperparameters like `max_position_embeddings`,
                used by SGLang for correct model initialization. This is
                the model's inherent design, not SGLang's runtime behavior.
            port: Optional port for multi-node initialization when nnodes > 1.
            trust_remote_code: Whether or not to allow for custom models
                defined on the Hub in their own modeling files.
            device_mesh: Optional `DeviceMesh` object for distributed setup.
            **kwargs: Additional keyword arguments, primarily `train_tp` for
                Megatron Backend integration to initialize hybrid engine
                process groups.
        """
        super().__init__()
        self.config = config
        self._device_mesh_cpu = device_mesh
        os.environ.setdefault("SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK", "true")

        (
            self._tool_schemas,
            self._tool_map,
            self._tool_call_parser_type,
            self._sgl_tools,
            self._function_call_parser,
        ) = self._initialize_tools(config, tokenizer)
        self.interaction: dict[str, BaseInteraction] = self._intitalize_interaction(config)
        # If turn on `free_cache_engine`, SGLang engine's KV cache
        # will be freed after each `generate_sequences` call.
        assert not (not config.enforce_eager and config.free_cache_engine), "disable CUDA graph (enforce_eager = False) if free cache engine"

        logger.info(f"tool_schemas: {self._tool_schemas}, tool_map: {self._tool_map}, tool_call_parser_type: {self._tool_call_parser_type}, sgl_tools: {self._sgl_tools}, function_call_parser: {self._function_call_parser}")

        self._init_distributed_env(device_mesh_cpu=device_mesh, **kwargs)

        self._verify_config(model_hf_config=model_hf_config)
        # initialize the inference engine
        self._init_inference_engine(trust_remote_code, actor_module, port)

        self._init_sampling_params(**kwargs)

        self.tokenizer = tokenizer
        self.pad_token_id = tokenizer.pad_token_id
```

</details>

ã€TODOã€‘ è¿™éƒ¨åˆ†æŒªåˆ°åé¢å»è§£é‡Šã€‚

### [`SGLangRollout._initialize_tools()`](https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L394)

`SGLangRollout._initialize_tools()` å‡½æ•°ç”¨äºåˆå§‹åŒ– Multi-turn å¯¹è¯ä¸­çš„å·¥å…·ã€‚

1. å¦‚æœæ²¡æœ‰å·¥å…·é…ç½®è·¯å¾„ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨å’Œå­—å…¸ã€‚
2. ä»é…ç½®æ–‡ä»¶åŠ è½½å·¥å…·å¹¶åˆå§‹åŒ–å·¥å…·åˆ—è¡¨ã€‚
3. åˆ›å»º OpenAI æ ¼å¼çš„å·¥å…· schema å’Œå·¥å…·åç§°åˆ°å·¥å…·å¯¹è±¡çš„æ˜ å°„ã€‚
4. æ ¹æ® Tokenizer ç±»å‹ç¡®å®šå·¥å…·è°ƒç”¨è§£æå™¨ã€‚
5. ä¸º SGLang åˆ›å»º `Tool` å¯¹è±¡ã€‚
6. å®ä¾‹åŒ– `FunctionCallParser`ã€‚

<details>
<summary>SGLangRollout._initialize_tools éƒ¨åˆ†æºç </summary>

```python
from sglang.function_calling.function_call_parser import FunctionCallParser
from sglang.utils.general import initialize_tools_from_config
from sglang.tools.tool import Tool
from omegaconf import OmegaConf

@registry.register(SGLangRollout)
    def _initialize_tools(self, config, tokenizer):
        """Initialize tools from configuration.

        Args:
            config: Configuration object containing tool-related settings,
                    specifically `config.multi_turn.tool_config_path`.
            tokenizer: The tokenizer instance used for parsing tool calls from
                       the model's generated text.

        Returns:
            tuple: A tuple containing:
                - tool_schemas (list[dict]): OpenAI-formatted JSON schemas
                  defining each tool's capabilities.
                - tool_map (dict[str, BaseTool]): A dictionary mapping tool
                  names to their executable `BaseTool` objects.
                - tool_call_parser_type (str): The identifier for the specific
                  parser type (e.g., 'json_mode', 'tool_code') used to extract
                  tool calls.
                - sgl_tools (list[sglang.srt.openai_api.protocol.Tool]): Tool
                  definitions optimized for SGLang's internal engine.
                - function_call_parser (sglang.srt.function_call_parser.FunctionCallParser):
                  The active parser instance responsible for extracting
                  structured tool calls from model outputs.
        """
        if config.multi_turn.tool_config_path is None:
            return [], {}, None, [], None

        tools_config_file = config.multi_turn.tool_config_path
        tool_list = initialize_tools_from_config(tools_config_file)

        logger.info(f"Initialize tools from configuration.: tool_list: {tool_list}")
        tool_schemas = [tool.get_openai_tool_schema().model_dump() for tool in tool_list]
        tool_map = {tool.name: tool for tool in tool_list}
        tool_call_parser_type = get_tool_call_parser_type(tokenizer)
        sgl_tools = [Tool.model_validate(tool_schema) for tool_schema in tool_schemas]
        function_call_parser = FunctionCallParser(
            sgl_tools,
            tool_call_parser_type,
        )

        return (
            tool_schemas,
            tool_map,
            tool_call_parser_type,
            sgl_tools,
            function_call_parser,
        )
```

</details>

ã€TODOã€‘æŒªåˆ° part 2ã€‚

### [`SGLangRollout.AsyncEngine`](https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L124)

å…³äº `SGLangRollout` è°ƒç”¨ tool çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬åœ¨ä¸‹æ–‡çš„è®­ç»ƒå¾ªç¯ä¸­å†å±•å¼€ï¼Œè¿™é‡Œå…ˆè®¨è®ºå®Œ SGLang çš„åˆå§‹åŒ–ã€‚ä¸ºäº†è°ƒç”¨ SGLang engine çš„æ¥å£ï¼Œverl è¿›è¡Œäº†ä¸€å±‚å°è£…ï¼Œå®ç°äº†æˆ‘ä»¬å¯¹ SGLang é™¤å¼€ rollout ä¹‹å¤–çš„æ‰€æœ‰æ¥å£ï¼š

1. release and resume memory occupationï¼šåœ¨è®­ç»ƒæ—¶é‡Šæ”¾æ‰æ˜¾å­˜å ç”¨å¹¶åœ¨è®­ç»ƒåæ¢å¤ã€‚
2. update weights from tensorï¼šè®­ç»ƒç»“æŸåæ›´æ–°æ¨¡å‹æƒé‡ã€‚
3. flush cacheï¼šæ¨¡å‹å‚æ•°æ›´æ–°ååˆ·æ–° KV cacheï¼Œå› ä¸ºä¹‹å‰çš„ KV cache å·²ç»å¤±æ•ˆäº†ã€‚

è¿™é‡Œæ¶‰åŠåˆ°äº†éå¸¸æ·±å…¥çš„å†…å­˜ç®¡ç†é—®é¢˜ï¼Œè¯»è€…å¯¹ SGLang engine åœ¨ verl é‡Œçš„æ˜¾å­˜ç®¡ç†æ„Ÿå…´è¶£ï¼Œæ¬¢è¿é˜…è¯»æ ‡å“¥çš„åšå®¢ [optimizing Memory Usage in verl](https://hebiao064.github.io/rl-memory-management)ï¼Œå†™çš„éå¸¸æ·±å…¥æµ…å‡ºã€‚

<details>
<summary>SGLangRollout ä½•æ—¶éœ€è¦ flush cache</summary>

è¿™ä¸€éƒ¨åˆ†å†…å®¹éœ€è¦å•ç‹¬æ‹å‡ºæ¥è®²è®²ã€‚SGLang engine çš„ release å’Œ resume éœ€è¦ä¿ç•™ CUDA Graphï¼Œå¦åˆ™ rollout æ•ˆç‡ä¼šå¤§å¹…é™ä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åŸºäº tom çš„ [torch_memory_saver](https://github.com/fzyzcjy/torch_memory_saver) å®ç°äº†ç‹¬ç«‹çš„æ˜¾å­˜ç®¡ç†ã€‚ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰ï¼š

1. `pause`ï¼›ä¿ç•™ mem savor ä½œç”¨åŸŸå†…æŒ‡å®š tensor çš„ virtual addressï¼Œä½†æ˜¯å°†å…¶ physical memory é‡Šæ”¾å›æ˜¾å­˜ç®¡ç†å™¨ã€‚
2. `resume`ï¼›å°†å…ˆå‰ `pause` çš„ tensor é‡æ–°ç”³è¯·ä¸€å— physical memoryï¼Œå¹¶å°†å…¶ virtual address æ˜ å°„åˆ°æ–°çš„ physical memoryã€‚

æ³¨æ„ï¼Œæ•´ä¸ª pause å’Œ resume çš„è¿‡ç¨‹ä¸­ï¼Œtensor çš„ virtual address ä¸ä¼šå‘ç”Ÿå˜åŒ–ï¼Œåªæ˜¯è¿™å— virtual address æ˜ å°„åˆ°çš„ physical memory æ”¹å˜äº†ã€‚å› æ­¤ï¼ŒCUDA Graph å¹¶æ²¡æœ‰å¤±æ•ˆï¼Œä¸å˜çš„ virtual address è®©è®¡ç®—æµä»æ—§å¯ä»¥æ­£å¸¸æ‰§è¡Œã€‚

verl å†…çš„ `release_memory_occupation` å’Œ `resume_memory_occupation` å°±æ˜¯åŸºäº `pause` å’Œ `resume` å®ç°çš„ã€‚å¬ä¸Šå»æ˜¯ä¸ªå®Œç¾çš„æ•…äº‹ï¼Œæˆ‘ä»¬ç”šè‡³å®ç°äº† [mutli-stage çš„æ˜¾å­˜ç®¡ç†](https://github.com/fzyzcjy/torch_memory_saver/pull/20)ï¼Œèƒ½å¤Ÿç‹¬ç«‹ release å’Œ resume kv cache å’Œ model weightsã€‚

ä¸è¿‡ï¼Œå¯¹äº kv cache è€Œè¨€ï¼Œåœ¨ kv cache è¢« release æ‰ä¹‹åï¼Œå®é™…ä¸Š kv cache çš„ tensor ä»æ—§ä¿ç•™ï¼Œåªæ˜¯å…¶ virtual address æ˜ å°„åˆ°çš„ physical memory è¢«é‡Šæ”¾äº†ã€‚ä¸æ­¤åŒæ—¶ï¼Œradix tree ä»æ—§ç´¢å¼•ç€æ•´ä¸ª kv cacheã€‚å½“ kv cache è¢« resume ä¹‹åï¼Œä¸€æ–¹é¢ä¹‹å‰ç‰©ç†å†…å­˜ä¸Šä¹‹å‰çš„ kv cache å·²ç»ä¸å¤å­˜åœ¨äº†ï¼Œå¦ä¸€æ–¹é¢æ¨¡å‹çš„å‚æ•°ä¹Ÿè¢«æ›´æ–°ã€‚å‡ºäºè¿™ä¸¤ç‚¹ï¼Œæˆ‘ä»¬ä¸€å®šè¦ä½¿ç”¨ flush cache æ¥å£æ¥åˆ·æ–° kv cache çš„ç´¢å¼•ï¼ˆradix treeï¼‰ã€‚

è¿™é‡Œåˆæœ‰ä¸ªéå¸¸æœ‰è¶£çš„è®¾è®¡ã€‚ä¹ä¸€æƒ³ kv cache çš„ç®¡ç†è¿™ä¹ˆéº»çƒ¦ï¼Œè¿˜è¦ flushï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥ delete kv cache ä»¥åŠ delete model weights å†é‡æ–°åˆå§‹åŒ–å‘¢ï¼Ÿæ˜¾ç„¶ï¼Œè¿™æ ·æ²¡æ³•åˆ©ç”¨å·²æœ‰çš„ cuda graphï¼Œéå¸¸æ¶ˆè€—æ—¶é—´ã€‚ä¿ç•™ virtual address ä¸å˜ä½†æ˜¯æ›´æ¢ physical memory çš„æ–¹æ¡ˆï¼Œè®© verl èƒ½å¤ŸæŒç»­åˆ©ç”¨å·²å»ºå¥½çš„ cuda graphã€‚

æœ€åä¸€ä¸ªé—®é¢˜ï¼Œä¸€å…±è¦å‡ æ¬¡ flush cache å‘¢ï¼Ÿæˆ‘ä¸ªäººç†è§£ï¼Œåœ¨ä¸€æ•´ä¸ª training engine è¢« pauseï¼Œresume ç„¶å update weights çš„è¿‡ç¨‹ä¸­ï¼Œå¿…é¡»è¦æœ‰ä¸€æ¬¡ flush cache æ¥åˆ·æ–° kv cache çš„ç´¢å¼•ï¼Œåªæ˜¯ verl å½“ä¸­ä¸ºäº†ä¿é™©ï¼Œåˆ·æ–°äº†å¾ˆå¤šæ¬¡ç½¢äº†ã€‚
</details>

<details>
<summary>SGLangRollout.AsyncEngine æºç </summary>

```python
class AsyncEngine(sglang.srt.entrypoints.engine.Engine):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # default to use dummy load format, which need to reload weights in first time
        self._need_reload = True

    async def release_memory_occupation(self):
        """Release GPU occupation temporarily."""
        obj = ReleaseMemoryOccupationReqInput()
        return await self.tokenizer_manager.release_memory_occupation(obj, None)

    async def resume_memory_occupation(self):
        return await self.tokenizer_manager.resume_memory_occupation(obj, None)

    async def update_weights_from_tensor(
        self,
        named_tensors: List[Tuple[str, torch.Tensor]],  # noqa: UP006
        load_format: Optional[str] = None,
        flush_cache: bool = True,
    ):
        """Update weights from distributed source. If there are going to be more updates, set `flush_cache` to be false
        to avoid duplicated cache cleaning operation."""
        obj = UpdateWeightsFromTensorReqInput(
            serialized_named_tensors=[MultiprocessingSerializer.serialize(named_tensors) for _ in range(self.server_args.tp_size)],
            load_format=load_format,
            flush_cache=flush_cache,
        )
        return await self.tokenizer_manager.update_weights_from_tensor(obj, None)

    async def flush_cache(self):
        return await self.tokenizer_manager.flush_cache()
```

</details>

### [`SGLangRollout._init_inference_engine()`](https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L325)

`SGLangRollout._init_inference_engine()` åˆå§‹åŒ–äº†å°è£…çš„ `AsyncEngine`ã€‚

<details>
<summary>SGLangRollout._init_inference_engine æºç </summary>

```python
def _init_inference_engine(self, trust_remote_code, actor_module, port):
    # initialize the inference engine
    nnodes = -(-self._tp_size // len(self.visible_devices_set))
    if nnodes > 1:
        ip = get_ip()
        port = get_open_port() if port is None else port
        [ip, port] = broadcast_pyobj(
            [ip, port],
            rank=self._rank,
            dist_group=self._device_mesh_cpu.get_group("tp"),
            src=self._device_mesh_cpu["tp"].mesh[0].item(),
            force_cpu_device=False,
        )
        dist_init_addr = f"[{ip}]:{port}" if is_ipv6(ip) else f"{ip}:{port}"
    else:
        dist_init_addr = None

    load_format = "dummy" if self.config.load_format.startswith("dummy") else self.config.load_format
    tp_size_per_node = self._tp_size // nnodes
    node_rank = self._tp_rank // tp_size_per_node
    first_rank_in_node = self._tp_rank % tp_size_per_node == 0

    if first_rank_in_node:
        rank = dist.get_rank()
        os.environ["SGLANG_BLOCK_NONZERO_RANK_CHILDREN"] = "0"
        self._engine = AsyncEngine(
            model_path=actor_module,
            dtype=self.config.dtype,
            mem_fraction_static=self.config.gpu_memory_utilization,
            enable_memory_saver=True,
            base_gpu_id=0,
            gpu_id_step=1,
            tp_size=self._tp_size,
            node_rank=node_rank,
            load_format=load_format,
            dist_init_addr=dist_init_addr,
            nnodes=nnodes,
            trust_remote_code=trust_remote_code,
            # NOTE(linjunrong): add rank to prevent SGLang generate same port inside PortArgs.init_new
            # when random.seed is being set during training
            port=30000 + rank,
            # NOTE(Chenyang): if you want to debug the SGLang engine output
            # please set the following parameters
            # Otherwise, it will make the engine run too slow
            # log_level="INFO",
            # log_requests=True,
            # log_requests_level=2,
            # max_running_requests=1,
        )
    else:
        self._engine = None

    self.sharding_manager = None
    self.is_sleep = True
```

</details>

è¿™é‡Œæœ€å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒSGLang engine å¹¶æ²¡æœ‰ä¸¥æ ¼å®ç° verl æ‰€å¸Œæœ›çš„ SPMD æ¨¡å¼ï¼ˆæ¯ä¸ª GPU ä¸Šçš„è¿›ç¨‹å®Œå…¨ä¸€æ ·ï¼‰ï¼Œè€Œæ˜¯é‡‡ç”¨äº† mock çš„ SPMDã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œå‡è®¾ tp size = 4ï¼ŒæŒ‰ç…§ verl çš„è®¾è®¡ï¼Œåº”è¯¥è¦ 4 å¼  GPU ä¸Šæ¯ä¸ªéƒ½è¿è¡Œä¸€ä¸ªç›¸åŒçš„ SGLang engineã€‚å®é™…ä¸Šçš„å®ç°æ˜¯åœ¨ GPU 0 ä¸Šå¯åŠ¨ä¸€ä¸ªè¿›ç¨‹å æ®å…¨éƒ¨ GPUï¼Œè€Œ GPU 1 2 3 ä¸Šä»…ä»…ä¿ç•™ä¸€ä¸ªç©ºè¿›ç¨‹ `None`ã€‚è™½ç„¶ verl team èµ·åˆè®¾å®šä¸­è®¤ä¸ºä¸¥æ ¼çš„ SPMD æ„ä¹‰å·¨å¤§ï¼Œä½†å®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸º mock çš„ SPMD å·²ç»è¶³å¤Ÿæ»¡è¶³æ€§èƒ½éœ€æ±‚ã€‚

ã€TODOã€‘ è¿™ä¹ˆæè¿°å¯èƒ½ä¸ä¸¥è°¨ã€‚

### [`TaskRunner.run()`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64)

å¾€ä¸‹èµ°äº†è¿™ä¹ˆå¤šå±‚ï¼Œæˆ‘ä»¬ç»ˆäºèƒ½å¤Ÿç»§ç»­å›åˆ° `TaskRunner` ç±»ã€‚ğŸ˜­

ã€TODOã€‘ä¸Šæ–‡å…¶å®ä¸»è¦æ˜¯ Actor Rolloutï¼Œè¿˜æ²¡æœ‰å…·ä½“è¯´ Actor çš„ training forward and backwardã€‚ä»¥åŠ Referenceï¼Œreward å’Œ critic çš„ training forward and backwardã€‚

1. åŠ è½½ã€è§£æå’ŒéªŒè¯è®­ç»ƒä»»åŠ¡çš„é…ç½®ï¼ˆä½¿ç”¨ `OmegaConf`ï¼‰ï¼Œç¡®ä¿æ‰€æœ‰å‚æ•°çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚
2. å°†æ¨¡å‹æ–‡ä»¶ä»è¿œç¨‹è·¯å¾„å¤åˆ¶åˆ°æœ¬åœ°ï¼Œç¡®ä¿æ‰€æœ‰ Worker éƒ½å¯ä»¥è®¿é—®ã€‚
3. ç»„ä»¶åˆå§‹åŒ–ï¼š
    * åˆå§‹åŒ– Tokenizer å’Œ Processorï¼Œç”¨äºæ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®çš„å¤„ç†ã€‚
    * æ ¹æ®é…ç½®ä¸­æŒ‡å®šçš„ Actor ç­–ç•¥ï¼ˆå¦‚ `fsdp` æˆ– `megatron`ï¼‰ï¼ŒåŠ¨æ€é€‰æ‹©ç›¸åº”çš„ Worker ç±»ï¼ˆä¾‹å¦‚ `ActorRolloutRefWorker` å’Œ `CriticWorker`ï¼‰ï¼Œå¹¶ç¡®å®šä½¿ç”¨çš„ `RayWorkerGroup` ç±»å‹ã€‚
    * å®šä¹‰ Ray èµ„æºæ± çš„è§„æ ¼å’Œè§’è‰²åˆ°èµ„æºæ± çš„æ˜ å°„ï¼Œç”¨äº GPU èµ„æºçš„åˆ†é…å’Œç®¡ç†ã€‚
    * åŠ è½½ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„å¥–åŠ±æ¨¡å‹ã€‚
    * åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œä»¥åŠè®­ç»ƒæ•°æ®é‡‡æ ·å™¨ã€‚
4. åˆ›å»º `RayPPOTrainer` å®ä¾‹ï¼Œå®ƒæ˜¯ç®¡ç†æ‰€æœ‰è®¡ç®—èµ„æºå’Œè®­ç»ƒæµç¨‹çš„ä¸­å¤®åè°ƒå™¨ã€‚
5. è°ƒç”¨ `RayPPOTrainer` çš„ `init_workers()` æ–¹æ³•ï¼Œå°†é…ç½®çš„ Worker ç±»å®ä¾‹åŒ–åˆ° Ray é›†ç¾¤çš„ GPU ä¸Šï¼Œä¸ºå®é™…è®¡ç®—åšå‡†å¤‡ã€‚
6. è°ƒç”¨ `RayPPOTrainer` çš„ `fit()` æ–¹æ³•ï¼Œå¯åŠ¨ PPO è®­ç»ƒå¾ªç¯ã€‚

<details>
<summary>TaskRunner.run æºç </summary>

```python
@ray.remote(num_cpus=1)
class TaskRunner:
    def run(self, config):

        from pprint import pprint
        from omegaconf import OmegaConf
        from verl.utils.fs import copy_to_local
        import socket
        import os

        print(f"TaskRunner hostname: {socket.gethostname()}, PID: {os.getpid()}")
        pprint(OmegaConf.to_container(config, resolve=True))
        OmegaConf.resolve(config)

        # æ¨¡å‹ä¸‹è½½
        local_path = copy_to_local(config.actor_rollout_ref.model.path, use_shm=config.actor_rollout_ref.model.get("use_shm", False))

        # Tokenizer å’Œ Processor åˆå§‹åŒ–
        from verl.utils import hf_processor, hf_tokenizer
        trust_remote_code = config.data.get("trust_remote_code", False)
        tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)
        processor = hf_processor(local_path, trust_remote_code=trust_remote_code, use_fast=True)

        # Worker ç±»å‹é€‰æ‹©
        if config.actor_rollout_ref.actor.strategy in ["fsdp", "fsdp2"]:
            from verl.single_controller.ray import RayWorkerGroup
            from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker
            actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == "async" else ActorRolloutRefWorker
            ray_worker_group_cls = RayWorkerGroup
        elif config.actor_rollout_ref.actor.strategy == "megatron":
            assert config.actor_rollout_ref.actor.strategy == config.critic.strategy
            from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup
            from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker
            actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == "async" else ActorRolloutRefWorker
            ray_worker_group_cls = NVMegatronRayWorkerGroup
        else:
            raise NotImplementedError

        from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role

        # è§’è‰²åˆ° Worker ç±»çš„æ˜ å°„
        role_worker_mapping = {
            Role.ActorRollout: ray.remote(actor_rollout_cls),
            Role.Critic: ray.remote(CriticWorker),
        }

        # èµ„æºæ± è§„æ ¼å’Œè§’è‰²æ˜ å°„
        global_pool_id = "global_pool"
        resource_pool_spec = {
            global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,
        }
        mapping = {
            Role.ActorRollout: global_pool_id,
            Role.Critic: global_pool_id,
        }

        # Reward Model Worker çš„åˆå§‹åŒ–
        if config.reward_model.enable:
            if config.reward_model.strategy in ["fsdp", "fsdp2"]:
                from verl.workers.fsdp_workers import RewardModelWorker
            elif config.reward_model.strategy == "megatron":
                from verl.workers.megatron_workers import RewardModelWorker
            else:
                raise NotImplementedError
            role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)
            mapping[Role.RewardModel] = global_pool_id

        # Reference Policy Worker çš„åˆå§‹åŒ–
        if config.algorithm.use_kl_in_reward or config.actor_rollout_ref.actor.use_kl_loss:
            role_worker_mapping[Role.RefPolicy] = ray.remote(ActorRolloutRefWorker)
            mapping[Role.RefPolicy] = global_pool_id

        # åŠ è½½å¥–åŠ±ç®¡ç†å™¨
        reward_fn = load_reward_manager(config, tokenizer, num_examine=0, **config.reward_model.get("reward_kwargs", {}))
        val_reward_fn = load_reward_manager(config, tokenizer, num_examine=1, **config.reward_model.get("reward_kwargs", {}))
        resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)

        from verl.utils.dataset.rl_dataset import collate_fn, create_rl_dataset, create_rl_sampler

        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        train_dataset = create_rl_dataset(config.data.train_files, config.data, tokenizer, processor)
        val_dataset = create_rl_dataset(config.data.val_files, config.data, tokenizer, processor)
        train_sampler = create_rl_sampler(config.data, train_dataset)

        # åˆå§‹åŒ– PPO è®­ç»ƒå™¨
        trainer = RayPPOTrainer(
            config=config,
            tokenizer=tokenizer,
            processor=processor,
            role_worker_mapping=role_worker_mapping,
            resource_pool_manager=resource_pool_manager,
            ray_worker_group_cls=ray_worker_group_cls,
            reward_fn=reward_fn,
            val_reward_fn=val_reward_fn,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            collate_fn=collate_fn,
            train_sampler=train_sampler,
            device_name=config.trainer.device,
        )
        # åˆå§‹åŒ–è®­ç»ƒå™¨çš„ Workers
        trainer.init_workers()
        # å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
        trainer.fit()
```
</details>


### [`RayPPOTrainer.__init__()`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L277)

1. ä¿å­˜ä¼ å…¥çš„é…ç½®å¯¹è±¡ã€tokenizerã€processorã€è§’è‰²åˆ° Worker çš„æ˜ å°„ã€èµ„æºæ± ç®¡ç†å™¨ä»¥åŠ WorkerGroup ç±»ã€‚
2. æ ¹æ®é…ç½®å¯ç”¨æˆ–ç¦ç”¨ Criticã€Reference Policyã€Reward Model å’Œ Hybrid Engine ç­‰åŠŸèƒ½ç»„ä»¶ã€‚
3. è°ƒç”¨ `_validate_config()` æ–¹æ³•éªŒè¯é…ç½®çš„åˆç†æ€§ã€‚
4. å­˜å‚¨è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€collate å‡½æ•°å’Œè®­ç»ƒæ•°æ®é‡‡æ ·å™¨ã€‚

<details>
<summary>RayPPOTrainer æºç </summary>

```python
class RayPPOTrainer:
    # TODO: support each role have individual ray_worker_group_cls,
    # i.e., support different backend of different role
    def __init__(
        self,
        config,
        tokenizer,
        role_worker_mapping: dict[Role, WorkerType],
        resource_pool_manager: ResourcePoolManager,
        ray_worker_group_cls: RayWorkerGroup = RayWorkerGroup,
        processor=None,
        reward_fn=None,
        val_reward_fn=None,
        train_dataset: Optional[Dataset] = None,
        val_dataset: Optional[Dataset] = None,
        collate_fn=None,
        train_sampler: Optional[Sampler] = None,
        device_name="cuda",
    ):
        """
        Initialize distributed PPO trainer with Ray backend.
        Note that this trainer runs on the driver process on a single CPU/GPU node.

        Args:
            config: Configuration object containing training parameters.
            tokenizer: Tokenizer used for encoding and decoding text.
            role_worker_mapping (dict[Role, WorkerType]): Mapping from roles to worker classes.
            resource_pool_manager (ResourcePoolManager): Manager for Ray resource pools.
            ray_worker_group_cls (RayWorkerGroup, optional): Class for Ray worker groups. Defaults to RayWorkerGroup.
            processor: Optional data processor, used for multimodal data.
            reward_fn: Function for computing rewards during training.
            val_reward_fn: Function for computing rewards during validation.
            train_dataset (Optional[Dataset], optional): Training dataset. Defaults to None.
            val_dataset (Optional[Dataset], optional): Validation dataset. Defaults to None.
            collate_fn: Function to collate data samples into batches.
            train_sampler (Optional[Sampler], optional): Sampler for the training dataset. Defaults to None.
            device_name (str, optional): Device name for training (e.g., "cuda", "cpu"). Defaults to "cuda".
        """

        # Store the tokenizer for text processing
        self.tokenizer = tokenizer
        self.processor = processor
        self.config = config
        self.reward_fn = reward_fn
        self.val_reward_fn = val_reward_fn

        self.hybrid_engine = config.actor_rollout_ref.hybrid_engine
        assert self.hybrid_engine, "Currently, only support hybrid engine"

        if self.hybrid_engine:
            assert Role.ActorRollout in role_worker_mapping, f"{role_worker_mapping.keys()=}"

        self.role_worker_mapping = role_worker_mapping
        self.resource_pool_manager = resource_pool_manager
        self.use_reference_policy = Role.RefPolicy in role_worker_mapping
        self.use_rm = Role.RewardModel in role_worker_mapping
        self.ray_worker_group_cls = ray_worker_group_cls
        self.device_name = device_name
        self.validation_generations_logger = ValidationGenerationsLogger()

        # if ref_in_actor is True, the reference policy will be actor without lora applied
        self.ref_in_actor = config.actor_rollout_ref.model.get("lora_rank", 0) > 0

        # define in-reward KL control
        # kl loss control currently not suppoorted
        if config.algorithm.use_kl_in_reward:
            self.kl_ctrl_in_reward = core_algos.get_kl_controller(config.algorithm.kl_ctrl)

        if self.config.algorithm.adv_estimator == AdvantageEstimator.GAE:
            self.use_critic = True
        elif self.config.algorithm.adv_estimator in [
            AdvantageEstimator.GRPO,
            AdvantageEstimator.GRPO_PASSK,
            AdvantageEstimator.REINFORCE_PLUS_PLUS,
            AdvantageEstimator.REMAX,
            AdvantageEstimator.RLOO,
            AdvantageEstimator.OPO,
            AdvantageEstimator.REINFORCE_PLUS_PLUS_BASELINE,
        ]:
            self.use_critic = False
        else:
            raise NotImplementedError

        self._validate_config()
        self._create_dataloader(train_dataset, val_dataset, collate_fn, train_sampler)
```

</details>

### [`RayPPOTrainer.init_workers()`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715)

`init_workers()` å‡½æ•°è´Ÿè´£åœ¨ Ray é›†ç¾¤ä¸Šå®ä¾‹åŒ–å’Œåˆå§‹åŒ– ActorRolloutã€Criticã€Reference Policy å’Œ Reward Model Workersã€‚

1.  **åˆ›å»ºèµ„æºæ± **ï¼šé€šè¿‡ `ResourcePoolManager` åˆ›å»º Ray èµ„æºæ± ã€‚
2.  **åˆå§‹åŒ–èµ„æºæ± åˆ°ç±»çš„æ˜ å°„**ï¼šä¸ºæ¯ä¸ªèµ„æºæ± åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨ä¸åŒè§’è‰² Worker çš„ `RayClassWithInitArgs` åŒ…è£…å™¨ã€‚`RayClassWithInitArgs` ç”¨äºå»¶è¿Ÿåˆå§‹åŒ– Workerï¼Œå­˜å‚¨äº† Worker çš„ç±»å’Œåˆå§‹åŒ–å‚æ•°ã€‚
3.  **åˆ›å»ºä¸åŒè§’è‰²çš„ Worker çš„ `RayClassWithInitArgs` å®ä¾‹**ï¼šæ ¹æ®é…ç½®å¯ç”¨æƒ…å†µï¼Œä¸º ActorRolloutã€Criticã€Reference Policy å’Œ Reward Model åˆ›å»ºå¯¹åº”çš„ `RayClassWithInitArgs` å®ä¾‹ã€‚
4.  **åˆå§‹åŒ– WorkerGroup**ï¼šéå†æ‰€æœ‰èµ„æºæ± ï¼Œå°†åŒä¸€èµ„æºæ± ä¸­çš„å¤šä¸ª Worker ç±»é€šè¿‡ `create_colocated_worker_cls` ç»„åˆæˆä¸€ä¸ªå…±ç½®ç±»ï¼Œç„¶åå®ä¾‹åŒ– `RayWorkerGroup`ã€‚`RayWorkerGroup` è´Ÿè´£åœ¨å¤šä¸ª GPU ä¸Šå¯åŠ¨å¤šä¸ª Worker å®ä¾‹ã€‚æœ€åè°ƒç”¨ `spawn()` æ–¹æ³•åœ¨ Ray ä¸­å®é™…åˆ›å»º Worker å®ä¾‹ã€‚
5.  **åˆå§‹åŒ–å„ä¸ª Worker**ï¼šæ ¹æ®è§’è‰²ä»åˆ›å»ºçš„ WorkerGroup å­—å…¸ä¸­è·å–å¯¹åº”çš„ WorkerGroupï¼Œå¹¶è°ƒç”¨å…¶ `init_model()` æ–¹æ³•ï¼ŒæŒ‰ç…§ä¾èµ–å…³ç³»ä¾æ¬¡åˆå§‹åŒ–ä¸åŒçš„ Worker æ¨¡å—ã€‚ActorRollout Worker é€šå¸¸æœ€ååˆå§‹åŒ–ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚

<details>
<summary>RayPPOTrainer.init_workers æºç </summary>

```python
    def init_workers(self):
        """Initialize distributed training workers using Ray backend.

        Creates:
        1. Ray resource pools from configuration
        2. Worker groups for each role (actor, critic, etc.)
        """
        self.resource_pool_manager.create_resource_pool()

        self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()}

        # create actor and rollout
        if self.hybrid_engine:
            resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)
            actor_rollout_cls = RayClassWithInitArgs(
                cls=self.role_worker_mapping[Role.ActorRollout],
                config=self.config.actor_rollout_ref,
                role="actor_rollout",
            )
            self.resource_pool_to_cls[resource_pool]["actor_rollout"] = actor_rollout_cls
        else:
            raise NotImplementedError

        # create critic
        if self.use_critic:
            resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic)
            critic_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.Critic], config=self.config.critic)
            self.resource_pool_to_cls[resource_pool]["critic"] = critic_cls

        # create reference policy if needed
        if self.use_reference_policy:
            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy)
            ref_policy_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RefPolicy], config=self.config.actor_rollout_ref, role="ref")
            self.resource_pool_to_cls[resource_pool]["ref"] = ref_policy_cls

        # create a reward model if reward_fn is None
        if self.use_rm:
            # we create a RM here
            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RewardModel)
            rm_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RewardModel], config=self.config.reward_model)
            self.resource_pool_to_cls[resource_pool]["rm"] = rm_cls

        # initialize WorkerGroup
        # NOTE: if you want to use a different resource pool for each role, which can support different parallel size,
        # you should not use `create_colocated_worker_cls`.
        # Instead, directly pass different resource pool to different worker groups.
        # See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information.
        all_wg = {}
        wg_kwargs = {}  # Setting up kwargs for RayWorkerGroup
        if OmegaConf.select(self.config.trainer, "ray_wait_register_center_timeout") is not None:
            wg_kwargs["ray_wait_register_center_timeout"] = self.config.trainer.ray_wait_register_center_timeout
        if OmegaConf.select(self.config.trainer, "profile_steps") is not None:
            wg_kwargs["profile_steps"] = OmegaConf.select(self.config.trainer, "profile_steps")
            assert OmegaConf.select(self.config.trainer, "worker_nsight_options") is not None, "worker_nsight_options must be set when profile_steps is set"
            wg_kwargs["worker_nsight_options"] = OmegaConf.to_container(OmegaConf.select(self.config.trainer, "worker_nsight_options"))

        for resource_pool, class_dict in self.resource_pool_to_cls.items():
            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)
            wg_dict = self.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls, device_name=self.device_name, **wg_kwargs)
            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())
            all_wg.update(spawn_wg)

        if self.use_critic:
            self.critic_wg = all_wg["critic"]
            self.critic_wg.init_model()

        if self.use_reference_policy and not self.ref_in_actor:
            self.ref_policy_wg = all_wg["ref"]
            self.ref_policy_wg.init_model()

        if self.use_rm:
            self.rm_wg = all_wg["rm"]
            self.rm_wg.init_model()

        # we should create rollout at the end so that vllm can have a better estimation of kv cache memory
        self.actor_rollout_wg = all_wg["actor_rollout"]
        self.actor_rollout_wg.init_model()

        # create async rollout manager and request scheduler
        self.async_rollout_mode = False
        if self.config.actor_rollout_ref.rollout.mode == "async":
            from verl.workers.rollout.async_server import AsyncLLMServerManager

            self.async_rollout_mode = True
            self.async_rollout_manager = AsyncLLMServerManager(
                config=self.config,
                worker_group=self.actor_rollout_wg,
            )
```

<details>
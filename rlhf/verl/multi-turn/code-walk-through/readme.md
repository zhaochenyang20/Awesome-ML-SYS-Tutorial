# verl Multi-turn Code Walk Through

æ‰¿è’™ç¤¾åŒºåšçˆ±ï¼ŒAgentic RL å¦‚ç«å¦‚è¼ï¼ŒSGLang RL Group çš„å·¥ä½œä¹Ÿå¤œä»¥ç»§æ—¥åœ¨å±•å¼€ã€‚è€ƒè™‘åˆ°å„å¤§ RL æ¡†æ¶çš„ä»£ç æ›´æ–°é¢‘ç‡æé«˜ï¼Œç¤¾åŒºäºŒæ¬¡å¼€å‘éœ€æ±‚å·¨å¤§ï¼Œæˆ‘ä»¬é€‰æ‹©ä»¥ verl å‡ºå‘ï¼Œåˆ†æå…¶ end to end mutli-turn RL è®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚æ•´ä½“ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›è¦†ç›–æ‰€æœ‰é‡è¦çš„ class ä»¥åŠå‡½æ•°ï¼Œæ›´ç»†ç²’åº¦çš„ä»£ç ä¸å†å±•å¼€ã€‚æˆ‘ä»¬çš„å†™ä½œé£æ ¼å¸Œæœ›èƒ½å¤Ÿ follow SGLang çš„ code-walk-throughï¼š

[SGLang Code Walk Through](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/sglang/code-walk-through/readme-CN.md)

ä¸ºäº†å‰åå†…å®¹çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬åŸºäº 76f63cffa5 çš„ commit è¿›è¡Œåˆ†æã€‚

æ•´ä¸ªè®­ç»ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¼šå…·ä½“å±•å¼€æ¯ä¸ªéƒ¨åˆ†ã€‚

```mermaid
flowchart LR
subgraph W2["Initialize"]
WP[Process Data] --> A
direction TB D1[Data Prepare] --> A
A[RayPPOTrainer] --> B1[Resource Group]
B1 --> Workers

    subgraph Workers["Workers"]
        direction TB
                WA[ActorRolloutWorker] --> WD[FSDP Engine]
        WB[CriticWorker] --> WD
        WC[RewardModelWorker] --> WD
        WD --> WE[SGLang Engine]
    end
    
    Workers --> C1[Hybrid Engine]
end

subgraph W3["Train Loop"]
    direction TB
    E[DataLoader] --> RolloutBox
    
    subgraph RolloutBox["Rollout"]
        F1[Prepare Data] --> F2[SGLang Async Rollout]
        F2 --> F3[Multi-turn Chat Process]
    end
    
    RolloutBox --> ExpBox
    
    subgraph ExpBox["Make Experience"]
        G1[Recompute Log Probs] --> G2[Compute Reward]
        G2 --> G3[Compute Advantage]
    end
    
    ExpBox --> UpdateBox
    
    subgraph UpdateBox["Train The Model"]
        H1[Load FSDP Model Weight] --> H2[Compute Gradient]
        H2 --> H3[Weights Update]
        H3 --> H4[Sync Weights]
    end
    
    UpdateBox --> E
end

W2 --> W3

```

## **æ•°æ®é¢„å¤„ç†**

ä»¥ [GSM8K](https://huggingface.co/datasets/openai/gsm8k) ä¸ºä¾‹ï¼Œé¢„å¤„ç†è„šæœ¬æ˜¯Â `examples/data_preprocess/gsm8k_multiturn_w_tool.py`ã€‚æ•´ä¸ªè„šæœ¬åªåšäº†ç»å…¸çš„ huggingface datasets mappingï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š

1. åŠ è½½ openai/gsm8k åŸå§‹æ•°æ®é›†ï¼ˆtrain/testï¼‰ã€‚
2. å¯¹æ¯æ¡åŸå§‹æ•°æ®ï¼Œç”Ÿæˆå¸¦æœ‰å·¥å…·è°ƒç”¨è¦æ±‚çš„ promptï¼ˆæ¯”å¦‚åœ¨ user turn å¼ºè°ƒæ¨¡å‹å¯ä»¥è°ƒç”¨Â `calc_gsm8k_reward`Â å·¥å…·ï¼Œæ¯ä¸ªqaè‡³å°‘è°ƒç”¨ä¸€æ¬¡ï¼‰ã€‚
3. åŒæ ·å¯¹äºæ¯æ¡åŸå§‹æ•°æ®ï¼Œè§£æç­”æ¡ˆï¼›å°† ground truth å†™å…¥ extra_info å­—æ®µã€‚
4. å­˜å‚¨ä¸º parquet æ–‡ä»¶ï¼Œåˆ†åˆ«ä¿ç•™ä¸º train.parquet å’Œ test.parquetï¼Œé»˜è®¤è·¯å¾„ä¸ºÂ `~/data/gsm8k/`ã€‚

## å¯åŠ¨è®­ç»ƒ

ä¸€ä¸ªå…¸å‹çš„å¯åŠ¨å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
# now ç”¨äºç”Ÿæˆå®éªŒå¯åŠ¨çš„æ—¶é—´å°¾ç¼€ï¼Œé¿å…é‡å¤å¯åŠ¨å®éªŒæ—¶è¦†ç›–å·²æœ‰ wandb log

function now() {
    date '+%Y-%m-%d-%H-%M'
}

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
nohup bash examples/sglang_multiturn/run_qwen2.5-3b_gsm8k_multiturn.sh \
    trainer.experiment_name=qwen2.5-3b_rm-gsm8k-sgl-multiturn-$now \
    > logs/gsm8k-$now.log 2>&1 &

```

## è„šæœ¬é…ç½®

verl çš„å„é¡¹å‚æ•°å®å±å¤æ‚ï¼Œæˆ‘ä»¬ä¼šå•ç‹¬ç¼–å†™æ–‡æ¡£æ¥åˆ†äº«å¯¹ verl å„ç±»å‚æ•°çš„ç†è§£ã€‚åœ¨è¿™ç¯‡æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æ ¼å¤–å¼ºè°ƒçš„æ˜¯ verl å„ç±» config çš„è¦†ç›–å…³ç³»ã€‚verl çš„é…ç½®æ–‡ä»¶åˆ©ç”¨ hydra è¿›è¡Œäº†**åˆ†å±‚è¦†ç›–**çš„è®¾è®¡æ¨¡å¼ã€‚

<details>

<summary>Hydra ç®€ä»‹</summary>

[**Hydra**](https://github.com/facebookresearch/hydra) æ˜¯ä¸€ä¸ªç”± Facebook Research å¼€å‘çš„ Python æ¡†æ¶ï¼Œæ—¨åœ¨**ä¼˜é›…åœ°é…ç½®å¤æ‚çš„åº”ç”¨ç¨‹åº**ã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºéœ€è¦ç®¡ç†å¤§é‡å‚æ•°å’Œè¿›è¡Œå¤šç»„å®éªŒçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨å­¦ä¹ é¡¹ç›®ã€‚Hydra çš„æ ¸å¿ƒç‰¹ç‚¹åœ¨äºå…¶**åŠ¨æ€ã€åˆ†å±‚å’Œå¯ç»„åˆçš„é…ç½®ç®¡ç†èƒ½åŠ›**ã€‚Hydra çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

* **åˆ†å±‚é…ç½® (Hierarchical Configuration)**ï¼šå¯ä»¥å°†é…ç½®åˆ†è§£æˆå¤šä¸ªå°å‹ã€æ¨¡å—åŒ–çš„ YAML æ–‡ä»¶ï¼Œå¹¶ä»¥ç›®å½•ç»“æ„è¿›è¡Œç»„ç»‡ã€‚è¿™ä½¿å¾—é…ç½®æ›´åŠ æ¸…æ™°ã€æ˜“äºç®¡ç†å’Œå¤ç”¨ã€‚
* **é…ç½®ç»„åˆ (Configuration Composition)**ï¼šHydra èƒ½å¤Ÿå°†è¿™äº›ç‹¬ç«‹çš„é…ç½®æ¨¡å—åŠ¨æ€åœ°ç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„é…ç½®å¯¹è±¡ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨ä¸»é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š `defaults` åˆ—è¡¨æ¥é€‰æ‹©å’Œç»„åˆä¸åŒçš„é…ç½®ç»„ä»¶ã€‚
* **å‘½ä»¤è¡Œè¦†ç›– (Command-line Overrides)**ï¼šè¿™æ˜¯ Hydra æœ€å¼ºå¤§çš„åŠŸèƒ½ä¹‹ä¸€ã€‚ä½ å¯ä»¥åœ¨è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼Œç›´æ¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–é…ç½®ä¸­çš„ä»»ä½•å€¼ã€‚è¿™ä½¿å¾—è¿›è¡Œå®éªŒå’Œå¿«é€Ÿè¿­ä»£å˜å¾—éå¸¸æ–¹ä¾¿ï¼Œæ— éœ€ä¿®æ”¹é…ç½®æ–‡ä»¶æœ¬èº«ã€‚
* **å¤šè¿è¡Œæ¨¡å¼ (Multi-run)**ï¼šHydra å…è®¸ä½ é€šè¿‡ä¸€ä¸ªå‘½ä»¤è¿è¡Œå¤šä¸ªå…·æœ‰ä¸åŒé…ç½®çš„å®éªŒã€‚è¿™å¯¹äºè¶…å‚æ•°æœç´¢å’Œæ¨¡å‹æ¯”è¾ƒéå¸¸æœ‰ç”¨ã€‚
* **åŠ¨æ€å·¥ä½œç›®å½• (Dynamic Working Directory)**ï¼šæ¯æ¬¡è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼ŒHydra éƒ½ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œç›®å½•ï¼Œå¹¶å°†å½“å‰è¿è¡Œçš„é…ç½®å’Œè¾“å‡ºä¿å­˜åˆ°è¯¥ç›®å½•ä¸­ï¼Œç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚
* **å¯¹è±¡å®ä¾‹åŒ– (Object Instantiation)**ï¼šHydra å¯ä»¥ç›´æ¥ä»é…ç½®ä¸­å®ä¾‹åŒ– Python å¯¹è±¡ï¼ˆç±»æˆ–å‡½æ•°ï¼‰ï¼Œè¿™å¤§å¤§ç®€åŒ–äº†ä»£ç ï¼Œä½¿é…ç½®æ›´å…·å£°æ˜æ€§ã€‚


Hydra å®ç°åˆ†å±‚è¦†ç›–çš„ä¸»è¦æœºåˆ¶æ˜¯**ç»„åˆ (Composition)** å’Œ **å‘½ä»¤è¡Œè¦†ç›– (Command-line Overrides)**ã€‚

1.  **åˆ†å±‚é…ç½®çš„ç»„ç»‡**ï¼š

é€šå¸¸ä¼šåˆ›å»ºä¸€ä¸ª `conf` ç›®å½•ï¼Œå¹¶åœ¨å…¶ä¸­ç»„ç»‡é…ç½®ã€‚ä¾‹å¦‚ï¼š

```yaml
.
â”œâ”€â”€ my_app.py
â””â”€â”€ conf
    â”œâ”€â”€ config.yaml
    â”œâ”€â”€ model
    â”‚   â”œâ”€â”€ cnn.yaml
    â”‚   â””â”€â”€ rnn.yaml
    â””â”€â”€ dataset
        â”œâ”€â”€ cifar10.yaml
        â””â”€â”€ imagenet.yaml
```

`config.yaml` æ˜¯ä½ çš„ä¸»é…ç½®æ–‡ä»¶ã€‚åœ¨ `model` ç›®å½•ä¸‹ï¼Œä½ å¯ä»¥å®šä¹‰ä¸åŒçš„æ¨¡å‹é…ç½®ï¼ˆå¦‚ `cnn.yaml`ã€`rnn.yaml`ï¼‰ï¼Œåœ¨ `dataset` ç›®å½•ä¸‹å®šä¹‰ä¸åŒçš„æ•°æ®é›†é…ç½®ï¼ˆå¦‚ `cifar10.yaml`ã€`imagenet.yaml`ï¼‰ã€‚

2.  **`defaults` åˆ—è¡¨è¿›è¡Œç»„åˆ**ï¼š

åœ¨ `config.yaml` ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ç‰¹æ®Šçš„ `defaults` åˆ—è¡¨æ¥æŒ‡å®šé»˜è®¤åŠ è½½å“ªäº›é…ç½®ç»„ä»¶ã€‚

**`conf/config.yaml` ç¤ºä¾‹ï¼š**

```yaml
defaults:
    - model: cnn       # é»˜è®¤åŠ è½½ conf/model/cnn.yaml
    - dataset: cifar10 # é»˜è®¤åŠ è½½ conf/dataset/cifar10.yaml
    - _self_          # ç¡®ä¿å½“å‰æ–‡ä»¶ä¸­çš„å…¶ä»–é…ç½®é¡¹ä¹Ÿè¢«åŠ è½½

# å…¶ä»–åº”ç”¨çº§åˆ«çš„é»˜è®¤é…ç½®
learning_rate: 0.001
epochs: 10
```

å½“ Hydra åŠ è½½ `config.yaml` æ—¶ï¼Œå®ƒä¼šæ ¹æ® `defaults` åˆ—è¡¨ä¸­çš„æŒ‡ç¤ºï¼Œè‡ªåŠ¨å°† `conf/model/cnn.yaml` å’Œ `conf/dataset/cifar10.yaml` çš„å†…å®¹åˆå¹¶åˆ°æœ€ç»ˆçš„é…ç½®å¯¹è±¡ä¸­ã€‚

3.  **å‘½ä»¤è¡Œè¦†ç›–**ï¼š

è¿™æ˜¯å®ç°çµæ´»è¦†ç›–çš„å…³é”®ã€‚ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–ä»»ä½•å·²åŠ è½½çš„é…ç½®å€¼ï¼ŒåŒ…æ‹¬åœ¨ `defaults` åˆ—è¡¨ä¸­æŒ‡å®šçš„ç»„ä»¶æˆ–å…¶å†…éƒ¨çš„ä»»ä½•å‚æ•°ã€‚

* **è¦†ç›–æ•´ä¸ªé…ç½®ç»„**ï¼š
è¦åˆ‡æ¢æ¨¡å‹ä» `cnn` åˆ° `rnn`ï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸­è¿™æ ·è¿è¡Œï¼š

```bash
python my_app.py model=rnn
```

è¿™å°†æŒ‡ç¤º Hydra åŠ è½½ `conf/model/rnn.yaml`ï¼Œå¹¶ç”¨å®ƒæ¥æ›¿æ¢é»˜è®¤çš„ `cnn` é…ç½®ã€‚

* **è¦†ç›–ç‰¹å®šå‚æ•°**ï¼š
ä½ å¯ä»¥æ·±å…¥åˆ°é…ç½®çš„ä»»ä½•å±‚çº§æ¥è¦†ç›–ç‰¹å®šçš„å‚æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ä¿®æ”¹å­¦ä¹ ç‡æˆ–æ•°æ®é›†çš„æŸä¸ªå‚æ•°ï¼š

```bash
python my_app.py learning_rate=0.01 dataset.batch_size=64
```

è¿™é‡Œï¼Œ`learning_rate` ç›´æ¥è¦†ç›–äº† `config.yaml` ä¸­çš„å€¼ï¼Œè€Œ `dataset.batch_size` åˆ™è¦†ç›–äº† `conf/dataset/cifar10.yaml`ï¼ˆæˆ–è€…ä½ é€šè¿‡ `dataset=imagenet` æŒ‡å®šçš„å…¶ä»–æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼‰ä¸­çš„ `batch_size` å‚æ•°ã€‚

* **æ·»åŠ æ–°å‚æ•° (ä½¿ç”¨ `+`)**ï¼š
å¦‚æœä½ æƒ³æ·»åŠ ä¸€ä¸ªåœ¨é»˜è®¤é…ç½®ä¸­ä¸å­˜åœ¨çš„æ–°å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨ `+` å‰ç¼€ï¼š

```bash
python my_app.py +optimizer.name=AdamW
```

* **åŠ¨æ€è¦†ç›– (ä½¿ç”¨ `++`)**ï¼š
å¦‚æœä½ å¸Œæœ›ä¿®æ”¹ä¸€ä¸ªå·²æœ‰å­—æ®µï¼Œæˆ–è€…åœ¨åŸé…ç½®ä¸­æ²¡æœ‰è¯¥å­—æ®µæ—¶è‡ªåŠ¨åˆ›å»ºå®ƒï¼Œå¯ä»¥ä½¿ç”¨ ++ã€‚è¿™ç§æ–¹å¼é€‚ç”¨äºéœ€è¦åŠ¨æ€æ·»åŠ æˆ–è¦†ç›–é…ç½®é¡¹çš„åœºæ™¯ï¼Œç¡®ä¿å­—æ®µæ€»æ˜¯è¢«è®¾ç½®ä¸ºä½ æŒ‡å®šçš„å€¼ï¼Œæ— è®ºå®ƒæ˜¯å¦å·²å­˜åœ¨ã€‚

```bash
python my_app.py ++model.num_layers=10
```

Hydra å†…éƒ¨ä½¿ç”¨ [OmegaConf](https://www.google.com/search?q=https://omegaconf.readthedocs.io/en/2.3_latest/) åº“æ¥å¤„ç†è¿™äº›é…ç½®å¯¹è±¡ï¼Œå®ƒæä¾›äº†å¼ºå¤§çš„åˆå¹¶å’Œè§£æåŠŸèƒ½ï¼Œä½¿å¾—åˆ†å±‚è¦†ç›–å’Œå€¼æ’å€¼ï¼ˆä¾‹å¦‚ï¼Œå¼•ç”¨å…¶ä»–é…ç½®å€¼æˆ–ç¯å¢ƒå˜é‡ï¼‰å˜å¾—éå¸¸å®¹æ˜“ã€‚

</details>


å›åˆ° verl multi turnï¼Œåœ¨æˆ‘ä»¬å¯åŠ¨çš„ `run_qwen2.5-3b_gsm8k_multiturn.sh` ä¸­ï¼Œè®¾ç½®äº†ï¼š

```bash
PROJECT_DIR="$(pwd)"
CONFIG_PATH="$PROJECT_DIR/examples/sglang_multiturn/config"

python3 -m verl.trainer.main_ppo \
    --config-path="$CONFIG_PATH" \
    --config-name='gsm8k_multiturn_grpo' \
```

è¿™æ„å‘³ç€è¿™æ¬¡ä»»åŠ¡çš„é»˜è®¤ config æ˜¯ `CONFIG_PATH` ä¸‹çš„ `gsm8k_multiturn_grpo.yaml`ï¼Œä¸”æ¥ä¸‹æ¥çš„å‚æ•°ä¼šè¦†ç›– `gsm8k_multiturn_grpo.yaml` ä¸­çš„é»˜è®¤å€¼ã€‚æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬æ¥è§‚å¯Ÿ `gsm8k_multiturn_grpo.yaml` çš„å†…å®¹ï¼š

```yaml
hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  max_prompt_length: 1024
  max_response_length: 1024
  train_batch_size: 256
  return_raw_chat: True

actor_rollout_ref:
  hybrid_engine: True
  rollout:
    name: sglang
    multi_turn:
      enable: True
      max_turns: 5
      # tool_config_path: "./config/tool_config/gsm8k_tool_config.yaml"
```

è¿™é‡Œ hydra è¯­æ³•ï¼Œä¼šå» `verl/trainer/config` ç›®å½•ä¸‹å¯»æ‰¾ `ppo_trainer.yaml` ä½œä¸ºåŸºç¡€é…ç½®ï¼Œå¹¶ä¸”è¦†ç›–ã€‚å› æ­¤ï¼Œå¯åŠ¨ `run_qwen2.5-3b_gsm8k_multiturn.sh` æ—¶ï¼Œå…ˆåŠ è½½ `gsm8k_multiturn_grpo.yaml` ä½œä¸ºåŸºç¡€é…ç½®å¹¶è¦†ç›–ï¼Œç„¶ååŠ è½½ `ppo_trainer.yaml` å¹¶è¦†ç›–ã€‚æœ€ç»ˆåˆå¹¶è¿™ä¸‰çº§é…ç½®ï¼Œå¾—åˆ°æœ€ç»ˆçš„ configã€‚

æœ€åï¼Œæ³¨æ„åˆ°åœ¨ `run_qwen2.5-3b_gsm8k_multiturn.sh` çš„æœ€åï¼Œæˆ‘ä»¬ï¼Œæˆ‘ä»¬è®¾ç½®äº† `actor_rollout_ref.rollout.multi_turn.tool_config_path="$PROJECT_DIR/examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml"`ï¼Œè¿™é‡ŒæŒ‡å®š multi_turn çš„ tool_config_path ä¸º `examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml`ã€‚è¿™ä¸€æ–‡ä»¶ä»…ä»…é…ç½®äº† gsm8k çš„ tool è°ƒç”¨ï¼Œå¹¶ä¸ä¼šè¦†ç›–ä¹‹å‰è®­ç»ƒçš„ configã€‚

## è®­ç»ƒä¸»å…¥å£ä¸åˆå§‹åŒ–

### Ray Actorï¼ŒRay Task å’Œ Ray Worker

åœ¨ä»‹ç» verl çš„è®­ç»ƒä¸»å…¥å£ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»‹ç» Ray çš„ä¸€äº›æ ¸å¿ƒæ¦‚å¿µã€‚Ray æ˜¯ä¸€ä¸ªç»Ÿä¸€è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç®€å•åœ°ä»å•æœºåˆ°å¤§å‹åˆ†å¸ƒå¼é›†ç¾¤çš„æ‰©å±•ï¼Œæä¾›æ„å»ºå’Œè¿è¡Œåˆ†å¸ƒå¼åº”ç”¨çš„åº•å±‚åŸºç¡€è®¾æ–½å’Œä¸€ç»„æ ¸å¿ƒåŸè¯­ã€‚Ray é€šè¿‡ä»¥ä¸‹åŠŸèƒ½å®ç°è¿™ä¸€ç›®æ ‡ï¼š

1. **ç»Ÿä¸€ API**ï¼šRay æä¾›äº†ä¸€å¥—ç®€å•æ˜“ç”¨çš„ Python APIï¼Œå°†æ™®é€šå‡½æ•°è½¬æ¢ä¸ºåˆ†å¸ƒå¼ä»»åŠ¡ï¼Œå°† Python ç±»è½¬æ¢ä¸ºåˆ†å¸ƒå¼æœåŠ¡ï¼Œä¹Ÿå³ Ray Actorã€‚Ray Actor å†…éƒ¨æŒä¹…å­˜å‚¨çš„æ•°æ®ç§°ä¸ºçŠ¶æ€ï¼Œå¯ä»¥åœ¨ Actor çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…è¢«å¤šæ¬¡è®¿é—®ã€ä¿®æ”¹å’Œç»´æŠ¤ï¼Œè€Œä¸ä¼šåœ¨æ¯æ¬¡æ–¹æ³•è°ƒç”¨ç»“æŸåæ¶ˆå¤±ã€‚
2. **å¼¹æ€§ä¼¸ç¼©**ï¼šRay å¯ä»¥å°†åº”ç”¨ä»å•ä¸ªæœºå™¨æ— ç¼æ‰©å±•åˆ°æ‹¥æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„é›†ç¾¤ï¼Œå¹¶èƒ½æ ¹æ®éœ€æ±‚è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚
3. **å®¹é”™æ€§**ï¼šRay å†…ç½®äº†å®¹é”™æœºåˆ¶ï¼Œå¯ä»¥å¤„ç†èŠ‚ç‚¹æ•…éšœå’Œä»»åŠ¡å¤±è´¥ï¼Œç¡®ä¿åº”ç”¨çš„å¥å£®æ€§ã€‚
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šRay ä¼˜åŒ–äº†åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†å’Œæ•°æ®ä¼ è¾“ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚

Ray Task å’Œ Ray Actor éƒ½æ˜¯ç”¨äºåˆ†å¸ƒå¼è®¡ç®—çš„æ ¸å¿ƒåŸè¯­ï¼Œä½†å®ƒä»¬å„è‡ªæœåŠ¡äºä¸åŒçš„ç›®çš„ï¼Œä¸»è¦åŒºåˆ«åœ¨äº**æ˜¯å¦ç»´æŠ¤çŠ¶æ€**ã€‚

Ray Task æ˜¯ Ray ä¸­æœ€åŸºæœ¬çš„è®¡ç®—å•å…ƒï¼Œä»£è¡¨ä¸€ä¸ªæ— çŠ¶æ€çš„è¿œç¨‹å‡½æ•°ã€‚Ray Task çš„æ¯æ¬¡æ‰§è¡Œéƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¿ç•™ä¹‹å‰çš„ä»»ä½•ä¿¡æ¯ã€‚å°±åƒè°ƒç”¨ä¸€ä¸ªæ™®é€šå‡½æ•°ï¼Œæ‰§è¡Œå®Œåå°±æ¸…é™¤å†…éƒ¨çŠ¶æ€ã€‚æˆ‘ä»¬è°ƒç”¨ä¸€ä¸ª Ray Task åï¼Œä¼šç«‹å³è¿”å›å¾—åˆ°ä¸€ä¸ª Ray ObjectRefï¼Œè€Œä¸æ˜¯å®é™…çš„ç»“æœã€‚ä¸»ç¨‹åºå¯ä»¥ç»§ç»­æ‰§è¡Œå…¶ä»–æ“ä½œï¼Œè€Œ Ray Task åˆ™åœ¨åå°å¹¶è¡Œè¿è¡Œã€‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨ `ray.get()` æ¥è·å– Task çš„å®é™…ç»“æœã€‚ Ray Task éå¸¸é€‚åˆå¹¶è¡Œæ‰§è¡Œå¤§é‡ç‹¬ç«‹ã€ä¸€æ¬¡æ€§çš„è®¡ç®—ä»»åŠ¡ï¼Œè­¬å¦‚æ•°æ®æ‰¹å¤„ç†ã€ç‹¬ç«‹çš„æ¨¡å‹æ¨ç†ç­‰åœºæ™¯ã€‚

Ray Actor æ˜¯ä¸€ç§ç‰¹æ®Šçš„ Ray Taskï¼Œæ­£å¦‚å‰æ–‡æ‰€è¿°ï¼Œå®ƒæœ‰è‡ªå·±çš„çŠ¶æ€å’Œæ–¹æ³•ã€‚å½“æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª Ray Actor åï¼ŒRay ä¼šåœ¨é›†ç¾¤ä¸­çš„æŸä¸ª **Ray Worker** ä¸Šå¯åŠ¨ä¸€ä¸ªä¸“é—¨çš„è¿›ç¨‹æ¥æ‰˜ç®¡è¿™ä¸ªå¯¹è±¡ã€‚è¯¥è¿›ç¨‹ä¼šä¸€ç›´è¿è¡Œï¼Œç›´åˆ°è¢«é”€æ¯ã€‚Actor å¯ä»¥ç»´æŠ¤å†…éƒ¨å˜é‡ï¼Œå¹¶ä¸”è¿™äº›å˜é‡åœ¨ Actor çš„ç”Ÿå‘½å‘¨æœŸå†…æ˜¯æŒä¹…å­˜åœ¨çš„ã€‚æ¯æ¬¡è°ƒç”¨ Actor çš„æ–¹æ³•ï¼Œéƒ½å¯ä»¥è®¿é—®å’Œä¿®æ”¹è¿™äº›çŠ¶æ€ã€‚è¿™ä¸æ™®é€šçš„ Ray Task ä¸åŒï¼Œæ™®é€š Task æ‰§è¡Œå®Œä¼šæ¸…é™¤å†…éƒ¨çŠ¶æ€ã€‚Ray Actor æ”¯æŒå¹¶å‘è¯·æ±‚ï¼ŒRay ä¼šè´Ÿè´£å°†è¿™äº›è¯·æ±‚åºåˆ—åŒ–æ‰§è¡Œï¼Œä¿è¯ Actor å†…éƒ¨çŠ¶æ€çš„ä¸€è‡´æ€§å’Œçº¿ç¨‹å®‰å…¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ `@ray.remote` è£…é¥°å™¨å°†ä¸€ä¸ª Python ç±»è½¬æ¢ä¸ºä¸€ä¸ª Ray Actor ç±»ï¼Œç„¶åé€šè¿‡ `.remote()` æ–¹æ³•å®ä¾‹åŒ–ä¸€ä¸ªè¿œç¨‹ Actorã€‚

æœ€åï¼ŒRay Worker æ˜¯ Ray é›†ç¾¤ä¸­çœŸæ­£æ‰§è¡Œä»£ç çš„å·¥ä½œå•å…ƒã€‚ä¸€ä¸ª Ray é›†ç¾¤é€šå¸¸ç”±ä¸€ä¸ª Head Node å’Œå¤šä¸ª Worker Nodes ç»„æˆã€‚æ¯ä¸ªèŠ‚ç‚¹ä¸Šéƒ½ä¼šè¿è¡Œä¸€ä¸ªæˆ–å¤šä¸ª Ray Worker è¿›ç¨‹ã€‚æ— è®ºæ˜¯æ™®é€šçš„ Ray Task è¿˜æ˜¯ Ray Actor çš„æ–¹æ³•ï¼Œæœ€ç»ˆéƒ½æ˜¯ç”± Ray Worker è¿›ç¨‹æ¥æ‰§è¡Œçš„ã€‚æ¯ä¸ª Ray Worker éƒ½ä¼šè¢«åˆ†é…ä¸€å®šçš„è®¡ç®—èµ„æºï¼ˆå¦‚ CPUã€GPUï¼‰ã€‚å½“ä½ æäº¤ä¸€ä¸ª Ray Task æˆ–åˆ›å»ºä¸€ä¸ª Ray Actor æ—¶ï¼ŒRay çš„è°ƒåº¦å™¨ä¼šæ‰¾åˆ°ä¸€ä¸ªæœ‰è¶³å¤Ÿèµ„æºçš„ Worker æ¥è¿è¡Œå®ƒã€‚Worker è¿›ç¨‹ä¹‹é—´ä»¥åŠ Worker è¿›ç¨‹ä¸å¤´èŠ‚ç‚¹ä¹‹é—´ä¼šè¿›è¡Œé€šä¿¡ï¼Œä»¥åè°ƒä»»åŠ¡æ‰§è¡Œã€ä¼ è¾“æ•°æ®å’Œç®¡ç†çŠ¶æ€ã€‚ä¸€ä¸ª Ray Worker é€šå¸¸å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ Python è¿›ç¨‹ã€‚å¯¹äºæ™®é€šçš„ Ray Taskï¼ŒRay Worker ç›¸å½“äºå‡½æ•°è§£é‡Šå™¨ï¼Œæ‰§è¡Œå®Œä»»åŠ¡åå¯èƒ½ä¼šè¢«å¤ç”¨å»æ‰§è¡Œå…¶ä»–ä»»åŠ¡ã€‚è€Œå¯¹äº Ray Actorï¼ŒRay ä¼šå¯åŠ¨ä¸€ä¸ªä¸“é—¨çš„ Worker è¿›ç¨‹æ¥æ‰˜ç®¡è¿™ä¸ª Actorï¼Œè¿™ä¸ª Worker è¿›ç¨‹çš„ç”Ÿå‘½å‘¨æœŸä¸ Actor çš„ç”Ÿå‘½å‘¨æœŸç»‘å®šã€‚

### `run_ppo()` å’Œ `TaskRunner.run()`

æœ‰äº† ray çš„æ¦‚å¿µï¼Œæˆ‘ä»¬å›åˆ°æ•´ä¸ª RL è®­ç»ƒæµç¨‹çš„èµ·ç‚¹ï¼š`verl.trainer.main_ppo.py` ä¸­çš„ [`run_ppo()`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L35)ï¼Œå®ƒè´Ÿè´£åˆå§‹åŒ– Ray é›†ç¾¤ï¼Œé…ç½® CPU èµ„æºå’Œè¿è¡Œæ—¶ç¯å¢ƒå˜é‡ï¼Œå¹¶åˆ›å»ºè¿œç¨‹ TaskRunner å®ä¾‹ã€‚

```python
def run_ppo(config) -> None:
    # åˆå§‹åŒ– Ray é›†ç¾¤ï¼Œé…ç½® CPU èµ„æºå’Œè¿è¡Œæ—¶ç¯å¢ƒå˜é‡
    ray.init(
        runtime_env={"env_vars": {...}},
        num_cpus=config.ray_init.num_cpus,
    )

    # åˆ›å»ºè¿œç¨‹ TaskRunner å®ä¾‹
    # TaskRunner æ˜¯ Ray ä¸­çš„ä¸€ä¸ªè¿œç¨‹ actorï¼Œå®ƒå°†åœ¨ Ray é›†ç¾¤ä¸Šå¼‚æ­¥æ‰§è¡Œä¸»è¦çš„è®­ç»ƒä»»åŠ¡
    runner = TaskRunner.remote()
    # å¼‚æ­¥æ‰§è¡Œè¿œç¨‹ä»»åŠ¡ runner.run()ï¼Œå¹¶ç­‰å¾…å…¶å®Œæˆ
    # é€šè¿‡ ray.get() é˜»å¡ç›´åˆ°è¿œç¨‹ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œç¡®ä¿æ•´ä¸ªåˆå§‹åŒ–æµç¨‹çš„é¡ºåºæ€§
    ray.get(runner.run.remote(config))
```

### ActorRolloutRefWorker å’Œ RayWorkerGroup çš„åˆå§‹åŒ–

`TaskRunner` æ˜¯ verl ä¸­å®ç° PPO/GRPO è®­ç»ƒçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒé€šè¿‡å°†æ•´ä¸ª RL è®­ç»ƒæµç¨‹å°è£…åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Ray Actor ä¸­ï¼Œå®ç°äº†ä»»åŠ¡çš„å°è£…ã€èµ„æºéš”ç¦»å’Œåˆ†å¸ƒå¼åè°ƒã€‚ä¸ºäº†è§£é‡Šæ¸…æ¥š `TaskRunner`ï¼Œæˆ‘ä»¬å°† verl å½“ä¸­æœ€è®©äººè´¹è§£ä¸”æœ€å¤æ‚çš„ `ActorRolloutRefWorker` å’Œ `RayWorkerGroup` è¿™ä¸¤ä¸ªç±»æå‰è§£é‡Šæ¸…æ¥šã€‚

æˆ‘ä»¬å…ˆä¸è®¨è®ºè¿™ä¸¤ä¸ªç±»åŠå…¶åŸºç±»çš„å…·ä½“æ„ä¹‰ï¼Œå…ˆè®¨è®ºæ¸…æ¥šå…¶å®ä¾‹å¯¹è±¡çš„åˆ›å»ºè¿‡ç¨‹ã€‚æˆ‘ä»¬æ³¨æ„åˆ°è¿™æ®µ `TaskRunner` çš„åˆå§‹åŒ–ä¸­å¼•å…¥ `ActorRolloutRefWorker` å’Œ `RayWorkerGroup` çš„ç›¸å…³ä»£ç ï¼š

<details>
<summary>TaskRunner ä¸­å¼•å…¥ ActorRolloutRefWorker</summary>

```python
        # Define worker classes based on the actor strategy.
        if config.actor_rollout_ref.actor.strategy in ["fsdp", "fsdp2"]:
            assert config.critic.strategy in ["fsdp", "fsdp2"]
            from verl.single_controller.ray import RayWorkerGroup
            from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker

            actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == "async" else ActorRolloutRefWorker
            ray_worker_group_cls = RayWorkerGroup

        elif config.actor_rollout_ref.actor.strategy == "megatron":
            assert config.actor_rollout_ref.actor.strategy == config.critic.strategy
            from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup
            from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker

            actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == "async" else ActorRolloutRefWorker
            ray_worker_group_cls = NVMegatronRayWorkerGroup

        else:
            raise NotImplementedError

        from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role

        # Map roles to their corresponding remote worker classes.
        role_worker_mapping = {
            Role.ActorRollout: ray.remote(actor_rollout_cls),
            Role.Critic: ray.remote(CriticWorker),
        }

        # Define the resource pool specification.
        # Map roles to the resource pool.
        global_pool_id = "global_pool"
        resource_pool_spec = {
            global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,
        }
        mapping = {
            Role.ActorRollout: global_pool_id,
            Role.Critic: global_pool_id,
        }
```

</details>

å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œåœ¨ `TaskRunner` çš„åˆå§‹åŒ–ä¸­ï¼Œä¼šæ ¹æ®å„ç±»é…ç½®å¼•å…¥å¯¹åº”çš„ `ActorRolloutRefWorker / AsyncActorRolloutRefWorker` ç±»ä»¥åŠ `RayWorkerGroup / NVMegatronRayWorkerGroup` ç±»ã€‚å¯¹äº SGLang è€Œè¨€ï¼Œä¸å­˜åœ¨ `AsyncActorRolloutRefWorker`ã€‚`ActorRolloutRefWorker` ç±»ç›´æ¥é€šè¿‡ `ray.remote(ActorRolloutRefWorker)` åˆ›å»ºä¸€ä¸ªè¿œç¨‹çš„ Ray Actorï¼Œå°†å…¶åŒ…è£…æˆä¸€ä¸ª Ray Actor ç±»ã€‚æ­¤æ—¶è¿˜è¿˜æ²¡æœ‰åˆ›å»ºä»»ä½•å®ä¾‹ï¼Œä¹Ÿæ²¡æœ‰åˆ†é…èµ„æºã€‚é‚£ä¹ˆï¼Œ`ActorRolloutRefWorker` ç±»åˆ°åº•åœ¨å“ªå„¿å®ä¾‹åŒ–å¹¶åˆ†é…èµ„æºçš„å‘¢ï¼Ÿ

å®é™…ä¸Šï¼Œåœ¨ `main_ppo.py` çš„ [172 è¡Œ](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L172)ï¼Œæ„é€ äº† `RayPPOTrainer` ç±»ï¼Œéšåè°ƒç”¨äº† `RayPPOTrainer.init_workers()` æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æŸ¥çœ‹ `RayPPOTrainer.init_workers()` æ–¹æ³•çš„[ç›¸å…³ä»£ç ](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715)ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæ¯ä¸€ä¸ª RL worker ç±»ï¼ˆæ¯”å¦‚ ActorRolloutRefWorkerï¼‰éƒ½ä¼šåˆ›é€ ä¸€ä¸ª work groupï¼ˆverl ä¸­çš„å„ç§ wg å˜é‡ï¼‰ï¼Œéšåè°ƒç”¨æ¯ä¸ª worker group çš„ `init_model()` æ–¹æ³•ï¼Œè€Œè¿™äº› worker group å®é™…ä¸Šéƒ½æ˜¯ `RayWorkerGroup` çš„å®ä¾‹ã€‚`RayWorkerGroup` çš„æ ¸å¿ƒä½œç”¨æ˜¯èµ„æºè°ƒåº¦çš„æ ¸å¿ƒä¸­é—´å±‚ï¼Œç»Ÿä¸€äº†å„ç§ RL worker ï¼ˆæ¯”å¦‚ ActorRolloutRefWorkerã€CriticWorkerï¼‰çš„æ¥å£ï¼Œè¿›è¡Œç»Ÿä¸€ç®¡ç†ï¼š

```python
wg_dict = self.ray_worker_group_cls(
    resource_pool=resource_pool,  # åªéœ€è¦æŒ‡å®šèµ„æºæ± 
    ray_cls_with_init=worker_dict_cls,  # æŒ‡å®š worker ç±»
    device_name=self.device_name,
)

# æ‰€æœ‰ worker éƒ½é€šè¿‡ç›¸åŒçš„æ¨¡å¼åˆ›å»ºï¼Œæˆ‘è¿™é‡Œè¿›è¡Œç®€åŒ–ï¼Œå®é™…ä¸Šçš„ä»£ç æ¯”è¾ƒç¹ç
actor_rollout_wg = RayWorkerGroup(resource_pool, actor_rollout_cls)
critic_wg = RayWorkerGroup(resource_pool, critic_cls)
ref_policy_wg = RayWorkerGroup(resource_pool, ref_policy_cls)
```

<details>
<summary>å„ç§ worker group å®é™…ä¸Šçš„åˆå§‹åŒ–</summary>

è¿™éƒ¨åˆ†ä»£ç åœ¨ [`ray_trainer.py`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L771) ä¸­ï¼š

```python
for resource_pool, class_dict in self.resource_pool_to_cls.items():
    worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)
    wg_dict = self.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls, device_name=self.device_name, **wg_kwargs)
    spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())
    all_wg.update(spawn_wg)

if self.use_critic:
    self.critic_wg = all_wg["critic"]
    self.critic_wg.init_model()

if self.use_reference_policy and not self.ref_in_actor:
    self.ref_policy_wg = all_wg["ref"]
    self.ref_policy_wg.init_model()

if self.use_rm:
    self.rm_wg = all_wg["rm"]
    self.rm_wg.init_model()

# we should create rollout at the end so that vllm can have a better estimation of kv cache memory
self.actor_rollout_wg = all_wg["actor_rollout"]
self.actor_rollout_wg.init_model()

# create async rollout manager and request scheduler
self.async_rollout_mode = False
if self.config.actor_rollout_ref.rollout.mode == "async":
    from verl.workers.rollout.async_server import AsyncLLMServerManager

    self.async_rollout_mode = True
    self.async_rollout_manager = AsyncLLMServerManager(
        config=self.config,
        worker_group=self.actor_rollout_wg,
    )
```

æ³¨æ„åˆ° `ray_worker_group_cls` å°±æ˜¯ `RayWorkerGroup` ç±»ï¼Œè€Œ `worker_dict_cls` å°±æ˜¯ `ActorRolloutRefWorker` ç±»ï¼Œæ‰€ä»¥æˆ‘çš„ç®€åŒ–æ˜¯å¾ˆåˆç†çš„ã€‚

</details>

å¦‚æ­¤ä»¥æ¥ï¼Œ`ActorRolloutRefWorker` å§”æ‰˜ç»™ `RayWorkerGroup` è¿›è¡Œåˆå§‹åŒ–ã€‚`RayWorkerGroup` è¿™ä¸ªç±»å°±æ˜¯ä¸“é—¨ç”¨äºèµ„æºè°ƒåº¦çš„ã€‚é€šè¿‡å…¶ç»Ÿä¸€çš„ `_init_with_resource_pool` [æ–¹æ³•](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/ray/base.py#L313)ï¼Œä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª workerï¼Œæœ€ç»ˆå®ä¾‹åŒ–æ¯ç§ RL worker å¹¶åˆ†é…èµ„æºã€‚

```python
def _init_with_resource_pool(self, resource_pool, ray_cls_with_init, ...):
    # ä» Ray ç”³è¯· Placement Groups
    pgs = resource_pool.get_placement_groups(strategy=strategy, device_name=self.device_name)
    
    # ä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª worker
    for local_rank in range(local_world_size):
        worker = ray_cls_with_init(placement_group=pg, placement_group_bundle_idx=local_rank, ...)
        self._workers.append(worker)
```

è¯»åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬åŸºæœ¬å¯¹ verl æœ‰äº†ä¸€äº›æ„Ÿè§‰ã€‚æ³¨æ„åˆ°ï¼Œåœ¨ verl å½“ä¸­æœ‰ä¸¤ä¸ªå¸¦æœ‰ Worker çš„ base classï¼Œä¸€ä¸ªå°±å«åš [`Worker`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/base/worker.py#L77)ï¼Œå¦ä¸€ä¸ªå«åš [`WorkerGroup`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/base/worker_group.py#L121)ã€‚`Worker` æ˜¯ RL é‡Œé¢çš„é€»è¾‘ç±»ï¼ˆæ¯”å¦‚ actor å’Œ criticï¼‰,å®é™…ç®¡ç† RL çš„æ•°æ®æµï¼Œè€Œ `WorkerGroup` åªç”¨äºåˆ†å¸ƒå¼ç³»ç»Ÿçš„èµ„æºè°ƒåº¦ã€‚


æ­¤å¤–ï¼Œä» `actor_rollout_wg` å’Œ `ref_policy_wg` çš„å®ä¾‹åŒ–å½“ä¸­ï¼Œä¹Ÿèƒ½çœ‹å‡ºä¸€äº›å­¦é—®ã€‚åœ¨ `ActorRolloutRefWorker` çš„è®¾è®¡å½“ä¸­ï¼ŒActor Trainingï¼ŒActor Rollout å’Œ Reference model æ˜¯ç”¨åŒä¸€ä¸ª worker class è¿›è¡Œç®¡ç†çš„ã€‚ä½†æ˜¯ï¼Œä¹‹åå§”æ‰˜ç»™ `RayWorkerGroup` åˆ›å»º worker group å¹¶ä¸”è°ƒç”¨èµ„æºçš„æ—¶å€™ï¼ŒActor Training å’Œ Actor Rollout æ˜¯ç”±åŒä¸€ç»„ `RayWorkerGroup` è¿›è¡Œèµ„æºç®¡ç†çš„ï¼ˆè¿™äºŒè€…æœ¬æ¥å°±è¦è¢«æ”¾åœ¨åŒä¸€ä¸ªèµ„æºç»„ä¸Šåš hybird engineï¼‰ï¼Œè€Œ Reference Model æ˜¯ç”±å¦ä¸€ç»„ `RayWorkerGroup` ç®¡ç†èµ„æºçš„ã€‚

æœ€åï¼Œæˆ‘å»é—®äº†ç›¸å…³å¼€å‘è€…ï¼Œä»–ä»¬ä¹Ÿè®¤ä¸ºæŠŠ Actor Rolloutï¼ŒActor Training å’Œ Reference Model æ”¾åœ¨åŒä¸€ä¸ª worker é‡Œæ˜¯ bad design ğŸ˜‚ï¼Œä¸ç”¨çº ç»“è¿™ç§è®¾è®¡æ˜¯å¦æœ‰ä»€ä¹ˆé«˜ç»è¿œç©ï¼Œå®Œå…¨æ²¡æœ‰ã€‚

### ActorRolloutRefWorker å…·ä½“å®ç°

å¦‚å‰æ–‡æ‰€è¯´ï¼Œ`ActorRolloutRefWorker` æ˜¯ verl ä¸­ç”¨äºç®¡ç† Actor Trainingï¼ŒActor Rollout å’Œ Reference Model çš„ worker classã€‚æˆ‘ä»¬å…·ä½“æ¥åˆ†æå…¶é€»è¾‘ä¸Šå®ç°çš„åŠŸèƒ½ã€‚æ³¨æ„ï¼Œæœ¬æ–‡æ¡£åªåˆ†æ FSDP backend ä¸‹çš„å®ç°ï¼Œmegatron ç•™ä½œåæ–‡ã€‚

1. [`ActorRolloutRefWorker.__init__()`](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/workers/fsdp_workers.py#L101)

1. è°ƒç”¨ Worker åŸºç±»çš„æ„é€ å‡½æ•°ï¼Œå¹¶ä¿å­˜é…ç½®ã€‚
2. å¦‚æœ PyTorch åˆ†å¸ƒå¼ç¯å¢ƒå°šæœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–ï¼ŒåŒ…æ‹¬è®¾ç½®é€šä¿¡åç«¯å’Œè¿›ç¨‹ç»„ã€‚
3. ä¸º FSDP åˆ›å»ºè®¾å¤‡ç½‘æ ¼ï¼Œç”¨äºæ¨¡å‹å‚æ•°çš„åˆ†ç‰‡ã€‚
4. å¦‚æœå¯ç”¨ Ulysses åºåˆ—å¹¶è¡Œï¼Œåˆ™åˆå§‹åŒ–å…¶è®¾å¤‡ç½‘æ ¼ã€‚
5. æ ¹æ®ä¼ å…¥çš„ `role` å‚æ•°è®¾ç½® Worker çš„å…·ä½“è§’è‰²ï¼ˆactor, rollout, refï¼‰ã€‚
6. æ ¹æ® Worker è§’è‰²é…ç½® profilerï¼Œç”¨äºæ€§èƒ½åˆ†æã€‚
7. é…ç½® parameter offload å’Œ optimizer offloadã€‚
8. ä¸º Actorï¼ŒRollout å’Œ Reference åˆ†åˆ« normalize batch sizeã€‚

ç¬¬ 8 æ­¥ä¸­é…ç½®äº†éå¸¸å¤šçš„ batch sizeï¼›verl çš„ batch size å‚æ•°æ»¡å¤©é£ï¼Œè™½ç„¶æˆ‘ä¸ªäººè®¤ä¸ºåå­—åŸºæœ¬æ˜¯å‡†ç¡®çš„ï¼Œä½†æ˜¯ç”±äºåå­—å¤ªåƒäº†ï¼Œä¸€å®šè¦åšå‡ºä¸€äº›åŒºåˆ†ã€‚äº‹å®ä¸Šï¼Œå‚æ•°åˆ†ææˆ‘ä»¬æœ‰å•ç‹¬çš„æ–‡æ¡£ï¼Œæˆ‘å…ˆæŠŠä¸€éƒ¨åˆ†å†…å®¹æå‰å…¬å¸ƒäº†ã€‚

1. `data.train_batch_size`ï¼šåœ¨ä¸€æ¬¡å®Œæ•´çš„ PPO è¿­ä»£ï¼ˆä» rollout åˆ° trainï¼‰ä¸­ï¼Œä»æ•°æ®é›†ä¸­é‡‡æ ·å¹¶ç”¨äºç”Ÿæˆ experience çš„æ€»æ ·æœ¬æ•°é‡ï¼Œå†³å®šäº†æ¯æ¬¡ policy æ›´æ–°æ‰€ä¾æ®çš„æ•°æ®é‡ã€‚
2. `actor_rollout_ref.actor.ppo_mini_batch_size`ï¼šè¿™ä¸ªå‚æ•°çš„åå­—å…¶å®æ˜¯å‡†ç¡®çš„ï¼Œå› ä¸º mini batch SGD å°±æ˜¯æ•°æ®åˆ°è¾¾äº†ä¸€ä¸ª mini batch å°±æ›´æ–°ä¸€æ¬¡æ¨¡å‹å‚æ•°ã€‚åœ¨ verl ä¸­ï¼Œæ¨¡å‹ä¼šåœ¨æ•°æ®ç´¯ç§¯åˆ°ä¸€ä¸ª mini batch åæ›´æ–°ä¸€æ¬¡å‚æ•°ã€‚
3. `actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu`ï¼šè¿™é‡Œå…¶å®æ˜¯ gradient accumulation çš„å‚æ•°ã€‚ç”±äºä¸€ä¸ª mini batch çš„æ•°æ®é‡å¯èƒ½ä»ç„¶å¤ªå¤§ï¼Œæ— æ³•ä¸€æ¬¡æ€§å‰å‘å’Œåå‘ä¼ æ’­ï¼Œå› æ­¤éœ€è¦å°†å…¶è¿›ä¸€æ­¥æ‹†åˆ†ä¸º micro batchã€‚æ¯ä¸ª micro batch ä¼šè®¡ç®—ä¸€æ¬¡æ¢¯åº¦å¹¶ä¸”ç´¯è®¡ï¼Œä½†æ˜¯ä¸ä¼šç«‹åˆ»æ›´æ–°æ¨¡å‹å‚æ•°ã€‚å¤„ç†å®Œæ•´ä¸ª mini batch åï¼Œæ‰ç”¨ç´¯ç§¯çš„æ¢¯åº¦è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚

æ­¤å¤–ï¼Œåœ¨ verl ä¸­ï¼Œç”±äº verl å¼ºè°ƒ SPMD ç­–ç•¥ï¼Œå¯ä»¥ç†è§£ä¸ºæ¯ä¸ª RL worker æ‰€å æ®çš„æ¯ä¸ª GPU ä¸Šå¸Œæœ›è¿›è¡Œå®Œå…¨ä¸€è‡´çš„æ“ä½œï¼Œæ‰€ä»¥ verl ä¼šè¦æ±‚æ¯ä¸ª GPU çš„ micro batch size ç›¸åŒã€‚å› æ­¤ï¼Œverl ä¼šæ£€æŸ¥ train batch size / gpu æ˜¯å¦æ•´é™¤ï¼Œå¦‚æœä¸æ•´é™¤ï¼Œåˆ™æŠ¥é”™ã€‚è¿™ä¸ªè®¾å®šå…¶å®å®Œå…¨æ²¡å¿…è¦ï¼›å¯¹äº rollout è€Œè¨€ï¼ŒSGLang å®Œå…¨ä¸éœ€è¦å‘é€çš„è¯·æ±‚æ•°é‡æ•´é™¤ DP æˆ–è€… TP sizeï¼Œæ›´ä½•å†µç›´æ¥è¦æ•´é™¤ gpu æ•°é‡å‘¢ï¼Ÿä½†æ˜¯ï¼Œå› ä¸º verl ä¼šç”¨ all gather ä» rollout çš„æ¯ä¸ª worker é‡Œæ”¶é›†æ•°æ®ï¼Œè¿™å°±è¦æ±‚ rollout çš„æ¯ä¸ª worker ä¸Šåˆ†åˆ°çš„æ•°æ®ä¸€è‡´ã€‚æ›´è¿›ä¸€æ­¥ï¼Œä¸ºäº† SPMDï¼Œåˆè¦æ±‚ rollout çš„æ¯ä¸ª gpu ä¸Šåˆ†åˆ°çš„æ•°æ®ä¸€è‡´ã€‚æœ€ç»ˆï¼Œè¿™å°±å¯¼è‡´ verl çš„ train batch size å¿…é¡»æ•´é™¤ gpu æ•°é‡ã€‚

åŒºåˆ†å¥½ mini batch å’Œ micro batch åï¼Œæˆ‘ä¹Ÿæ˜¯æœ€è¿‘æ‰æ˜ç™½ PPO ä¸­æ˜¯å¦‚ä½•ç»´æŠ¤ on policy çš„ã€‚æˆ‘ä¹‹å‰ä¸€ç›´ä»¥ä¸ºæˆ‘ä»¬éƒ½æ˜¯åœ¨åšä¸¥æ ¼ on policy çš„è®­ç»ƒï¼Œä½†æ˜¯ä¸€ä¸ª train batch size ä¸‹æœ‰å¥½å‡ ä¸ª mini batchï¼Œä¼¼ä¹ç¬¬ä¸€ä¸ª mini batch ç»“æŸä¹‹åï¼Œç›®æ ‡ç­–ç•¥ï¼ˆtarget policyï¼Œè¢«è®­ç»ƒçš„ policyï¼‰å’Œè¡Œä¸ºç­–ç•¥ï¼ˆbehavior policyï¼Œç”¨äºåœ¨ç¯å¢ƒä¸­é‡‡æ ·çš„ policyï¼‰å°±ä¸ä¸€è‡´äº†ã€‚ä¸€æ¬¡é‡‡æ ·ä¼šè®­ç»ƒå¾ˆå¤šä¸ª mini batchï¼Œä»ç¬¬ä¸€ä¸ª mini batch ç»“æŸå°±ä¸æ˜¯ on policy äº†ã€‚äº‹å®ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œæˆ‘ä»¬æ³¨æ„åˆ° PPO çš„ loss functionï¼š

$$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t) \right] $$

å…¶ä¸­çš„ $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹ä¼˜åŠ¿å‡½æ•°çš„çŸ«æ­£æ¯”ä¾‹ï¼Œè€Œ $\hat{A}_t$ å°±æ˜¯ advantageã€‚å¯¹äº LLM çš„ PPO è€Œè¨€ï¼Œ$\pi_{\theta_{old}}(a_t | s_t)$ ä»£è¡¨ç€é‡‡æ ·æ—¶ behavior policy åœ¨ç»™å®š $s_t$ æ—¶ï¼Œé€‰æ‹© $a_t$ çš„æ¦‚ç‡ï¼Œè€Œ $\pi_\theta(a_t | s_t)$ å°±æ˜¯ target policy åœ¨è®­ç»ƒä¸­çš„æ¯ä¸€æ­¥ç»™å®š $s_t$ æ—¶ï¼Œé€‰æ‹© $a_t$ çš„æ¦‚ç‡ã€‚å¯¹ LLM è€Œè¨€ï¼Œ`s_t` æ˜¯ prompt å‰ç¼€ï¼Œè€Œ `a_t` ä»…ä»…æ˜¯ prompt åçš„é‚£ä¸€ä¸ª tokenã€‚è¿™ä¸€æ¦‚ç‡å…¶å®å°±æ˜¯ inference å¾—åˆ°çš„ log probsï¼›æˆ‘ä»¬å°†æ”¶é›†å¾—åˆ°çš„ (prompt, action) åˆ†åˆ«ç»è¿‡ target policy å’Œ behaviour policy å¾—åˆ° log probsï¼Œç„¶åäºŒè€… log probs ç›¸å‡å†å–å¯¹æ•°ï¼Œå°±æ˜¯çŸ«æ­£é¡¹çš„å€¼ã€‚ä»è€Œï¼Œå³ä¾¿ç¬¬ä¸€ä¸ª mini batch ä¹‹å target policy å°±å·²ç»å’Œ behaviour policy ä¸ä¸€è‡´äº†ï¼Œä»ç„¶å¯ä»¥é€šè¿‡ log probs è¿›è¡ŒçŸ«æ­£ï¼Œä¹Ÿå³ importance samplingã€‚

è¿™æ ·ä¸€æ¥ï¼Œåˆæœ‰äº†ä¸¤ä¸ªé—®é¢˜ï¼šlog probs åº”è¯¥å¦‚ä½•å¾—åˆ°ï¼Ÿå®é™…ä¸Šæ¯æ¬¡é‡‡æ ·æ—¶éƒ½æ˜¯å‘é€ç»™ rolloutå›ºå®šæ•°é‡çš„ requestsï¼Œå¦‚æœæ¯ä¸ª (prompt, action) å¯¹éƒ½ä¼šè®¡ç®—ä¸€æ¬¡ loss çš„è¯ï¼Œå²‚ä¸æ˜¯æ›´é•¿çš„ sequence ä¼šè®¡ç®—æ›´å¤šæ¬¡ï¼Ÿ

å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œè¿™åˆæ˜¯ç»å…¸çš„[ç²¾åº¦é—®é¢˜](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md#introduction)ã€‚å¦‚åŒæˆ‘åœ¨é“¾æ¥åˆ°çš„æ–‡ç« ä¸­æ‰€è¯´çš„ï¼Œrollout engine ç›®å‰åªæœ‰é‡‡æ ·å¾—åˆ°çš„ token èƒ½ç”¨ï¼Œè€Œå¾—åˆ°çš„ log probs ä»¥åŠ reward ç²¾åº¦éƒ½ä¸å¤Ÿï¼Œä¸èƒ½ç”¨äºè®­ç»ƒã€‚behaviour policy å’Œ target policy ä¸ºäº†åš importance sampling æ‰€éœ€çš„ log probs éƒ½å¾—ç”¨ training engine é‡ç®—ã€‚ä¸è¿‡è¦ç®—èµ·æ¥ä¹Ÿä¸éº»çƒ¦ï¼Œåœ¨ç¬¬ä¸€ä¸ª mini batch å¯åŠ¨å‰ï¼Œè¿™æ—¶å€™ target behaviour æ˜¯ä¸€è‡´çš„ï¼Œé‡ç®— log probs å¹¶ä¸”å­˜ä¸‹æ¥å³å¯ã€‚

å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œçš„ç¡®å¦‚æ­¤ã€‚ä¸€æ¡å¾ˆé•¿çš„ prompt + answer åºåˆ—ç¡®å®ä¼šäº§ç”Ÿéå¸¸å¤šçš„ (prompt, action) å¯¹ï¼Œå…¶ä¸­æ¯ä¸ªå¯¹éƒ½å¯ä»¥çœ‹ä½œä¸€ä¸ª (state, action) å¯¹ã€‚è€Œä¸”ç†è®ºä¸Šæ¯ä¸ªè¿™æ ·çš„ (prompt, action) å¯¹éƒ½ä¼šå‚ä¸ Loss çš„è®¡ç®—ã€‚è¿™ç¡®å®å¯èƒ½å¯¼è‡´é•¿åºåˆ—ä¸­çš„ token ä¼šåœ¨ Loss è®¡ç®—ä¸­å æ®æ›´å¤§çš„æ¯”ä¾‹ï¼Œè®©æ¨¡å‹è¿‡åº¦å…³æ³¨é•¿åºåˆ—çš„ä¼˜åŒ–ï¼Œè€Œå¯¹çŸ­åºåˆ—çš„ä¼˜åŒ–ä¸è¶³ã€‚ä¸è¿‡ï¼Œverl çš„ rollout engine ä¼šè‡ªåŠ¨å¯¹æ¯ä¸ª (prompt, action) å¯¹è¿›è¡ŒåŠ æƒï¼Œä»è€Œè®©é•¿åºåˆ—å’ŒçŸ­åºåˆ—çš„ token åœ¨ Loss è®¡ç®—ä¸­å æ®ç›¸åŒçš„æƒé‡ã€‚ä¸ºäº†ç¼“è§£è¿™ç§æƒ…å†µï¼Œæœ‰å¾ˆå¤šç›¸å…³æ–¹æ³•ï¼š

<details>
<summary>æ ·æœ¬åŠ æƒæ–¹æ³•</summary>

åºåˆ—çº§åˆ«åŠ æƒï¼š ä¸€ç§ç›´æ¥çš„æ–¹æ³•æ˜¯åœ¨è®¡ç®— Loss æ—¶ï¼Œç»™æ¥è‡ªä¸åŒåºåˆ—çš„æ ·æœ¬èµ‹äºˆä¸åŒçš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œç»™æ¯ä¸ªå®Œæ•´åºåˆ—ä¸€ä¸ªå›ºå®šçš„æƒé‡ï¼ˆæ¯”å¦‚ 1ï¼‰ï¼Œç„¶åå°†è¿™ä¸ªæƒé‡å‡åŒ€åˆ†é…ç»™è¯¥åºåˆ—ä¸­çš„æ¯ä¸ª (prompt, action) å¯¹ã€‚è¿™æ ·ï¼Œæ— è®ºåºåˆ—å¤šé•¿ï¼Œå®ƒå¯¹æ€» Loss çš„è´¡çŒ®éƒ½ç›¸åŒã€‚å¦‚æœä¸€ä¸ªåºåˆ—æœ‰ N ä¸ª tokenï¼Œé‚£ä¹ˆæ¯ä¸ª (prompt, action) å¯¹çš„æƒé‡å°±æ˜¯ 1/Nã€‚

æŒ‰é•¿åº¦åˆ†æ¡¶ï¼š åœ¨æ•°æ®æ”¶é›†åï¼Œå¯ä»¥æ ¹æ®åºåˆ—é•¿åº¦å¯¹æ ·æœ¬è¿›è¡Œæ’åºï¼Œå¹¶å°è¯•å°†ç›¸ä¼¼é•¿åº¦çš„åºåˆ—æ”¾å…¥åŒä¸€ä¸ª mini-batchã€‚è¿™æœ‰åŠ©äºæé«˜è®¡ç®—æ•ˆç‡ï¼Œå› ä¸ºå¯ä»¥å‡å°‘ paddingï¼Œä½†å¯¹äºè§£å†³ Loss è´¡çŒ®ä¸å‡è¡¡çš„ä½œç”¨æœ‰é™ã€‚

å›ºå®š Token æ•°é‡çš„æ‰¹æ¬¡ï¼š æœ€å¸¸è§ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ˜¯æ„å»ºæ‰¹æ¬¡æ—¶ï¼Œä¸å›ºå®šæ ·æœ¬æ•°é‡ï¼Œè€Œæ˜¯å›ºå®šæ‰¹æ¬¡ä¸­çš„æ€» token æ•°é‡ã€‚è¿™æ ·ï¼Œä¸€ä¸ª mini-batch å¯èƒ½åŒ…å« 4 æ¡é•¿åºåˆ—ï¼Œä¹Ÿå¯èƒ½åŒ…å« 40 æ¡çŸ­åºåˆ—ï¼Œç¡®ä¿æ¯æ¬¡æ›´æ–°æ—¶å¤„ç†çš„æ€»è®¡ç®—é‡å’Œæ¢¯åº¦æ¥æºçš„æ€» token æ•°æ˜¯æ’å®šçš„ï¼Œä»è€Œç¼“è§£é•¿çŸ­åºåˆ—çš„ä¸å‡è¡¡é—®é¢˜ã€‚

Loss å½’ä¸€åŒ–ï¼šåœ¨è®¡ç®—æ¯ä¸ª mini-batch çš„ Loss æ—¶ï¼Œå¯ä»¥å°†å…¶é™¤ä»¥è¯¥ mini-batch ä¸­å®é™…çš„ token æ•°é‡ã€‚è¿™ç¡®ä¿äº† Loss å€¼ä¸ä¼šä»…ä»…å› ä¸ºæ‰¹æ¬¡ä¸­åŒ…å«äº†æ›´å¤š token è€Œå¢å¤§ï¼Œä»è€Œä¸ºä¸åŒå¤§å°çš„ mini-batchesï¼ˆå¦‚æœä¸æ˜¯æŒ‰å›ºå®š token æ•°æ„å»ºï¼‰æä¾›ä¸€ä¸ªå…¬å¹³çš„æ¯”è¾ƒåŸºç¡€ã€‚

æˆªæ–­ï¼šè®¾å®šä¸€ä¸ª max_length å‚æ•°ï¼Œé™åˆ¶æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚è™½ç„¶è¿™ä¸ç›´æ¥è§£å†³å·²æœ‰é•¿åºåˆ—çš„æƒé‡é—®é¢˜ï¼Œä½†å¯ä»¥é˜²æ­¢ç”Ÿæˆè¿‡é•¿çš„åºåˆ—ï¼Œä»è€Œé™åˆ¶æç«¯ä¸å‡è¡¡çš„å‘ç”Ÿã€‚

</details>

whateverï¼Œè§£é‡Šäº†è¿™ä¹ˆå¤šï¼Œé¡ºç€ç†è§£ verl çš„æ¡†æ¶è¿›ä¸€æ­¥å­¦ä¹ äº† RL ç®—æ³•å’Œç³»ç»Ÿï¼Œè¿™é‡Œå…¶å®å’Œ multi-turn éƒ½è¿˜æ²¡æœ‰å…³ç³»ã€‚æˆ‘ä»¬è¿˜æ˜¯å›åˆ° `ActorRolloutRefWorker` çš„æºç ä¸Šã€‚

<details>
<summary> `ActorRolloutRefWorker.__init__()` </summary>

```python
def __init__(self, config: DictConfig, role: str):
        # åˆå§‹åŒ– Worker åŸºç±»
        Worker.__init__(self)

        # å­˜å‚¨é…ç½®ä¿¡æ¯
        self.config = config
        import torch.distributed

        # å¦‚æœåˆ†å¸ƒå¼ç¯å¢ƒå°šæœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–
        if not torch.distributed.is_initialized():
            rank = int(os.environ.get("RANK", 0))
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            torch.distributed.init_process_group(backend=f"cpu:gloo,{get_device_name()}:{get_nccl_backend()}", rank=rank, world_size=world_size)

        # ä¸º FSDP æ„å»ºè®¾å¤‡ç½‘æ ¼
        world_size = torch.distributed.get_world_size()
        self.device_mesh = create_device_mesh(world_size=world_size, fsdp_size=self.config.actor.fsdp_config.fsdp_size)

        # ä¸º Ulysses åºåˆ—å¹¶è¡Œæ„å»ºè®¾å¤‡ç½‘æ ¼
        self.ulysses_device_mesh = None
        self.ulysses_sequence_parallel_size = self.config.actor.get("ulysses_sequence_parallel_size", 1)
        dp = world_size // self.ulysses_sequence_parallel_size
        if self.ulysses_sequence_parallel_size > 1:
            self.ulysses_device_mesh = init_device_mesh(device_name, mesh_shape=(dp, self.ulysses_sequence_parallel_size), mesh_dim_names=["dp", "sp"])

        # åˆå§‹åŒ– Ulysses åˆ†ç‰‡ç®¡ç†å™¨
        self.ulysses_sharding_manager = FSDPUlyssesShardingManager(self.ulysses_device_mesh)
        # è·å– LoRA rank å’Œæ˜¯å¦ä½¿ç”¨ LoRA çš„æ ‡å¿—
        self._lora_rank = self.config.model.get("lora_rank", 0)
        self._is_lora = self._lora_rank > 0

        # è®¾ç½® Worker è§’è‰²å’Œç›¸å…³æ ‡å¿—
        self.role = role
        assert self.role in ["actor", "rollout", "ref", "actor_rollout", "actor_rollout_ref"]

        self._is_actor = self.role in ["actor", "actor_rollout", "actor_rollout_ref"]
        self._is_rollout = self.role in ["rollout", "actor_rollout", "actor_rollout_ref"]
        self._is_ref = self.role in ["ref", "actor_rollout_ref"]

        profiler_config: Optional[ProfilerConfig] = None
        # æ ¹æ®è§’è‰²è·å–æ€§èƒ½åˆ†æé…ç½®
        if self._is_actor:
            profiler_config = omega_conf_to_dataclass(config.actor.get("profiler", {}), ProfilerConfig)
        if self._is_rollout:
            profiler_config = omega_conf_to_dataclass(config.rollout.get("profiler", {}), ProfilerConfig)
        if self._is_ref:
            profiler_config = omega_conf_to_dataclass(config.ref.get("profiler", {}), ProfilerConfig)

        # åˆå§‹åŒ–åˆ†å¸ƒå¼æ€§èƒ½åˆ†æå™¨
        DistProfilerExtension.__init__(self, DistProfiler(rank=self.rank, config=profiler_config))

        # è®¾ç½®å‚æ•°å’Œä¼˜åŒ–å™¨å¸è½½æ ‡å¿—
        self._is_offload_param = False
        self._is_offload_optimizer = False
        if self._is_actor:
            self._is_offload_param = self.config.actor.fsdp_config.get("param_offload", False)
            self._is_offload_optimizer = self.config.actor.fsdp_config.get("optimizer_offload", False)
        elif self._is_ref:
            self._is_offload_param = self.config.ref.fsdp_config.get("param_offload", False)

        # è§„èŒƒåŒ– actor ç›¸å…³é…ç½®
        if self._is_actor:
            self.config.actor.ppo_mini_batch_size *= self.config.rollout.n
            self.config.actor.ppo_mini_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size
            assert self.config.actor.ppo_mini_batch_size > 0, f"ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be larger than 0 after normalization"
            # micro bsz
            if self.config.actor.ppo_micro_batch_size is not None:
                self.config.actor.ppo_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size
                self.config.actor.ppo_micro_batch_size_per_gpu = self.config.actor.ppo_micro_batch_size

            if self.config.actor.ppo_micro_batch_size_per_gpu is not None:
                assert self.config.actor.ppo_mini_batch_size % self.config.actor.ppo_micro_batch_size_per_gpu == 0, f"normalized ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be divisible by ppo_micro_batch_size_per_gpu {self.config.actor.ppo_micro_batch_size_per_gpu}"
                assert self.config.actor.ppo_mini_batch_size // self.config.actor.ppo_micro_batch_size_per_gpu > 0, f"normalized ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be larger than ppo_micro_batch_size_per_gpu {self.config.actor.ppo_micro_batch_size_per_gpu}"

        # è§„èŒƒåŒ– rollout ç›¸å…³é…ç½®
        if self._is_rollout and self.config.rollout.log_prob_micro_batch_size is not None:
            self.config.rollout.log_prob_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size
            self.config.rollout.log_prob_micro_batch_size_per_gpu = self.config.rollout.log_prob_micro_batch_size
        # è§„èŒƒåŒ– ref ç›¸å…³é…ç½®
        if self._is_ref and self.config.ref.log_prob_micro_batch_size is not None:
            self.config.ref.log_prob_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size
            self.config.ref.log_prob_micro_batch_size_per_gpu = self.config.ref.log_prob_micro_batch_size
```

</details>

ã€todoã€‘

#### `ActorRolloutRefWorker.init_model()`

```python
import importlib
import torch
from omegaconf import OmegaConf
from typing import Optional
from verl.utils.distributed import dispatch
from verl.utils.distributed import Dispatch
from verl.models.mcore.utils import offload_megatron_model_to_cpu, offload_megatron_optimizer
from verl.utils.flops import FlopsCounter
from verl.trainer.ppo.ray_trainer import RayPPOTrainer
from verl.utils.torch_dist import get_torch_device
from verl.utils.megatron import (
    get_model,
    ModelType,
    load_megatron_gptmodel_weights,
    init_megatron_optim_config,
    get_megatron_optimizer,
    get_megatron_optimizer_param_scheduler,
    load_mcore_dist_weights,
)
from verl.models.ppo_actor import MegatronPPOActor
from verl.config import PrecisionType
from verl.utils.generation import get_generation_config
from verl.checkpoint import MegatronCheckpointManager
from verl.registry import registry

@registry.register(RayPPOTrainer)
class RayPPOTrainer:
    def __init__(self, config, tokenizer, processor, role_worker_mapping, resource_pool_manager, ray_worker_group_cls, reward_fn, val_reward_fn, train_dataset, val_dataset, collate_fn, train_sampler, device_name):
        # ... (previous init)
        self.ref_in_actor = config.actor_rollout_ref.get("ref_in_actor", False)

    def init_workers(self):
        # ... (previous init_workers)

@registry.register(ActorRolloutRefWorker)
class ActorRolloutRefWorker(Worker, WorkerProfilerExtension):
    def __init__(self, config: DictConfig, role: str, name=None):
        # ... (previous init)
        self._is_offload_param = self.config.actor.megatron.get("offload_param", False)
        self._is_offload_optimizer = self.config.actor.megatron.get("offload_optimizer", False)
        self._ref_is_offload_param = self.config.ref.megatron.get("offload_param", False)
        self.share_embeddings_and_output_weights = self.config.model.get("share_embeddings_and_output_weights", True)
        self.local_path = None
        self.hf_config = None
        self.tf_config = None

    def _init_hf_config_and_tf_config(self, model_path, trust_ckpt_path, dtype, override_model_config, override_transformer_config, trust_remote_code):
        from transformers import AutoConfig, AutoTokenizer
        from megatron.tokenizer import build_tokenizer_config
        from megatron.config import TransformerConfig

        hf_config = AutoConfig.from_pretrained(model_path, trust_remote_code=trust_remote_code, **override_model_config)
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=trust_remote_code)
        tokenizer_config = build_tokenizer_config(tokenizer)
        tf_config = TransformerConfig(tokenizer_config=tokenizer_config, override=override_transformer_config, **hf_config.to_dict())
        tf_config.data_type = dtype
        tf_config.params_dtype = dtype
        self.hf_config = hf_config
        self.tf_config = tf_config
        self.local_path = trust_ckpt_path

    @dispatch(dispatch_mode=Dispatch.ONE_TO_ALL)
    def init_model(self):
        # å¤–éƒ¨åº“å¯¼å…¥
        if self.config.model.get("external_lib", None) is not None:
            importlib.import_module(self.config.model.external_lib)

        # é…ç½®è§£æ
        override_model_config = OmegaConf.to_container(self.config.model.get("override_config", OmegaConf.create()))
        if self._is_actor:
            override_transformer_config = OmegaConf.to_container(self.config.actor.megatron.get("override_transformer_config", OmegaConf.create()), resolve=True)
        elif self._is_ref:
            override_transformer_config = OmegaConf.to_container(self.config.ref.megatron.get("override_transformer_config", OmegaConf.create()), resolve=True)
        else:
            override_transformer_config = None

        # æ•°æ®ç±»å‹è®¾ç½®
        self.param_dtype = torch.bfloat16
        self.dtype = PrecisionType.to_dtype(self.param_dtype)

        # Actor å’Œ Rollout æ¨¡å‹æ„å»º
        if self._is_actor or self._is_rollout:
            optim_config = self.config.actor.optim if self._is_actor else None
            self.actor_module, self.actor_optimizer, self.actor_optimizer_scheduler, self.actor_model_config, self.actor_optim_config = self._build_model_optimizer(
                model_path=self.config.model.path,
                optim_config=optim_config,
                override_model_config=override_model_config,
                override_transformer_config=override_transformer_config,
            )

            # å‚æ•°å¸è½½
            if self._is_offload_param:
                offload_megatron_model_to_cpu(self.actor_module)
            if self._is_offload_optimizer:
                offload_megatron_optimizer(self.actor_optimizer)

        # Actor åˆå§‹åŒ–
        if self._is_actor:
            self.actor = MegatronPPOActor(
                config=self.config.actor,
                model_config=self.actor_model_config,
                hf_config=self.hf_config,
                tf_config=self.tf_config,
                actor_module=self.actor_module,
                actor_optimizer=self.actor_optimizer,
            )

        # Rollout åˆå§‹åŒ–
        if self._is_rollout:
            trust_remote_code = self.config.model.get("trust_remote_code", False)
            self.rollout, self.sharding_manager = self._build_rollout(trust_remote_code=trust_remote_code)
            self.rollout.sharding_manager = self.sharding_manager

        # Reference Policy åˆå§‹åŒ–
        if self._is_ref:
            self.ref_module, self.ref_model_config = self._build_model_optimizer(
                model_path=self.config.model.path,
                optim_config=None,
                override_model_config=override_model_config,
                override_transformer_config=override_transformer_config,
            )
            self.ref_policy = MegatronPPOActor(
                config=self.config.ref,
                model_config=self.ref_model_config,
                hf_config=self.hf_config,
                tf_config=self.tf_config,
                actor_module=self.ref_module,
                actor_optimizer=None,
            )
            if self._ref_is_offload_param:
                offload_megatron_model_to_cpu(self.ref_module)

        # æ£€æŸ¥ç‚¹ç®¡ç†å™¨åˆå§‹åŒ–
        if self._is_actor:
            from verl.utils.model_utils import print_model_size
            self.flops_counter = FlopsCounter(self.actor_model_config)
            self.checkpoint_mananager = MegatronCheckpointManager(
                config=self.config,
                actor=self.actor,
                critic=getattr(self, "critic", None),
                actor_model_config=self.actor_model_config,
            )
            if self.rank == 0:
                print_model_size(self.actor_module[0])

        get_torch_device().empty_cache()
```

`ActorRolloutRefWorker.init_model()` å‡½æ•°æ˜¯æ¨¡å‹åˆå§‹åŒ–çš„å…³é”®ç‚¹ï¼Œä¸»è¦è´Ÿè´£ï¼š

1.  **å¤–éƒ¨åº“å¯¼å…¥**ï¼šå¦‚æœé…ç½®ä¸­æŒ‡å®šäº†å¤–éƒ¨åº“ï¼Œåˆ™è¿›è¡Œå¯¼å…¥ã€‚
2.  **é…ç½®è§£æ**ï¼šè§£ææ¨¡å‹å’Œ Transformer çš„è¦†ç›–é…ç½®ã€‚
3.  **æ•°æ®ç±»å‹è®¾ç½®**ï¼šè®¾ç½®æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚
4.  **Actor å’Œ Rollout æ¨¡å‹æ„å»º**ï¼šæ ¹æ® Worker è§’è‰²æ„å»º Actor æ¨¡å—ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚
5.  **å‚æ•°å¸è½½**ï¼šå¦‚æœé…ç½®äº†å‚æ•°å¸è½½ï¼Œåˆ™å°† Megatron æ¨¡å‹å’Œä¼˜åŒ–å™¨å¸è½½åˆ° CPUï¼Œä»¥ä¼˜åŒ–å†…å­˜ã€‚
6.  **Actor åˆå§‹åŒ–**ï¼šå¦‚æœå½“å‰ Worker æ˜¯ Actor è§’è‰²ï¼Œåˆ™å®ä¾‹åŒ– `MegatronPPOActor`ã€‚
7.  **Rollout åˆå§‹åŒ–**ï¼šå¦‚æœå½“å‰ Worker æ˜¯ Rollout è§’è‰²ï¼Œåˆ™æ„å»º Rollout æ¨¡å—å’Œåˆ†ç‰‡ç®¡ç†å™¨ï¼ˆé€šè¿‡ `_build_rollout()`ï¼‰ã€‚
8.  **Reference Policy åˆå§‹åŒ–**ï¼šå¦‚æœå½“å‰ Worker æ˜¯ Reference Policy è§’è‰²ï¼Œåˆ™æ„å»ºå…¶æ¨¡å—å¹¶å®ä¾‹åŒ– `MegatronPPOActor`ã€‚
9.  **æ£€æŸ¥ç‚¹ç®¡ç†å™¨åˆå§‹åŒ–**ï¼šå¦‚æœå½“å‰ Worker æ˜¯ Actor è§’è‰²ï¼Œåˆ™åˆå§‹åŒ– `FlopsCounter` å’Œ `MegatronCheckpointManager`ï¼Œç”¨äºè®¡ç®—æ¨¡å‹æµ®ç‚¹è¿ç®—é‡å’Œç®¡ç†æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚
10. **æ¸…ç†ç¼“å­˜**ï¼šæ¸…ç† PyTorch ç¼“å­˜ä»¥é‡Šæ”¾å†…å­˜ã€‚

#### `ActorRolloutRefWorker._build_model_optimizer()`

```python
from verl.utils.torch_dist import get_device_name
from verl.models.mcore import get_model
from verl.config import ModelType

@registry.register(ActorRolloutRefWorker)
class ActorRolloutRefWorker:
    # ... (previous methods)

    def _build_model_optimizer(self, model_path, optim_config, override_model_config, override_transformer_config):
        # é…ç½®åˆå§‹åŒ–
        self._init_hf_config_and_tf_config(model_path, model_path, self.dtype, override_model_config, override_transformer_config, self.config.model.get("trust_remote_code", False))
        self.generation_config = get_generation_config(self.local_path)

        # æ¨¡å‹æä¾›è€…å‡½æ•°
        def megatron_actor_model_provider(pre_process, post_process):
            from verl.models.mcore import init_mcore_model
            parallel_model = init_mcore_model(
                self.tf_config,
                self.hf_config,
                pre_process,
                post_process,
                share_embeddings_and_output_weights=self.share_embeddings_and_output_weights,
                value=False,
                freeze_moe_router=override_model_config.get("moe_config", {}).get("freeze_moe_router", False)
            )
            parallel_model.to(get_device_name())
            return parallel_model

        # Actor å’Œ Rollout æ¨¡å‹æ„å»º
        if self._is_actor and self._is_rollout:
            actor_module = get_model(
                megatron_actor_model_provider,
                wrap_with_ddp=True,
                use_distributed_optimizer=self.config.actor.megatron.use_distributed_optimizer,
            )
            if self.config.actor.load_weight:
                if self.config.actor.megatron.use_dist_checkpointing:
                    load_mcore_dist_weights(actor_module, self.config.actor.megatron.dist_checkpointing_path, is_value_model=False)
                else:
                    load_megatron_gptmodel_weights(self.config, self.hf_config, actor_module, params_dtype=self.dtype, is_value_model=False)

            # Reference Policy æ¨¡å‹æ„å»º
        elif self._is_ref:
            ref_module = get_model(
                model_provider_func=megatron_actor_model_provider,
                model_type=ModelType.encoder_or_decoder,
                wrap_with_ddp=False,
                use_distributed_optimizer=self.config.ref.megatron.use_distributed_optimizer,
            )
            if self.config.ref.load_weight:
                if self.config.ref.megatron.use_dist_checkpointing:
                    load_mcore_dist_weights(ref_module, self.config.ref.megatron.dist_checkpointing_path, is_value_model=False)
                else:
                    load_megatron_gptmodel_weights(self.config, self.hf_config, ref_module, params_dtype=self.dtype, is_value_model=False)
            return ref_module, self.hf_config

        # ä¼˜åŒ–å™¨åˆå§‹åŒ–
        if self._is_actor:
            optim_config_megatron = init_megatron_optim_config(optim_config)
            actor_optimizer = get_megatron_optimizer(model=actor_module, config=optim_config_megatron)
            actor_optimizer_scheduler = get_megatron_optimizer_param_scheduler(optimizer=actor_optimizer, config=optim_config)
        else:
            optim_config = None
            actor_optimizer = None
            actor_optimizer_scheduler = None

        return actor_module, actor_optimizer, actor_optimizer_scheduler, self.hf_config, optim_config
```

`ActorRolloutRefWorker._build_model_optimizer()` å‡½æ•°è´Ÿè´£æ„å»º Megatron æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼š

1.  **é…ç½®åˆå§‹åŒ–**ï¼šåˆå§‹åŒ– Hugging Face å’Œ Transformer é…ç½®ï¼Œå¹¶è·å– Generation Configã€‚
2.  **æ¨¡å‹æä¾›è€…å‡½æ•°**ï¼šå®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•° `megatron_actor_model_provider`ï¼Œç”¨äºåˆå§‹åŒ– Megatron æ¨¡å‹ã€‚
3.  **Actor å’Œ Rollout æ¨¡å‹æ„å»º**ï¼šå¦‚æœå½“å‰ Worker æ—¢æ˜¯ Actor åˆæ˜¯ Rollout è§’è‰²ï¼Œåˆ™è·å–æ¨¡å‹å¹¶åŠ è½½æƒé‡ã€‚
4.  **Reference Policy æ¨¡å‹æ„å»º**ï¼šå¦‚æœå½“å‰ Worker æ˜¯ Reference Policy è§’è‰²ï¼Œåˆ™è·å–æ¨¡å‹å¹¶åŠ è½½æƒé‡ã€‚
5.  **ä¼˜åŒ–å™¨åˆå§‹åŒ–**ï¼šå¦‚æœå½“å‰ Worker æ˜¯ Actor è§’è‰²ï¼Œåˆ™åˆå§‹åŒ– Megatron ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚

#### `ActorRolloutRefWorker._build_rollout()`

```python
from verl.utils.torch_dist import init_device_mesh, get_device_name

layer_name_mapping = {
    "decoder.layers": "model.decoder.layers",
    "norm.weight": "model.decoder.final_layernorm.weight",
    "norm.bias": "model.decoder.final_layernorm.bias",
    "embed_tokens.weight": "model.embed_tokens.weight",
    "lm_head.weight": "model.lm_head.weight",
}

@registry.register(ActorRolloutRefWorker)
class ActorRolloutRefWorker:
    # ... (previous methods)

    def _build_rollout(self, trust_remote_code=False):
        # è®¾å¤‡ç½‘æ ¼åˆ›å»º
        infer_tp = self.config.rollout.tensor_model_parallel_size
        dp = self.world_size // infer_tp
        assert self.world_size % infer_tp == 0
        rollout_device_mesh = init_device_mesh(
            get_device_name(),
            mesh_shape=(dp, infer_tp),
            mesh_dim_names=["dp", "infer_tp"]
        )

        # SGLang Rollout æ„å»º
        if self.config.rollout.name == "sglang":
            from verl.workers.rollout.sglang_rollout import SGLangRollout
            from verl.workers.sharding_manager.megatron_sglang import MegatronSGLangShardingManager
            from verl.models.mcore import get_mcore_weight_converter

            rollout = SGLangRollout(
                actor_module=self.config.model.path,
                config=self.config.rollout,
                tokenizer=self.tokenizer,
                model_hf_config=self.actor_model_config,
                trust_remote_code=trust_remote_code,
                device_mesh=rollout_device_mesh,
            )
            weight_converter = get_mcore_weight_converter(self.actor_model_config, self.dtype)
            sharding_manager = MegatronSGLangShardingManager(
                actor_module=self.actor.actor_module,
                inference_engine=rollout._engine,
                model_config=self.actor_model_config,
                transformer_config=self.tf_config,
                layer_name_mapping=layer_name_mapping,
                weight_converter=weight_converter,
                device_mesh=rollout_device_mesh,
            )

            return rollout, sharding_manager
        else:
            raise NotImplementedError
```

`ActorRolloutRefWorker._build_rollout()` å‡½æ•°ä¸“é—¨ç”¨äºæ„å»º Rollout æ¨¡å—ï¼Œç‰¹åˆ«æ˜¯ SGLang Rolloutï¼š

1.  **è®¾å¤‡ç½‘æ ¼åˆ›å»º**ï¼šä¸º Rollout åˆ›å»ºæ¨ç†å¼ é‡å¹¶è¡Œï¼ˆ`infer_tp`ï¼‰è®¾å¤‡ç½‘æ ¼ã€‚
2.  **SGLang Rollout æ„å»º**ï¼šå¦‚æœ Rollout åç§°æ˜¯ "sglang"ï¼Œåˆ™å¯¼å…¥å¹¶å®ä¾‹åŒ– `SGLangRollout` å’Œ `MegatronSGLangShardingManager`ã€‚`MegatronSGLangShardingManager` è´Ÿè´£åœ¨ FSDP è®­ç»ƒæ ¼å¼å’Œ SGLang æ¨ç†æ ¼å¼ä¹‹é—´è½¬æ¢æ¨¡å‹æƒé‡ã€‚

### `c` åˆå§‹åŒ–æµç¨‹

#### `SGLangRollout.__init__()`

```python
import os
from typing import Dict, List, Tuple, Optional
from omegaconf import DictConfig, OmegaConf
from sglang.function_calling.function_call_parser import FunctionCallParser
from sglang.utils.general import initialize_tools_from_config
from sglang.tools.tool import Tool
from transformers import AutoTokenizer
from verl.utils.fs import copy_to_local
from verl.utils.torch_dist import get_torch_device
from verl.workers.rollout import Rollout

def get_tool_call_parser_type(tokenizer):
    if isinstance(tokenizer, AutoTokenizer):
        if tokenizer.name_or_path in ["meta-llama/Llama-3-8B", "meta-llama/Llama-3-70B"]:
            return "llama3"
    return None

class SGLangRollout(Rollout):
    def __init__(self, actor_module, config, tokenizer, model_hf_config, port=None, trust_remote_code=False, device_mesh=None, **kwargs):
        super().__init__()
        self.config = config
        self._device_mesh_cpu = device_mesh
        os.environ.setdefault("SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK", "true")

        (
            self._tool_schemas,
            self._tool_map,
            self._tool_call_parser_type,
            self._sgl_tools,
            self._function_call_parser,
        ) = self._initialize_tools(config, tokenizer)

        self._init_distributed_env(device_mesh_cpu=device_mesh, **kwargs)
        self._verify_config(model_hf_config=model_hf_config)
        self._init_inference_engine(trust_remote_code, actor_module, port)
        self._init_sampling_params(**kwargs)

        self.tokenizer = tokenizer
        self.pad_token_id = tokenizer.pad_token_id
```

`SGLangRollout` çš„æ„é€ å‡½æ•°è´Ÿè´£ç®¡ç† SGLang æ¨ç†å¼•æ“ï¼š

1.  **åŸºç¡€åˆå§‹åŒ–**ï¼šè°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°å¹¶è®¾ç½®é…ç½®å’Œè®¾å¤‡ç½‘æ ¼ã€‚
2.  **å·¥å…·ç³»ç»Ÿåˆå§‹åŒ–**ï¼šå¦‚æœé…ç½®äº†å·¥å…·ï¼Œåˆ™é€šè¿‡ `_initialize_tools()` åˆå§‹åŒ–å·¥å…· schemasã€map å’Œè§£æå™¨ï¼Œæ”¯æŒ Multi-turn å¯¹è¯ä¸­çš„å·¥å…·ä½¿ç”¨ã€‚
3.  **åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–**ï¼šåˆå§‹åŒ– SGLang æ¨ç†æ‰€éœ€çš„åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆé€šè¿‡ `_init_distributed_env()`ï¼Œä»£ç æœªæä¾›ï¼‰ã€‚
4.  **é…ç½®éªŒè¯**ï¼šé€šè¿‡ `_verify_config()` éªŒè¯æ¨¡å‹é…ç½®ï¼ˆä»£ç æœªæä¾›ï¼‰ã€‚
5.  **æ¨ç†å¼•æ“åˆå§‹åŒ–**ï¼šé€šè¿‡ `_init_inference_engine()` åˆå§‹åŒ– SGLang æ¨ç†å¼•æ“ã€‚
6.  **é‡‡æ ·å‚æ•°åˆå§‹åŒ–**ï¼šé€šè¿‡ `_init_sampling_params()` åˆå§‹åŒ–ç”Ÿæˆåºåˆ—çš„é‡‡æ ·å‚æ•°ï¼ˆä»£ç æœªæä¾›ï¼‰ã€‚
7.  **Tokenizer è®¾ç½®**ï¼šè®¾ç½® Tokenizer å’Œ padding token IDã€‚

#### `SGLangRollout._initialize_tools()`

```python
from sglang.function_calling.function_call_parser import FunctionCallParser
from sglang.utils.general import initialize_tools_from_config
from sglang.tools.tool import Tool
from omegaconf import OmegaConf

@registry.register(SGLangRollout)
class SGLangRollout:
    # ... (previous methods)

    def _initialize_tools(self, config, tokenizer):
        if config.multi_turn.tool_config_path is None:
            return [], {}, None, [], None

        tools_config_file = config.multi_turn.tool_config_path
        tools_config = OmegaConf.load(tools_config_file)
        tool_list = initialize_tools_from_config(tools_config)

        tool_schemas = [tool.get_openai_tool_schema().model_dump() for tool in tool_list]
        tool_map = {tool.name: tool for tool in tool_list}

        tool_call_parser_type = get_tool_call_parser_type(tokenizer)

        sgl_tools = [Tool.model_validate(tool_schema) for tool_schema in tool_schemas]

        function_call_parser = FunctionCallParser(
            sgl_tools,
            tool_call_parser_type,
        )

        return tool_schemas, tool_map, tool_call_parser_type, sgl_tools, function_call_parser
```

`SGLangRollout._initialize_tools()` å‡½æ•°ç”¨äºåˆå§‹åŒ– Multi-turn å¯¹è¯ä¸­çš„å·¥å…·ï¼š

1.  **æ£€æŸ¥å·¥å…·é…ç½®**ï¼šå¦‚æœæ²¡æœ‰å·¥å…·é…ç½®è·¯å¾„ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨å’Œå­—å…¸ã€‚
2.  **åŠ è½½å·¥å…·é…ç½®**ï¼šä»é…ç½®æ–‡ä»¶åŠ è½½å·¥å…·å¹¶åˆå§‹åŒ–å·¥å…·åˆ—è¡¨ã€‚
3.  **å·¥å…· schema åˆ›å»º**ï¼šåˆ›å»º OpenAI æ ¼å¼çš„å·¥å…· schema å’Œå·¥å…·åç§°åˆ°å·¥å…·å¯¹è±¡çš„æ˜ å°„ã€‚
4.  **å·¥å…·è°ƒç”¨è§£æå™¨ç±»å‹ç¡®å®š**ï¼šæ ¹æ® Tokenizer ç±»å‹ç¡®å®šå·¥å…·è°ƒç”¨è§£æå™¨ã€‚
5.  **SGLang å·¥å…·åˆ›å»º**ï¼šä¸º SGLang åˆ›å»º `Tool` å¯¹è±¡ã€‚
6.  **å‡½æ•°è°ƒç”¨è§£æå™¨åˆ›å»º**ï¼šå®ä¾‹åŒ– `FunctionCallParser`ã€‚

#### `SGLangRollout._init_inference_engine()`

```python
from verl.utils.fs import copy_to_local
from verl.utils.torch_dist import get_device_name

@registry.register(SGLangRollout)
class SGLangRollout:
    # ... (previous methods)

    def _init_inference_engine(self, trust_remote_code, actor_module, port):
        local_path = copy_to_local(actor_module, use_shm=self.config.model.get("use_shm", False))

        if self._device_mesh_cpu is not None:
            self._rank = self._device_mesh_cpu["tp"].mesh[0].item()
            self._tp_rank = self._device_mesh_cpu["tp"].mesh[1].item()
            self._tp_size = self._device_mesh_cpu["tp"].mesh.shape[1]
        else:
            self._rank = 0
            self._tp_rank = 0
            self._tp_size = 1

        if self._tp_rank == 0:
            from sglang.srt.managers import EngineManager

            self._engine = EngineManager(
                model_path=local_path,
                trust_remote_code=trust_remote_code,
                **self.config.engine_kwargs
            )
        else:
            self._engine = None
```

`SGLangRollout._init_inference_engine()` å‡½æ•°è´Ÿè´£åˆå§‹åŒ– SGLang æ¨ç†å¼•æ“ï¼š

1.  **æœ¬åœ°è·¯å¾„å¤„ç†**ï¼šç¡®ä¿æ¨¡å‹è·¯å¾„æ˜¯æœ¬åœ°å¯è®¿é—®çš„ã€‚
2.  **åˆ†å¸ƒå¼ç¯å¢ƒè®¾ç½®**ï¼šæ ¹æ®è®¾å¤‡ç½‘æ ¼è®¾ç½® rank å’Œ tensor parallel sizeã€‚
3.  **SGLang å¼•æ“åˆ›å»º**ï¼šä»…åœ¨ tensor parallel rank ä¸º 0 çš„è¿›ç¨‹ä¸Šåˆ›å»º `EngineManager` å®ä¾‹ï¼Œç”¨äºé«˜æ•ˆåœ°æ‰§è¡Œæ¨¡å‹æ¨ç†ã€‚å…¶ä»–è¿›ç¨‹ä¸ç›´æ¥æŒæœ‰ `EngineManager` å®ä¾‹ã€‚

å¸Œæœ›è¿™æ¬¡æ•´ç†èƒ½å¤Ÿå¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£ task runner çš„ä»£ç é€»è¾‘ã€‚æˆ‘ä¿ç•™äº†æ‚¨æ–‡æ¡£ä¸­è§£æçš„æ¯ä¸€éƒ¨åˆ†å†…å®¹ï¼Œå¹¶å°è¯•æŒ‰ç…§ä»ä¸»å…¥å£åˆ°æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–çš„é¡ºåºè¿›è¡Œç»„ç»‡ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•å…¶ä»–é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„è°ƒæ•´ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚

-------


[å…·ä½“ä»£ç ](https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64)å¦‚ä¸‹ï¼š

<details>
<summary>TaskRunner</summary>

```python
@ray.remote(num_cpus=1)
class TaskRunner:
    def run(self, config):

        from pprint import pprint
        from omegaconf import OmegaConf
        from verl.utils.fs import copy_to_local
        import socket
        import os

        print(f"TaskRunner hostname: {socket.gethostname()}, PID: {os.getpid()}")
        pprint(OmegaConf.to_container(config, resolve=True))
        OmegaConf.resolve(config)

        # æ¨¡å‹ä¸‹è½½
        local_path = copy_to_local(config.actor_rollout_ref.model.path, use_shm=config.actor_rollout_ref.model.get("use_shm", False))

        # Tokenizer å’Œ Processor åˆå§‹åŒ–
        from verl.utils import hf_processor, hf_tokenizer
        trust_remote_code = config.data.get("trust_remote_code", False)
        tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)
        processor = hf_processor(local_path, trust_remote_code=trust_remote_code, use_fast=True)

        # Worker ç±»å‹é€‰æ‹©
        if config.actor_rollout_ref.actor.strategy in ["fsdp", "fsdp2"]:
            from verl.single_controller.ray import RayWorkerGroup
            from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker
            actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == "async" else ActorRolloutRefWorker
            ray_worker_group_cls = RayWorkerGroup
        elif config.actor_rollout_ref.actor.strategy == "megatron":
            assert config.actor_rollout_ref.actor.strategy == config.critic.strategy
            from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup
            from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker
            actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == "async" else ActorRolloutRefWorker
            ray_worker_group_cls = NVMegatronRayWorkerGroup
        else:
            raise NotImplementedError

        from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role

        # è§’è‰²åˆ° Worker ç±»çš„æ˜ å°„
        role_worker_mapping = {
            Role.ActorRollout: ray.remote(actor_rollout_cls),
            Role.Critic: ray.remote(CriticWorker),
        }

        # èµ„æºæ± è§„æ ¼å’Œè§’è‰²æ˜ å°„
        global_pool_id = "global_pool"
        resource_pool_spec = {
            global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,
        }
        mapping = {
            Role.ActorRollout: global_pool_id,
            Role.Critic: global_pool_id,
        }

        # Reward Model Worker çš„åˆå§‹åŒ–
        if config.reward_model.enable:
            if config.reward_model.strategy in ["fsdp", "fsdp2"]:
                from verl.workers.fsdp_workers import RewardModelWorker
            elif config.reward_model.strategy == "megatron":
                from verl.workers.megatron_workers import RewardModelWorker
            else:
                raise NotImplementedError
            role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)
            mapping[Role.RewardModel] = global_pool_id

        # Reference Policy Worker çš„åˆå§‹åŒ–
        if config.algorithm.use_kl_in_reward or config.actor_rollout_ref.actor.use_kl_loss:
            role_worker_mapping[Role.RefPolicy] = ray.remote(ActorRolloutRefWorker)
            mapping[Role.RefPolicy] = global_pool_id

        # åŠ è½½å¥–åŠ±ç®¡ç†å™¨
        reward_fn = load_reward_manager(config, tokenizer, num_examine=0, **config.reward_model.get("reward_kwargs", {}))
        val_reward_fn = load_reward_manager(config, tokenizer, num_examine=1, **config.reward_model.get("reward_kwargs", {}))
        resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)

        from verl.utils.dataset.rl_dataset import collate_fn, create_rl_dataset, create_rl_sampler

        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        train_dataset = create_rl_dataset(config.data.train_files, config.data, tokenizer, processor)
        val_dataset = create_rl_dataset(config.data.val_files, config.data, tokenizer, processor)
        train_sampler = create_rl_sampler(config.data, train_dataset)

        # åˆå§‹åŒ– PPO è®­ç»ƒå™¨
        trainer = RayPPOTrainer(
            config=config,
            tokenizer=tokenizer,
            processor=processor,
            role_worker_mapping=role_worker_mapping,
            resource_pool_manager=resource_pool_manager,
            ray_worker_group_cls=ray_worker_group_cls,
            reward_fn=reward_fn,
            val_reward_fn=val_reward_fn,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            collate_fn=collate_fn,
            train_sampler=train_sampler,
            device_name=config.trainer.device,
        )
        # åˆå§‹åŒ–è®­ç»ƒå™¨çš„ Workers
        trainer.init_workers()
        # å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
        trainer.fit()
```

</details>

`TaskRunner.run()` å‡½æ•°æ˜¯æ•´ä¸ªåˆ†å¸ƒå¼è®­ç»ƒçš„æ ¸å¿ƒåè°ƒå™¨ï¼Œå…¶ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1.  **é…ç½®ç®¡ç†**ï¼šåŠ è½½ã€è§£æå’ŒéªŒè¯è®­ç»ƒä»»åŠ¡çš„é…ç½®ï¼ˆä½¿ç”¨ `OmegaConf`ï¼‰ï¼Œç¡®ä¿æ‰€æœ‰å‚æ•°çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚
2.  **æ¨¡å‹ä¸‹è½½**ï¼šå°†æ¨¡å‹æ–‡ä»¶ä»è¿œç¨‹è·¯å¾„å¤åˆ¶åˆ°æœ¬åœ°ï¼Œç¡®ä¿æ‰€æœ‰ Worker éƒ½å¯ä»¥è®¿é—®ã€‚
3.  **ç»„ä»¶åˆå§‹åŒ–**ï¼š
      * åˆå§‹åŒ– Tokenizer å’Œ Processorï¼Œç”¨äºæ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®çš„å¤„ç†ã€‚
      * æ ¹æ®é…ç½®ä¸­æŒ‡å®šçš„ Actor ç­–ç•¥ï¼ˆå¦‚ `fsdp` æˆ– `megatron`ï¼‰ï¼ŒåŠ¨æ€é€‰æ‹©ç›¸åº”çš„ Worker ç±»ï¼ˆä¾‹å¦‚ `ActorRolloutRefWorker` å’Œ `CriticWorker`ï¼‰ï¼Œå¹¶ç¡®å®šä½¿ç”¨çš„ `RayWorkerGroup` ç±»å‹ã€‚
      * é…ç½®ä¸åŒè§’è‰²ï¼ˆå¦‚ `ActorRollout` å’Œ `Critic`ï¼‰åˆ° Ray è¿œç¨‹ Worker ç±»çš„æ˜ å°„ã€‚
      * å®šä¹‰ Ray èµ„æºæ± çš„è§„æ ¼å’Œè§’è‰²åˆ°èµ„æºæ± çš„æ˜ å°„ï¼Œç”¨äº GPU èµ„æºçš„åˆ†é…å’Œç®¡ç†ã€‚
      * åŠ è½½ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„å¥–åŠ±æ¨¡å‹ã€‚
      * åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œä»¥åŠè®­ç»ƒæ•°æ®é‡‡æ ·å™¨ã€‚
4.  **`RayPPOTrainer` åˆå§‹åŒ–**ï¼šåˆ›å»º `RayPPOTrainer` å®ä¾‹ï¼Œå®ƒæ˜¯ç®¡ç†æ‰€æœ‰è®¡ç®—èµ„æºå’Œè®­ç»ƒæµç¨‹çš„ä¸­å¤®åè°ƒå™¨ã€‚
5.  **Worker åˆå§‹åŒ–**ï¼šè°ƒç”¨ `RayPPOTrainer` çš„ `init_workers()` æ–¹æ³•ï¼Œå°†é…ç½®çš„ Worker ç±»å®ä¾‹åŒ–åˆ° Ray é›†ç¾¤çš„ GPU ä¸Šï¼Œä¸ºå®é™…è®¡ç®—åšå‡†å¤‡ã€‚
6.  **å¯åŠ¨è®­ç»ƒ**ï¼šè°ƒç”¨ `RayPPOTrainer` çš„ `fit()` æ–¹æ³•ï¼Œå¯åŠ¨ PPO è®­ç»ƒå¾ªç¯ã€‚

### `RayPPOTrainer` åˆå§‹åŒ–æµç¨‹

`RayPPOTrainer` æ˜¯è®­ç»ƒæµç¨‹çš„ä¸­å¤®åè°ƒå™¨ï¼Œè´Ÿè´£è°ƒåº¦å’Œåè°ƒå„ä¸ª Worker çš„å·¥ä½œã€‚

#### `RayPPOTrainer.__init__()`

```python
def __init__(self, config, tokenizer, processor, role_worker_mapping, resource_pool_manager, ray_worker_group_cls, reward_fn, val_reward_fn, train_dataset, val_dataset, collate_fn, train_sampler, device_name):
    # åŸºç¡€é…ç½®è®¾ç½®
    self.config = config
    self.tokenizer = tokenizer
    self.processor = processor
    self.role_worker_mapping = role_worker_mapping
    self.resource_pool_manager = resource_pool_manager
    self.ray_worker_group_cls = ray_worker_group_cls
    self.reward_fn = reward_fn
    self.val_reward_fn = val_reward_fn
    self.device_name = device_name

    # åŠŸèƒ½æ ‡å¿—è®¾ç½®
    self.use_critic = config.critic.enable
    self.use_reference_policy = config.algorithm.use_kl_in_reward or config.actor_rollout_ref.actor.use_kl_loss
    self.use_rm = config.reward_model.enable
    self.hybrid_engine = config.actor_rollout_ref.hybrid_engine

    # é…ç½®éªŒè¯
    self._validate_config()

    # æ•°æ®åŠ è½½å™¨åˆ›å»º
    self.train_dataset = train_dataset
    self.val_dataset = val_dataset
    self.collate_fn = collate_fn
    self.train_sampler = train_sampler
```

`RayPPOTrainer` çš„æ„é€ å‡½æ•°ä¸»è¦è´Ÿè´£ï¼š

1.  **åŸºç¡€é…ç½®è®¾ç½®**ï¼šä¿å­˜ä¼ å…¥çš„é…ç½®å¯¹è±¡ã€tokenizerã€processorã€è§’è‰²åˆ° Worker çš„æ˜ å°„ã€èµ„æºæ± ç®¡ç†å™¨ä»¥åŠ WorkerGroup ç±»ã€‚
2.  **åŠŸèƒ½æ ‡å¿—è®¾ç½®**ï¼šæ ¹æ®é…ç½®å¯ç”¨æˆ–ç¦ç”¨ Criticã€Reference Policyã€Reward Model å’Œ Hybrid Engine ç­‰åŠŸèƒ½ç»„ä»¶ã€‚
3.  **é…ç½®éªŒè¯**ï¼šè°ƒç”¨ `_validate_config()` æ–¹æ³•éªŒè¯é…ç½®çš„åˆç†æ€§ã€‚
4.  **æ•°æ®åŠ è½½å™¨å­˜å‚¨**ï¼šå­˜å‚¨è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€collate å‡½æ•°å’Œè®­ç»ƒæ•°æ®é‡‡æ ·å™¨ã€‚

#### `_validate_config()`

```python
def _validate_config(self):
    # GPU æ•°é‡éªŒè¯
    n_gpus = self.config.trainer.n_gpus_per_node * self.config.trainer.nnodes

    # æ‰¹æ¬¡å¤§å°éªŒè¯
    if self.config.actor_rollout_ref.actor.strategy == "megatron":
        model_parallel_size = self.config.actor_rollout_ref.actor.megatron.tensor_model_parallel_size * self.config.actor_rollout_ref.actor.megatron.pipeline_model_parallel_size
        assert n_gpus % (model_parallel_size * self.config.actor_rollout_ref.actor.megatron.context_parallel_size) == 0
        megatron_dp = n_gpus // (model_parallel_size * self.config.actor_rollout_ref.actor.megatron.context_parallel_size)
        minimal_bsz = megatron_dp * self.config.actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu
    else:
        minimal_bsz = n_gpus

    real_train_batch_size = self.config.data.train_batch_size * self.config.actor_rollout_ref.rollout.n
    assert real_train_batch_size % minimal_bsz == 0, f"real_train_batch_size ({real_train_batch_size}) must be divisible by minimal possible batch size ({minimal_bsz})"

    # Multi-turn é…ç½®éªŒè¯
    if self.config.actor_rollout_ref.rollout.multi_turn.enable:
        assert self.config.actor_rollout_ref.rollout.multi_turn.tool_config_path is not None
        from verl.trainer.ppo.config import AdvantageEstimator
        assert self.config.algorithm.adv_estimator in [AdvantageEstimator.GRPO]
```

`_validate_config()` å‡½æ•°æ‰§è¡Œé…ç½®çš„å®Œæ•´æ€§æ£€æŸ¥ï¼ŒåŒ…æ‹¬ï¼š

1.  **GPU æ•°é‡éªŒè¯**ï¼šè®¡ç®—å¹¶æ£€æŸ¥å¯ç”¨ GPU æ€»æ•°ã€‚
2.  **æ‰¹æ¬¡å¤§å°éªŒè¯**ï¼šæ ¹æ® Megatron æˆ–å…¶ä»–ç­–ç•¥è®¡ç®—æœ€å°æ‰¹æ¬¡å¤§å°ï¼Œå¹¶ç¡®ä¿å®é™…è®­ç»ƒæ‰¹æ¬¡å¤§å°æ˜¯æœ€å°æ‰¹æ¬¡å¤§å°çš„æ•´æ•°å€ã€‚
3.  **Multi-turn é…ç½®éªŒè¯**ï¼šå¦‚æœå¯ç”¨äº† Multi-turn å¯¹è¯è®­ç»ƒï¼Œåˆ™æ£€æŸ¥å·¥å…·é…ç½®è·¯å¾„å’Œä¼˜åŠ¿ä¼°è®¡å™¨ç±»å‹æ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚

### Worker åˆå§‹åŒ–æµç¨‹ (`init_workers()`)

`init_workers()` å‡½æ•°è´Ÿè´£åœ¨ Ray é›†ç¾¤ä¸Šå®ä¾‹åŒ–å’Œåˆå§‹åŒ– ActorRolloutã€Criticã€Reference Policy å’Œ Reward Model Workersã€‚

```python
def init_workers(self):
    # åˆ›å»ºèµ„æºæ± 
    self.resource_pool_manager.create_resource_pool()

    # åˆå§‹åŒ–èµ„æºæ± åˆ°ç±»çš„æ˜ å°„
    self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()}

    # åˆ›å»º ActorRollout Worker
    if self.hybrid_engine:
        resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)
        actor_rollout_cls = RayClassWithInitArgs(
            cls=self.role_worker_mapping[Role.ActorRollout],
            config=self.config.actor_rollout_ref,
            role="actor_rollout",
        )
        self.resource_pool_to_cls[resource_pool]["actor_rollout"] = actor_rollout_cls

    # åˆ›å»º Critic Worker
    if self.use_critic:
        resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic)
        critic_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.Critic], config=self.config.critic)
        self.resource_pool_to_cls[resource_pool]["critic"] = critic_cls

    # åˆ›å»º Reference Policy Worker
    if self.use_reference_policy:
        resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy)
        ref_policy_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RefPolicy], config=self.config.actor_rollout_ref, role="ref")
        self.resource_pool_to_cls[resource_pool]["ref"] = ref_policy_cls

    # åˆ›å»º Reward Model Worker
    if self.use_rm:
        resource_pool = self.resource_pool_manager.get_resource_pool(Role.RewardModel)
        rm_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RewardModel], config=self.config.reward_model)
        self.resource_pool_to_cls[resource_pool]["rm"] = rm_cls

    # åˆå§‹åŒ– WorkerGroup
    all_wg = {}
    for resource_pool, class_dict in self.resource_pool_to_cls.items():
        # åˆ›å»ºå…±ç½® Worker ç±»
        from verl.utils.ray import create_colocated_worker_cls, RayClassWithInitArgs
        worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)

        # åˆ›å»º WorkerGroup
        wg_kwargs = {}
        if self.config.actor_rollout_ref.actor.strategy == "megatron":
            wg_kwargs["meta_parallel_size"] = self.config.actor_rollout_ref.actor.megatron.tensor_model_parallel_size
            wg_kwargs["pipeline_parallel_size"] = self.config.actor_rollout_ref.actor.megatron.pipeline_model_parallel_size

        wg_dict = self.ray_worker_group_cls(
            resource_pool=resource_pool,
            ray_cls_with_init=worker_dict_cls,
            device_name=self.device_name,
            **wg_kwargs
        )

        # ç”Ÿæˆ Worker
        spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())
        all_wg.update(spawn_wg)

    # åˆå§‹åŒ–å„ä¸ª Worker
    if self.use_critic:
        self.critic_wg = all_wg["critic"]
        self.critic_wg.init_model()

    if self.use_reference_policy and not getattr(self, "ref_in_actor", False):
        self.ref_policy_wg = all_wg["ref"]
        self.ref_policy_wg.init_model()

    if self.use_rm:
        self.rm_wg = all_wg["rm"]
        self.rm_wg.init_model()

    self.actor_rollout_wg = all_wg["actor_rollout"]
    self.actor_rollout_wg.init_model()
```

`init_workers()` çš„ä¸»è¦æ­¥éª¤åŒ…æ‹¬ï¼š

1.  **åˆ›å»ºèµ„æºæ± **ï¼šé€šè¿‡ `ResourcePoolManager` åˆ›å»º Ray èµ„æºæ± ã€‚
2.  **åˆå§‹åŒ–èµ„æºæ± åˆ°ç±»çš„æ˜ å°„**ï¼šä¸ºæ¯ä¸ªèµ„æºæ± åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨ä¸åŒè§’è‰² Worker çš„ `RayClassWithInitArgs` åŒ…è£…å™¨ã€‚`RayClassWithInitArgs` ç”¨äºå»¶è¿Ÿåˆå§‹åŒ– Workerï¼Œå­˜å‚¨äº† Worker çš„ç±»å’Œåˆå§‹åŒ–å‚æ•°ã€‚
3.  **åˆ›å»ºä¸åŒè§’è‰²çš„ Worker çš„ `RayClassWithInitArgs` å®ä¾‹**ï¼šæ ¹æ®é…ç½®å¯ç”¨æƒ…å†µï¼Œä¸º ActorRolloutã€Criticã€Reference Policy å’Œ Reward Model åˆ›å»ºå¯¹åº”çš„ `RayClassWithInitArgs` å®ä¾‹ã€‚
4.  **åˆå§‹åŒ– WorkerGroup**ï¼šéå†æ‰€æœ‰èµ„æºæ± ï¼Œå°†åŒä¸€èµ„æºæ± ä¸­çš„å¤šä¸ª Worker ç±»é€šè¿‡ `create_colocated_worker_cls` ç»„åˆæˆä¸€ä¸ªå…±ç½®ç±»ï¼Œç„¶åå®ä¾‹åŒ– `RayWorkerGroup`ã€‚`RayWorkerGroup` è´Ÿè´£åœ¨å¤šä¸ª GPU ä¸Šå¯åŠ¨å¤šä¸ª Worker å®ä¾‹ã€‚æœ€åè°ƒç”¨ `spawn()` æ–¹æ³•åœ¨ Ray ä¸­å®é™…åˆ›å»º Worker å®ä¾‹ã€‚
5.  **åˆå§‹åŒ–å„ä¸ª Worker**ï¼šæ ¹æ®è§’è‰²ä»åˆ›å»ºçš„ WorkerGroup å­—å…¸ä¸­è·å–å¯¹åº”çš„ WorkerGroupï¼Œå¹¶è°ƒç”¨å…¶ `init_model()` æ–¹æ³•ï¼ŒæŒ‰ç…§ä¾èµ–å…³ç³»ä¾æ¬¡åˆå§‹åŒ–ä¸åŒçš„ Worker æ¨¡å—ã€‚ActorRollout Worker é€šå¸¸æœ€ååˆå§‹åŒ–ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚

### `ResourcePoolManager.create_resource_pool()`

```python
class ResourcePoolManager:
    def __init__(self, resource_pool_spec, mapping):
        self.resource_pool_spec = resource_pool_spec
        self.mapping = mapping
        self.resource_pool_dict = {}

    def create_resource_pool(self):
        # ä¸ºæ¯ä¸ªèµ„æºæ± åˆ›å»º RayResourcePool
        from verl.utils.ray import RayResourcePool
        for resource_pool_name, process_on_nodes in self.resource_pool_spec.items():
            resource_pool = RayResourcePool(
                process_on_nodes=process_on_nodes,
                use_gpu=True,
                max_colocate_count=1,
                name_prefix=resource_pool_name
            )
            self.resource_pool_dict[resource_pool_name] = resource_pool
        # æ£€æŸ¥èµ„æºå¯ç”¨æ€§
        self._check_resource_available()

    def get_resource_pool(self, role):
        pool_id = self.mapping.get(role)
        return self.resource_pool_dict.get(pool_id)

    def _check_resource_available(self):
        from ray.cluster_utils import get_ray_nodes
        ray_nodes = get_ray_nodes()
        total_gpus = sum([node['Resources'].get('GPU', 0) for node in ray_nodes])
        required_gpus = sum([len(nodes) for nodes in self.resource_pool_spec.values()])
        assert total_gpus >= required_gpus, f"Total available GPUs: {total_gpus}, required GPUs: {required_gpus}"
```

`ResourcePoolManager.create_resource_pool()` å‡½æ•°è´Ÿè´£ä¸º Ray é›†ç¾¤åˆ›å»ºå’Œç®¡ç†èµ„æºæ± ï¼Œæ­¥éª¤åŒ…æ‹¬ï¼š

1.  **ä¸ºæ¯ä¸ªèµ„æºæ± åˆ›å»º `RayResourcePool`**ï¼š`RayResourcePool` å°è£…äº† Ray æ”¾ç½®ç»„ï¼ˆPlacement Groupï¼‰çš„é€»è¾‘ï¼Œç”¨äºæ§åˆ¶ Worker çš„ç‰©ç†ä½ç½®å’Œèµ„æºåˆ†é…ã€‚
2.  **æ£€æŸ¥èµ„æºå¯ç”¨æ€§**ï¼šéªŒè¯ Ray é›†ç¾¤æ˜¯å¦æœ‰è¶³å¤Ÿçš„ GPU èµ„æºæ¥æ»¡è¶³å®šä¹‰çš„èµ„æºæ± éœ€æ±‚ã€‚

### `RayWorkerGroup.spawn()`

```python
import ray
import time

from ray.util.placement_group import placement_group_table, get_placement_group
from ray.worker import list_named_actors

class RayWorkerGroup:
    def __init__(self, resource_pool, ray_cls_with_init, device_name, **kwargs):
        self.resource_pool = resource_pool
        self.ray_cls_with_init = ray_cls_with_init
        self.device_name = device_name
        self._workers = []
        self._worker_names = []
        self.name_prefix = ray_cls_with_init.cls.__name__.lower()
        self._ray_wait_register_center_timeout = 10

    def spawn(self, prefix_set=None):
        strategy = "PACK"
        use_gpu = True
        num_gpus = 1
        local_world_size = self.resource_pool.max_colocate_count
        from verl.utils.ray import get_placement_group_by_id
        pgs = self.resource_pool.get_placement_groups(strategy=strategy, device_name=self.device_name)
        world_size = self.resource_pool.world_size

        def func_generator(worker_index, worker_name):
            return {"rank": worker_index, "local_rank": worker_index % local_world_size}

        for rank in range(world_size):
            pg = pgs[rank // local_world_size]
            local_rank = rank % local_world_size
            name = f"{self.name_prefix}_{rank}"
            worker = self.ray_cls_with_init(
                placement_group=pg,
                placement_group_bundle_idx=local_rank,
                use_gpu=use_gpu,
                num_gpus=num_gpus,
                device_name=self.device_name,
                name=name
            )
            self._workers.append(worker)
            self._worker_names.append(name)

            if rank == 0:
                register_center_actor = None
                actor_name = f"{self.name_prefix}_register_center"
                start_time = time.time()

                while time.time() - start_time < self._ray_wait_register_center_timeout:
                    if actor_name in list_named_actors():
                        register_center_actor = ray.get_actor(actor_name)
                        break
                    time.sleep(1)

        self._bind_worker_method(self.ray_cls_with_init.cls, func_generator)

        return {prefix: self for prefix in prefix_set}

    def _bind_worker_method(self, cls, func_generator):
        for method_name in dir(cls):
            if method_name.startswith("_") or method_name in ["__init__", "init_model"]:
                continue
            method = getattr(cls, method_name)
            if callable(method):
                def create_remote_fn(method_name):
                    def remote_fn(self, *args, **kwargs):
                        futures = [
                            getattr(worker, method_name).remote(*args, **kwargs, **func_generator(i, self._worker_names[i]))
                            for i, worker in enumerate(self._workers)
                        ]
                        return futures
                    return remote_fn
                setattr(self, method_name, create_remote_fn(method_name))

    def get_workers(self):
        return self._workers
```

`RayWorkerGroup.spawn()` æ–¹æ³•åœ¨ Ray ä¸­å®é™…åˆ›å»º Worker å®ä¾‹ï¼Œå¹¶å°†å…¶ç»„ç»‡æˆ WorkerGroupï¼š

1.  **è·å–æ”¾ç½®ç»„**ï¼šä»èµ„æºæ± è·å– Ray æ”¾ç½®ç»„ï¼Œç”¨äºæ§åˆ¶ Worker çš„ç‰©ç†ä½ç½®ï¼Œç¡®ä¿ Worker å®ä¾‹èƒ½å¤Ÿè¢«æœ‰æ•ˆåœ°æ”¾ç½®åœ¨ Ray é›†ç¾¤çš„èŠ‚ç‚¹ä¸Šï¼Œå¹¶æ»¡è¶³èµ„æºçº¦æŸã€‚
2.  **åˆ›å»º Worker å®ä¾‹**ï¼šéå†æ¯ä¸ª rankï¼ˆworker çš„ç¼–å·ï¼‰ï¼Œä½¿ç”¨ `RayClassWithInitArgs` åŒ…è£…å™¨åˆ›å»ºè¿œç¨‹ Workerï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°å†…éƒ¨åˆ—è¡¨ä¸­ã€‚åœ¨ rank 0 ä¸Šä¼šç­‰å¾…æ³¨å†Œä¸­å¿ƒ Actor å°±ç»ªã€‚
3.  **ç»‘å®š Worker æ–¹æ³•**ï¼šå°† Ray è¿œç¨‹æ–¹æ³•ç»‘å®šåˆ° WorkerGroup å¯¹è±¡ï¼Œè¿™æ ·å¯ä»¥åœ¨ WorkerGroup çº§åˆ«æ–¹ä¾¿åœ°è°ƒç”¨è¿œç¨‹ Worker çš„æ–¹æ³•ã€‚

ã€TODOã€‘
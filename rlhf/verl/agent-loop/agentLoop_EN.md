# AgentLoop Code Walkthrough

AgentLoop is a module in Verl for managing multi-turn rollouts in RLHF training. It abstracts the complex logic previously embedded in `SGLangRollout`, offering a cleaner and more extensible way to handle LLM interactions, tool calls, and state management. This walkthrough explores how AgentLoop replaces legacy components like `ActorRolloutRefWorker` with a more composable structure, making it easier to customize and scale multi-turn agent behaviors.

After reading the entire document, I find the design of AgentLoop quite solid. The main issue lies with the historical burden of `ActorRolloutRefWorker`. If we treat the entire `ActorRolloutRefWorker` as an `sgl.Engine`, then within AgentLoop, there are two wrapping layers: `AsyncSGLangServer` and `AsyncLLMServerManager`.

`AsyncSGLangServer` wraps `sgl.Engine` into a FastAPI-based server, while `AsyncLLMServerManager` wraps the server with a router that performs load balancing—essentially acting as the `sglang` router. These two layers are well-designed; the real trouble comes from the overly complex wrapping of `ActorRolloutRefWorker`.

As for `AgentLoopManager`, `AgentLoopWorker`, and `AgentLoop`, I believe `AgentLoopWorker` may not be strictly necessary, whereas the other two components are quite reasonable.

## Author

Changyi Yang(CMU),  Chenyang Zhao(Amazon), Huapeng Zhou(UW)

## Related Resources

**Script**
 https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/tool_examples/agent_loop.md

**Related PRs**
 https://github.com/volcengine/verl/pull/2124
 https://github.com/volcengine/verl/pull/2563
 https://github.com/volcengine/verl/pull/A2598

**Commit We Are Looking At**
 https://github.com/volcengine/verl/tree/c5b189a1af496d0bc68320cd1d5bd7a1f1e3638a

## Call Overview

main_ppo.py -> RayPPOTrainer(fit)-> AgentLoopManager(async) -> AgentLoopWorker -> AsyncLLMServerManager -> AsyncSGLangServer -> AsyncActorRolloutRefWorker -> SGLangRollout -> AsyncEngine -> sgl.Engine

- `TaskRunner` starts training by invoking `RayPPOTrainer.fit()`.
- `RayPPOTrainer` manages the overall training loop and calls `AgentLoopManager.generate_sequences()` to initiate downward calls, while also initializing `AsyncActorRolloutRefWorker`.
- `AgentLoopManager` initializes `dp` number of `AsyncSGLangServer` instances, and then creates `num_rollout_workers` instances of `AgentLoopWorker`.
- Each `AgentLoopWorker` uses its assigned `agent_name` from the pre-registered `_agent_loop_registry` to initialize `train_batch_size / num_rollout_workers` `AgentLoop` instances. For GRPO, `train_batch_size` needs to be multiplied by the group size. Users can register their own `AgentLoop` classes if needed. Currently, `ToolAgentLoop` fully replaces the `_req_level_generate_sequences`-based tool call management previously handled inside `SGLangRollout`. In other words, tool state management in multi-turn RL used to reside inside `SGLangRollout`, but `AgentLoop` abstracts this layer out, and `SGLangRollout` simply becomes a token-in-token-out wrapper exposed through `AsyncSGLangServer`.
- Once initialized, `AgentLoop` manages all tool-call states and, based on the output of the policy, invokes the downstream modules: `AsyncLLMServerManager` → `AsyncSGLangServer` → `AsyncActorRolloutRefWorker` → `SGLangRollout` → `AsyncEngine` → `sgl.Engine`, to obtain the model output. After returning the result, the lifecycle of `AgentLoop` ends.
- `AgentLoopWorker` collects results from all `AgentLoop` instances and passes them to `AgentLoopManager`, awaiting the next round of requests.
- `AgentLoopManager` gathers all responses from its `AgentLoopWorker` instances and returns them.

![image-20250731154859113](./assets/image-20250731154859113.png)

## AgentLoopManager

AgentLoop’s top-level controller, responsible for managing the lifecycle of `AgentLoopWorker` instances and LLM servers. Its core method is `generate_sequences`, which performs downward calls to obtain the trajectories generated by the policy model under the given agent loop environment.

### Core APIs

Initialized inside `RayPPOTrainer`:

```python
if self.config.actor_rollout_ref.rollout.mode == "async":
    from verl.experimental.agent_loop import AgentLoopManager

    self.async_rollout_mode = True
    self.async_rollout_manager = AgentLoopManager(
        config=self.config,
        worker_group=self.actor_rollout_wg,
    )
```

The actual initialization is very concise:

**`__init__`**

```python
def __init__(self, config: DictConfig, worker_group: RayWorkerGroup):
    """Initialize agent loop manager.

    Args:
        config (DictConfig): trainer config.
        worker_group (RayWorkerGroup): ActorRolloutRef worker group.
    """
    self.config = config
    self.worker_group = worker_group

    self._initialize_llm_servers()
    self._init_agent_loop_workers()

    # Initially we're in sleep mode.
    self.sleep()
```

- Receives the worker group corresponding to `ActorRolloutRefWorker`, which is used in `_initialize_llm_servers` to locate the corresponding `RolloutWorker`;
- Initializes the LLM servers and the agent loop workers;

**`_initialize_llm_servers`**

- Calculates `dp` size: `self.rollout_dp_size = self.worker_group.world_size // self.rollout_tp_size`
- Obtains the server class (e.g., `AsyncSGLangServer`) via `async_server_class(rollout_backend=...)`, which serves as a communication proxy with the underlying `sgl.Engine`
- Initializes `dp` number of servers using Ray, creating one server instance per `dp` rank
- Retrieves and stores the address of each server using `ray.get(server.get_server_address.remote())`
- Initializes all servers by calling `ray.get([server.init_engine.remote() for server in self.async_llm_servers])`; each server looks up and acquires its corresponding SGLang engine workers from the Ray actor registry via name prefix matching

```python
def _initialize_llm_servers(self):
    # Calculate dp size
    self.rollout_tp_size = self.config.actor_rollout_ref.rollout.tensor_model_parallel_size
    self.rollout_dp_size = self.worker_group.world_size // self.rollout_tp_size

    # Get worker info for node affinity scheduling
    register_center = ray.get_actor(f"{self.worker_group.name_prefix}_register_center")
    workers_info = ray.get(register_center.get_worker_info.remote())
    assert len(workers_info) == self.worker_group.world_size

    self.async_llm_servers = [None] * self.rollout_dp_size
    self.server_addresses = [None] * self.rollout_dp_size

    # Get the server class, e.g., AsyncSGLangServer
    if self.config.actor_rollout_ref.rollout.agent.custom_async_server:
        server_class = async_server_class(
            rollout_backend=self.config.actor_rollout_ref.rollout.name,
            rollout_backend_module=self.config.actor_rollout_ref.rollout.agent.custom_async_server.path,
            rollout_backend_class=self.config.actor_rollout_ref.rollout.agent.custom_async_server.name,
        )
    else:
        server_class = async_server_class(rollout_backend=self.config.actor_rollout_ref.rollout.name)

    # Initialize dp-rank number of AsyncServers using Ray
    unready_dp_ranks = set(range(self.rollout_dp_size))
    while len(unready_dp_ranks) > 0:
        servers = {
            rollout_dp_rank: server_class.options(
                # Ensure server is colocated with its corresponding worker
                scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(
                    node_id=workers_info[rollout_dp_rank * self.rollout_tp_size],
                    soft=False,
                ),
                name=f"async_llm_server_{rollout_dp_rank}",
            ).remote(self.config, self.rollout_dp_size, rollout_dp_rank, self.worker_group.name_prefix)
            for rollout_dp_rank in unready_dp_ranks
        }

        # Record server addresses
        for rollout_dp_rank, server in servers.items():
            try:
                address = ray.get(server.get_server_address.remote())
                self.server_addresses[rollout_dp_rank] = address
                self.async_llm_servers[rollout_dp_rank] = server
                unready_dp_ranks.remove(rollout_dp_rank)
            except Exception:
                ray.kill(server)
                print(f"rollout server {rollout_dp_rank} failed, maybe address already in use, restarting...")

        # Initialize servers: let each one acquire its corresponding workers from Ray
        ray.get([server.init_engine.remote() for server in self.async_llm_servers])
```

**`_init_agent_loop_workers`**

Initializes `rollout.agent.num_workers` number of `AgentLoopWorker` instances using Ray:

```python
def _init_agent_loop_workers(self):
    self.agent_loop_workers = []
    for i in range(self.config.actor_rollout_ref.rollout.agent.num_workers):
        self.agent_loop_workers.append(
            AgentLoopWorker.options(
                name=f"agent_loop_worker_{i}",
            ).remote(self.config, self.async_llm_servers)
        )
```

**`generate_sequences`**

- If `free_cache_engine` is configured, first call `self.wake_up()` to wake up all LLM servers
- Use `prompts.chunk(len(self.agent_loop_workers))` to split the input batch into chunks
- Distribute chunks to `AgentLoopWorker` instances and await their parallel results via Ray
- After processing, call `self.sleep()` to put servers into sleep mode to release GPU memory
- Collect performance metrics such as latency
- Concatenate the outputs from all workers and return the result

[Code link](https://github.com/volcengine/verl/blob/c5b189a1af496d0bc68320cd1d5bd7a1f1e3638a/verl/experimental/agent_loop/agent_loop.py#L486)

```python
def generate_sequences(self, prompts: DataProto) -> DataProto:
    if self.config.actor_rollout_ref.rollout.free_cache_engine:
        self.wake_up()  # Wake up all LLM servers

    chunkes = prompts.chunk(len(self.agent_loop_workers))  # Split prompts by number of workers

    outputs = ray.get(
        [worker.generate_sequences.remote(chunk) for worker, chunk in zip(self.agent_loop_workers, chunkes)]
    )  # Dispatch chunks in parallel to AgentLoopWorkers

    output = DataProto.concat(outputs)  # Aggregate output from all workers

    if self.config.actor_rollout_ref.rollout.free_cache_engine:
        self.sleep()  # Put servers into sleep mode to release GPU memory

    # Compute performance metrics
    metrics = [output.meta_info["metrics"] for output in outputs]
    timing = self._performance_metrics(metrics, output)

    output.meta_info = {"timing": timing}
    return output
```

## AsyncSGLangServer

An asynchronous server implementation based on SGLang, inherited from `AsyncServerBase`. It runs as a Ray remote actor and is responsible for forwarding requests to the underlying SGLang Engine. Due to the design of SGLang, calling `generate` only requires invoking the master worker (i.e., inference TP rank 0 in verl).

### Core APIs

**`init_engine`**

Asynchronously initializes the SGLang engine:

- Uses `ray.util.list_named_actors` to list all matching actors
- Parses actor names using the naming pattern `self.wg_prefix + "WorkerDict_"`
- Based on `dp_rank` and `tp_size`, assigns actors and determines the master worker (tp rank 0)

```python
async def init_engine(self):
    if self.workers:
        # avoid init twice
        return
    all_actors = ray.util.list_named_actors(all_namespaces=True)
    matched_actors = [
        actor for actor in all_actors if actor.get("name", None).startswith(self.wg_prefix + "WorkerDict_")
    ]

    gpu_per_node = len(set([actor["name"].split(":")[1] for actor in matched_actors]))
    # total gpu num
    assert len(matched_actors) == self._dp_size * self._tp_size

    for matched_actor in matched_actors:
        fields = matched_actor["name"].split(":")
        assert len(fields) == 2, f"invalid actor name: {matched_actor['name']}"
        pg_index, local_rank = int(fields[0].split("_")[-1]), int(fields[1])

        current_global_rank = gpu_per_node * pg_index + local_rank
        worker_dp_rank = current_global_rank // self._tp_size
        worker_tp_rank = current_global_rank % self._tp_size

        if worker_dp_rank == self._dp_rank:
            worker = ray.get_actor(**matched_actor)
            self.workers.append(worker)

            if worker_tp_rank == 0:
                self.master_worker = worker
```

**`chat_completion`**

Handles `chat_completion` requests:

```python
async def chat_completion(self, raw_request: Request):
    request = await raw_request.json()
    output_future = self.master_worker.chat_completion.remote(request)
    [outputs] = await asyncio.gather(output_future)
    return JSONResponse(outputs)
```

- Forwards the request to the master worker
- Returns the response in JSON format

**`generate`**

Token-in-token-out call to get inference results from the SGLang Engine:

```python
async def generate(self, prompt_ids: List[int], sampling_params: Dict[str, Any], request_id: str) -> List[int]:
    return await self.master_worker.generate.remote(prompt_ids, sampling_params, request_id)
```

- Directly calls the master worker’s `generate` method
- Supports custom sampling parameters

## AsyncLLMServerManager

Manages multiple OpenAI-compatible LLM servers (e.g., `AsyncSGLangServer`), providing load balancing and session affinity functionality. It supports a least-requests load balancing strategy and ensures that multi-turn conversations with the same session ID are routed to the same server to enable prefix caching. You can think of it as a simple router/load balancer layer.

### Initialization

- Stores the list of server handles and shuffles them randomly
- Initializes a least-requests heap:
   `self.weighted_serveres = [[0, (hash(server), server)] for server in server_handles]`
- Creates an LRU cache:
   `self.request_id_to_server = LRUCache(maxsize=max_cache_size)`
   to map `request_id` to specific servers

```python
def __init__(self, config: DictConfig, server_handles: List[ray.actor.ActorHandle], max_cache_size: int = 10000):
    """Initialize the AsyncLLMServerManager.

    Args:
        config (DictConfig): YAML config.
        server_handles (List[ray.actor.ActorHandle]): OpenAI compatible LLM server actor handles.
        max_cache_size (int, optional): max cache size for request_id to server mapping. Defaults to 10000.
    """
    self.config = config
    self.server_handles = server_handles
    random.shuffle(self.server_handles)

    # Least requests load balancing
    self.weighted_serveres = [[0, (hash(server), server)] for server in server_handles]
    heapq.heapify(self.weighted_serveres)

    # LRU cache to map request_id to server
    self.request_id_to_server = LRUCache(maxsize=max_cache_size)
```

### `_choose_server`

```python
def _choose_server(self, request_id: str) -> ray.actor.ActorHandle:
    if request_id in self.request_id_to_server:
        return self.request_id_to_server[request_id]  # Session affinity
    
    server = self.weighted_serveres[0][1][1]  # Least-requested server
    self.weighted_serveres[0][0] += 1  # Increment request count
    heapq.heapreplace(self.weighted_serveres, self.weighted_serveres[0])
    self.request_id_to_server[request_id] = server
    return server
```

- **Session affinity**: same `request_id` always routes to the same server
- **Least-requests**: new requests go to the server with the lowest request count
- **Dynamic updates**: server load state is maintained using a min-heap

### `generate`

```python
@rollout_trace_op
async def generate(self, request_id, *, prompt_ids: List[int], sampling_params: Dict[str, Any]) -> List[int]:
    server = self._choose_server(request_id)
    output = await server.generate.remote(
        request_id=request_id,
        prompt_ids=prompt_ids,
        sampling_params=sampling_params,
    )
    return output
```

- Chooses a server based on `request_id`
- Asynchronously calls the server's generate endpoint (token-in-token-out)
- Supports performance tracing via `@rollout_trace_op`

## AgentLoopWorker

`AgentLoopWorker` is responsible for receiving data and passing it down to the specific `AgentLoop`. Although the name suggests it's a worker:

1. From Ray's perspective, `AgentLoopWorker` is stateful—a Ray actor, not a Ray worker;
2. Its core method `generate` is a wrapper that delegates to other classes; for example, `single_turn_agent_loop` and `tool_agent_loop` both call `generate` (which themselves further delegate internally, explained later).

### `__init__`

```python
@ray.remote
class AgentLoopWorker:
    """Agent loop worker takes a batch of messages and run each message in an agent loop."""

    def __init__(self, config: DictConfig, server_handles: list[ray.actor.ActorHandle]):
        """Initialize agent loop manager.

        Args:
            config (DictConfig): YAML config.
            server_handles (List[ray.actor.ActorHandle]): OpenAI compatible LLM server actor handles.
        """
        self.config = config
        self.server_manager = AsyncLLMServerManager(config, server_handles)

        model_path = config.actor_rollout_ref.model.path
        self.model_name = "/".join(model_path.split("/")[-2:])
        local_path = copy_to_local(config.actor_rollout_ref.model.path)
        self.tokenizer = hf_tokenizer(local_path, trust_remote_code=True)

        trace_config = self.config.actor_rollout_ref.rollout.get("trace", {})
        RolloutTraceConfig.init(
            self.config.trainer.project_name,
            self.config.trainer.experiment_name,
            trace_config.get("backend"),
            trace_config.get("token2text", False),
        )
```

- The upstream `config` and `server_handles` are passed in to initialize `AsyncLLMServerManager`, which is saved as `self.server_manager` and passed downstream;
- Sets `model_path`, `local_path`, and `tokenizer` based on `config.actor_rollout_ref.model.path`;
- Configures `RolloutTraceConfig` for trajectory tracing.

### `generate_sequences`

```python
async def generate_sequences(self, batch: DataProto) -> DataProto:
    """Generate sequences from agent loop.

    Args:
        batch (DataProto): Input batch.

    Returns:
        DataProto: Output batch.
    """
    config = self.config.actor_rollout_ref.rollout
    sampling_params = dict(
        temperature=config.temperature,
        top_p=config.top_p,
        repetition_penalty=1.0,
    )

    # override sampling params for validation
    if batch.meta_info.get("validate", False):
        sampling_params["top_p"] = config.val_kwargs.top_p
        sampling_params["temperature"] = config.val_kwargs.temperature

    # by default, assume single-turn agent
    if "agent_name" not in batch.non_tensor_batch:
        batch.non_tensor_batch["agent_name"] = np.array(["single_turn_agent"] * len(batch), dtype=object)

    tasks = []
    agent_names = batch.non_tensor_batch["agent_name"]
    raw_prompts = batch.non_tensor_batch["raw_prompt"]
    if "index" in batch.non_tensor_batch:
        index = batch.non_tensor_batch["index"]
    else:
        index = np.arange(len(raw_prompts))

    trajectory_info = await get_trajectory_info(
        batch.meta_info.get("global_steps", -1), index, batch.meta_info.get("validate", False)
    )

    for agent_name, messages, trajectory in zip(agent_names, raw_prompts, trajectory_info, strict=True):
        tasks.append(
            asyncio.create_task(self._run_agent_loop(agent_name, messages.tolist(), sampling_params, trajectory))
        )
    outputs = await asyncio.gather(*tasks)

    output = self._postprocess(outputs)
    return output
```

- Uses the upstream `config` to construct `sampling_params`; if the batch is for validation, overrides with validation config.
- Extracts `agent_name`, `raw_prompts`, and `index` from `batch.meta_info`; then computes `trajectory_info` which tracks how many rollouts were performed for each prompt.
- Iterates through `agent_names`, `raw_prompts`, and `trajectory_info`, concurrently executing `_run_agent_loop`.
- `_run_agent_loop` instantiates the appropriate `agent_loop` using `agent_name` and calls its `run` method to generate outputs.
- `_postprocess` formats the results into `DataProto`, adding padding, masks, and position encodings.

```python
async def _run_agent_loop(
    self,
    agent_name: str,
    messages: list[dict[str, Any]],
    sampling_params: dict[str, Any],
    trajectory: dict[str, Any],
) -> AgentLoopOutput:
    with rollout_trace_attr(
        step=trajectory["step"],
        sample_index=trajectory["sample_index"],
        rollout_n=trajectory["rollout_n"],
        validate=trajectory["validate"],
        name="agent_loop",
    ):
        assert agent_name in _agent_loop_registry, (
            f"Agent loop {agent_name} not registered, registered agent loops: {_agent_loop_registry.keys()}"
        )

        agent_loop_config = _agent_loop_registry[agent_name]
        agent_loop = hydra.utils.instantiate(
            config=agent_loop_config,
            trainer_config=_DummyConfig(config=self.config),
            server_manager=self.server_manager,
            tokenizer=self.tokenizer,
        )
        output = await agent_loop.run(messages, sampling_params)
        return output
def _postprocess(self, inputs: list[AgentLoopOutput]) -> DataProto:
    # prompts: left pad
    # responses: right pad
    # input_ids = prompt + response
    # attention_mask: [0,0,0,0,1,1,1,1, | 1,1,1,0,0,0,0,0]
    # position_ids:   [0,0,0,0,0,1,2,3, | 4,5,6,7,8,9,10,11]

    self.tokenizer.padding_side = "left"
    outputs = self.tokenizer.pad(
        [{"input_ids": input.prompt_ids} for input in inputs],
        padding="max_length",
        max_length=self.config.actor_rollout_ref.rollout.prompt_length,
        return_tensors="pt",
        return_attention_mask=True,
    )
    prompt_ids, prompt_attention_mask = outputs["input_ids"], outputs["attention_mask"]

    self.tokenizer.padding_side = "right"
    outputs = self.tokenizer.pad(
        [{"input_ids": input.response_ids} for input in inputs],
        padding="max_length",
        max_length=self.config.actor_rollout_ref.rollout.response_length,
        return_tensors="pt",
        return_attention_mask=True,
    )
    response_ids, response_attention_mask = outputs["input_ids"], outputs["attention_mask"]

    outputs = self.tokenizer.pad(
        [{"input_ids": input.response_mask} for input in inputs],
        padding="max_length",
        max_length=self.config.actor_rollout_ref.rollout.response_length,
        return_tensors="pt",
        return_attention_mask=False,
    )
    response_mask = outputs["input_ids"]
    assert response_ids.shape == response_mask.shape
    response_mask = response_mask * response_attention_mask

    input_ids = torch.cat([prompt_ids, response_ids], dim=1)
    attention_mask = torch.cat([prompt_attention_mask, response_attention_mask], dim=1)
    position_ids = (attention_mask.cumsum(dim=1) - 1) * attention_mask

    batch = TensorDict(
        {
            "prompts": prompt_ids,  # [bsz, prompt_length]
            "responses": response_ids,  # [bsz, response_length]
            "response_mask": response_mask,  # [bsz, response_length]
            "input_ids": input_ids,  # [bsz, prompt_length + response_length]
            "attention_mask": attention_mask,  # [bsz, prompt_length + response_length]
            "position_ids": position_ids,  # [bsz, prompt_length + response_length]
        },
        batch_size=len(input_ids),
    )

    num_turns = np.array([input.num_turns for input in inputs], dtype=np.int32)
    metrics = [input.metrics.model_dump() for input in inputs]
    return DataProto(batch=batch, non_tensor_batch={"__num_turns__": num_turns}, meta_info={"metrics": metrics})
```

## AgentLoop

We’ve finally reached the actual agent loop implementation. Here we examine two specific types of `AgentLoop`.

## SingleTurnAgentLoop

This `agent_loop` is the default single-turn conversation loop that handles simple one-turn Q&A and does not support tool calls. The most important method here is `run`:

1. The `messages` passed into the `agent_loop` are actually the `raw_prompt` obtained from the batch, and `apply_chat_template` is used here;
2. Calls `generate` from `server_manager` to obtain `response_ids`;
3. Calculates `response_mask`, truncates based on `response_length`, and wraps everything into `AgentLoopOutput`; padding is handled later in the `_postprocess` function of `AgentLoopManager`.

```python
class SingleTurnAgentLoop(AgentLoopBase):
    """Naive agent loop that only does single-turn chat completion."""

    def __init__(self, config, server_manager, tokenizer):
        super().__init__(config, server_manager, tokenizer)
        self.prompt_length = config.actor_rollout_ref.rollout.prompt_length
        self.response_length = config.actor_rollout_ref.rollout.response_length

    async def run(self, messages: list[dict[str, Any]], sampling_params: dict[str, Any]) -> AgentLoopOutput:
        metrics = {}
        request_id = uuid4().hex
        prompt_ids = await self.loop.run_in_executor(
            None, lambda: self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)
        )

        with simple_timer("generate_sequences", metrics):
            response_ids = await self.server_manager.generate(
                request_id=request_id, prompt_ids=prompt_ids, sampling_params=sampling_params
            )
        response_mask = [1] * len(response_ids)

        output = AgentLoopOutput(
            prompt_ids=prompt_ids,
            response_ids=response_ids[: self.response_length],
            response_mask=response_mask[: self.response_length],
            num_turns=2,
            metrics=metrics,
        )
        return output
```

## ToolAgentLoop

This is the most core component. `ToolAgentLoop` supports multi-turn conversations and tool calls. It can now fully cover the tool call management previously implemented in `SGLangRollout` via `_async_rollout_a_request`. Compared to that, the number of states and transitions in `ToolAgentLoop` is simpler. Essentially, `AgentLoop` externalizes the tool state management that used to be embedded in `SGLangRollout`.

### `init_class`

Below are explanations for key parameters:

1. **`tool_response_truncate_side`**: Controls the truncation strategy when tool response content is too long.
   - `"left"`: Truncate from the left, keep the beginning with a suffix `...(truncated)`
   - `"right"`: Truncate from the right, keep the end with a prefix `(truncated)...`
   - Other values: Truncate from the middle, keep both ends, insert `...(truncated)...` in the middle
2. **`tool_config_path`**: Specifies the config file that contains the list and definitions of tools, used to initialize available tools. Example: `verl/examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml`

```yaml
tools:
  - class_name: "verl.tools.gsm8k_tool.Gsm8kTool"
    config: 
      type: native
    tool_schema:
      type: "function"
      function:
        name: "calc_gsm8k_reward"
        description: "A tool for calculating the reward of gsm8k. (1.0 if parsed answer is correct, 0.0 if parsed answer is incorrect or not correctly parsed)"
        parameters:
          type: "object"
          properties:
            answer:
              type: "string"
              description: "The model's answer to the GSM8K math problem, must be a digits"
          required: ["answer"]
```

- **`tool_list`**, **`tool_schemas`**: Obtained via `initialize_tools_from_config(tool_config_path)`
- **`tool_parser`**: Determined by config like `actor_rollout_ref.rollout.multi_turn.format=hermes`. For example, `HermesToolParser` extracts content between `<tool_call></tool_call>`, returning `function_call` (with `function_name` and `function_arguments`) and other `content`.

```python
@classmethod
def init_class(cls, config, tokenizer):
    if cls._class_initialized:
        return
    cls._class_initialized = True
    print("Performing class-level ToolAgentLoop initialization")

    # Initialize tools from config file
    cls.tokenizer = tokenizer
    cls.max_user_turns = config.actor_rollout_ref.rollout.multi_turn.max_user_turns
    cls.max_assistant_turns = config.actor_rollout_ref.rollout.multi_turn.max_assistant_turns
    cls.max_parallel_calls = config.actor_rollout_ref.rollout.multi_turn.max_parallel_calls
    cls.max_tool_response_length = config.actor_rollout_ref.rollout.multi_turn.max_tool_response_length
    cls.tool_response_truncate_side = config.actor_rollout_ref.rollout.multi_turn.tool_response_truncate_side
    tool_config_path = config.actor_rollout_ref.rollout.multi_turn.tool_config_path
    tool_list = initialize_tools_from_config(tool_config_path) if tool_config_path else []
    cls.tools = {tool.name: tool for tool in tool_list}
    cls.tool_schemas = [tool.tool_schema.model_dump(exclude_unset=True, exclude_none=True) for tool in tool_list]
    cls.tool_parser = cls.get_tool_parser(config.actor_rollout_ref.rollout.multi_turn.format)
    print(f"Initialized tools: {cls.tools}")

    cls.prompt_length = config.actor_rollout_ref.rollout.prompt_length
    cls.response_length = config.actor_rollout_ref.rollout.response_length
    cls.system_prompt = tokenizer.apply_chat_template([{}], add_generation_prompt=False, tokenize=True)
```

### `run`

- Just like in `single_turn_agent_loop`, uses `apply_chat_template` on prompts;
- Initializes `user_turns` and `assistant_turns`, then enters the multi-turn loop until termination:
  - Sends `prompt_ids` to `server_manager.generate` to get `response_ids`, appends the response to `prompt_ids`, and increments `assistant_turns`;
  - Checks stopping conditions: prompt too long, no tool call found, max turns exceeded;
  - Asynchronously executes `_call_tool`: extracts `function_call` from `response_ids`, runs the tool’s `execute(instance_id, tool_args)`, and gets `tool_response`;
  - The `tool_response` is then `apply_chat_template`-d into `tool_response_ids`, appended to `prompt_ids`, and `user_turns` is incremented;
- After the loop ends, constructs an `AgentLoopOutput` where `num_turns = user_turns + assistant_turns + 1` (the prompt counts as one user turn).

```python
@rollout_trace_op
async def run(self, messages: list[dict[str, Any]], sampling_params: dict[str, Any]) -> AgentLoopOutput:
    metrics = {}
    request_id = uuid4().hex
    prompt_ids = await self.loop.run_in_executor(
        None,
        lambda: self.tokenizer.apply_chat_template(
            messages, tools=self.tool_schemas, add_generation_prompt=True, tokenize=True
        ),
    )
    response_mask = []

    user_turns, assistant_turns = 0, 0
    while True:
        with simple_timer("generate_sequences", metrics):
            response_ids = await self.server_manager.generate(
                request_id=request_id, prompt_ids=prompt_ids, sampling_params=sampling_params
            )
        prompt_ids += response_ids
        response_mask += [1] * len(response_ids)
        assistant_turns += 1

        if len(response_mask) >= self.response_length:
            break
        if self.max_assistant_turns and assistant_turns >= self.max_assistant_turns:
            break
        if self.max_user_turns and user_turns >= self.max_user_turns:
            break

        tool_calls = await self.tool_parser.extract_tool_calls(response_ids)
        if not tool_calls:
            break

        tasks = []
        for tool_call in tool_calls[: self.max_parallel_calls]:
            tasks.append(self._call_tool(tool_call))
        with simple_timer("tool_calls", metrics):
            tool_responses = await asyncio.gather(*tasks)
        if any(isinstance(item, Exception) for item in tool_responses):
            break

        tool_response_ids = await self.loop.run_in_executor(
            None,
            lambda messages=tool_responses: self.tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=True
            ),
        )
        tool_response_ids = tool_response_ids[len(self.system_prompt):]

        if len(response_mask) + len(tool_response_ids) >= self.response_length:
            break

        prompt_ids += tool_response_ids
        response_mask += [0] * len(tool_response_ids)
        user_turns += 1

    response_ids = prompt_ids[-len(response_mask):]
    prompt_ids = prompt_ids[: len(prompt_ids) - len(response_mask)]

    output = AgentLoopOutput(
        prompt_ids=prompt_ids,
        response_ids=response_ids[: self.response_length],
        response_mask=response_mask[: self.response_length],
        num_turns=user_turns + assistant_turns + 1,
        metrics=metrics,
    )
    return output
```

### `call_tool`

Uses the tool list to invoke the specified tool (e.g., `calc_gsm8k_reward` from config). Arguments are parsed from the response using the tool parser. If the tool executes successfully, it will release any allocated resources. Finally, the `tool_response` is truncated according to the `tool_response_truncate_side`.

```python
async def _call_tool(self, tool_call: FunctionCall) -> dict[str, str]:
    """Call tool and return tool response."""
    tool, instance_id = None, None
    try:
        tool_name = tool_call.name
        tool_args = json.loads(tool_call.arguments)
        tool = self.tools[tool_name]

        instance_id = await tool.create()
        tool_response, _, _ = await tool.execute(instance_id, tool_args)
    except Exception as e:
        logger.exception(f"Error when executing tool: {e}")
        return e
    finally:
        if tool and instance_id:
            await tool.release(instance_id)

    if len(tool_response) > self.max_tool_response_length:
        if self.tool_response_truncate_side == "left":
            tool_response = tool_response[: self.max_tool_response_length] + "...(truncated)"
        elif self.tool_response_truncate_side == "right":
            tool_response = "(truncated)..." + tool_response[-self.max_tool_response_length:]
        else:
            length = self.max_tool_response_length // 2
            tool_response = tool_response[:length] + "...(truncated)..." + tool_response[-length:]

    return {
        "role": "tool",
        "content": tool_response,
    }
```
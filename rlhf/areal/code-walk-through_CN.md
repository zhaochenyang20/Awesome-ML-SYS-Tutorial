# Code Walkthrough of AReaL

[English](code-walk-through_EN.md) | [ä¸­æ–‡](code-walk-through_CN.md)

æ—©åœ¨ 25 å¹´å¹´åˆï¼Œå°±æœ‰æŸä½ infra åœˆèµ„æ·±å¤§ä½¬ç››èµ AReaL çš„ä»£ç å†™çš„æ˜¯æ•´ä¸ªåœˆå­é‡Œæœ€æœ‰è‰ºæœ¯æ€§çš„ã€‚è¶ç€ AReaL åœ¨ 25 å¹´ç»è¿‡äº†å‡ ä¸ªé‡å¤§ç‰ˆæœ¬çš„å‘å¸ƒï¼Œä»¥åŠ [asystem-amem](https://github.com/inclusionAI/asystem-amem) ç­‰æ¡†æ¶ç›¸å…³å·¥ä½œçš„å‘å¸ƒï¼Œæˆ‘ä»¬ SGLang RL å°ç»„å‘å¤§å®¶åˆ†äº«è¿™ä»½ AReaL çš„å­¦ä¹ ç¬”è®°ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå´ç¿¼è€å¸ˆæ˜¯ RL é¢†åŸŸç»éªŒä¸°å¯Œåˆå¹´è½»çš„é¢†å†›ç§‘å­¦å®¶ï¼ˆå¯¹å´è€å¸ˆè€Œè¨€ï¼Œå¹´è½»å’Œç»éªŒä¸°å¯Œå¹¶ä¸çŸ›ç›¾ ğŸ˜‚ï¼‰ï¼ŒAReaL åœ¨å¼‚æ­¥è®­ç»ƒä¸Šçš„è®¾è®¡å¯è°“æ˜¯ä¸šç•Œé¡¶æµï¼Œè¿™ä¹Ÿä¼šæ˜¯æˆ‘ä»¬ç€é‡å­¦ä¹ çš„éƒ¨åˆ†ã€‚

# Start from Example

æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼ˆ`examples/math/gsm8k_grpo.py`ï¼‰å¼€å§‹å­¦ä¹  AReaL çš„å·¥ä½œæµç¨‹ï¼š

![AReaL Architecture Overview](areal_overview.jpg)

<details>
<summary>gsm8k_grpo.py ä¸­ main() ä»£ç </summary>

```python
def main(args):
    config, _ = load_expr_config(args, GRPOConfig)

    rank = int(os.getenv("RANK"))
    tokenizer = load_hf_tokenizer(config.tokenizer_path)

    seeding.set_random_seed(config.seed, key=f"trainer{rank}")
    allocation_mode = AllocationMode.from_str(config.allocation_mode)
    parallel_strategy = allocation_mode.train
    assert parallel_strategy is not None

    # Initialize train engine
    actor = FSDPPPOActor(config=config.actor)
    actor.create_process_group(parallel_strategy=parallel_strategy)

    # Create dataset and dataloaders
    train_dataset = get_custom_dataset(
        split="train", dataset_config=config.train_dataset, tokenizer=tokenizer
    )
    valid_dataset = get_custom_dataset(
        split="test", dataset_config=config.valid_dataset, tokenizer=tokenizer
    )

    train_dataloader = create_dataloader(
        train_dataset,
        rank=actor.data_parallel_rank,
        world_size=actor.data_parallel_world_size,
        dataset_config=config.train_dataset,
    )
    valid_dataloader = create_dataloader(
        valid_dataset,
        rank=actor.data_parallel_rank,
        world_size=actor.data_parallel_world_size,
        dataset_config=config.valid_dataset,
    )
    ft_spec = FinetuneSpec(
        total_train_epochs=config.total_train_epochs,
        dataset_size=len(train_dataloader) * config.train_dataset.batch_size,
        train_batch_size=config.train_dataset.batch_size,
    )

    # Initialize inference engine
    rollout = RemoteSGLangEngine(config.rollout)
    rollout.initialize(train_data_parallel_size=parallel_strategy.dp_size)
    eval_rollout = RemoteSGLangEngine(deepcopy(config.rollout))
    # NOTE: eval does not have any offpolicyness control
    eval_rollout.config.max_head_offpolicyness = int(1e12)
    eval_rollout.initialize()

    weight_update_meta = WeightUpdateMeta.from_fsdp_xccl(allocation_mode)

    actor.initialize(None, ft_spec)
    actor.connect_engine(rollout, weight_update_meta)

    ref = None
    if config.actor.kl_ctl > 0 and config.ref is not None:
        ref = FSDPPPOActor(config=config.ref)
        ref.create_process_group(parallel_strategy=parallel_strategy)
        ref.initialize(None, ft_spec)

    # Create rollout workflow
    if tokenizer.pad_token_id not in config.gconfig.stop_token_ids:
        config.gconfig.stop_token_ids.append(tokenizer.pad_token_id)
    if tokenizer.eos_token_id not in config.gconfig.stop_token_ids:
        config.gconfig.stop_token_ids.append(tokenizer.eos_token_id)
    workflow = RLVRWorkflow(
        reward_fn=gsm8k_reward_fn,
        gconfig=config.gconfig,
        tokenizer=tokenizer,
        enable_thinking=False,
        dump_dir=os.path.join(
            StatsLogger.get_log_path(config.stats_logger), "generated"
        ),
    )
    eval_workflow = RLVRWorkflow(
        reward_fn=gsm8k_reward_fn,
        gconfig=config.gconfig.new(temperature=0.6),
        tokenizer=tokenizer,
        enable_thinking=False,
        rollout_stat_scope="eval-rollout",
        dump_dir=os.path.join(
            StatsLogger.get_log_path(config.stats_logger), "generated-eval"
        ),
    )

    # Run training.
    saver = Saver(config.saver, ft_spec)
    stats_logger = StatsLogger(config, ft_spec)
    evaluator = Evaluator(config.evaluator, ft_spec)

    recover_handler = RecoverHandler(config.recover, ft_spec)
    recover_info = recover_handler.load(
        actor,
        saver,
        evaluator,
        stats_logger,
        train_dataloader,
        inference_engine=rollout,
        weight_update_meta=weight_update_meta,
    )
    start_step = (
        recover_info.last_step_info.next().global_step
        if recover_info is not None
        else 0
    )

    total_epochs = config.total_train_epochs
    steps_per_epoch = len(train_dataloader)
    max_steps = total_epochs * steps_per_epoch

    for global_step in range(start_step, max_steps):
        epoch = global_step // steps_per_epoch
        step = global_step % steps_per_epoch
        step_info = StepInfo(
            global_step=global_step,
            epoch=epoch,
            epoch_step=step,
            steps_per_epoch=steps_per_epoch,
        )

        with stats_tracker.record_timing("rollout"):
            batch = actor.prepare_batch(
                train_dataloader,
                granularity=actor.config.group_size,
                workflow=workflow,
                should_accept_fn=lambda sample: True,
            )

        if config.actor.recompute_logprob or config.actor.use_decoupled_loss:
            with stats_tracker.record_timing("recompute_logp"):
                logp = actor.compute_logp(batch)
                batch["prox_logp"] = logp
                log_gpu_stats("recompute logp")

        if ref is not None:
            with stats_tracker.record_timing("ref_logp"):
                batch["ref_logp"] = ref.compute_logp(batch)
                log_gpu_stats("ref logp")

        with stats_tracker.record_timing("compute_advantage"):
            actor.compute_advantages(batch)
            log_gpu_stats("compute advantages")

        with stats_tracker.record_timing("train_step"):
            actor.ppo_update(batch)
            actor.step_lr_scheduler()
            log_gpu_stats("ppo update")

        # pause inference for updating weights, save, and evaluation
        rollout.pause()

        with stats_tracker.record_timing("update_weights"):
            actor.update_weights(weight_update_meta)

            actor.set_version(global_step + 1)
            rollout.set_version(global_step + 1)
            eval_rollout.set_version(global_step + 1)

        with stats_tracker.record_timing("save"):
            saver.save(actor, epoch, step, global_step, tokenizer=tokenizer)

        with stats_tracker.record_timing("checkpoint_for_recover"):
            recover_handler.dump(
                actor,
                step_info,
                saver,
                evaluator,
                stats_logger,
                train_dataloader,
                tokenizer=tokenizer,
            )

        current_platform.synchronize()
        dist.barrier(group=actor.cpu_group)

        with stats_tracker.record_timing("eval"):

            def evaluate_fn():
                if actor.is_data_parallel_head():
                    cnt = 0
                    for data in valid_dataloader:
                        for item in data:
                            eval_rollout.submit(item, eval_workflow)
                            cnt += 1
                    eval_rollout.wait(cnt, timeout=None)
                current_platform.synchronize()
                dist.barrier(group=actor.cpu_group)

            evaluator.evaluate(
                evaluate_fn,
                epoch,
                step,
                global_step,
            )

        current_platform.synchronize()
        dist.barrier(group=actor.cpu_group)

        # Upload statistics to the logger (e.g., wandb)
        stats = stats_tracker.export_all(reduce_group=actor.data_parallel_group)
        stats_logger.commit(epoch, step, global_step, stats)

        current_platform.synchronize()
        dist.barrier(group=actor.cpu_group)

        # Resume rollout
        rollout.resume()

    stats_logger.close()
    eval_rollout.destroy()
    rollout.destroy()
    if ref is not None:
        ref.destroy()
    actor.destroy()
```
</details>


## Init

åˆå§‹åŒ– Train Engine (å¦‚Â `FSDPPPOActor`)ã€Inference Engine (å¦‚Â `RemoteSGLangEngine`) å’Œå¯é€‰çš„ Reference Engineï¼ŒåŠ è½½æ•°æ®é›† (`DataLoader`)ï¼Œé€šè¿‡Â `weight_update_meta`Â å»ºç«‹èµ· Actor å‘Â Rollout Engine ä¼ è¾“æƒé‡çš„é€šé“ã€‚

åˆ›å»º Rollout Workflowã€‚ä¸Šå›¾ä½¿ç”¨çš„æ˜¯æœ€åŸºç¡€çš„Â `RLVRWorkflow`ã€‚AReaL è¿˜æä¾›äº†Â `MultiTurnWorkflow`ï¼Œæ”¯æŒå¤æ‚çš„å¤šè½®å¯¹è¯ RL ä»»åŠ¡ï¼Œä»–ä»¬éƒ½æ˜¯æŠ½è±¡ç±»`RolloutWorkflow`ï¼ˆ`AReaL/areal/api/workflow_api.py`ï¼‰çš„å®ç°ã€‚è¿™äº› `RolloutWorkflow` çš„å®ç°ç»Ÿä¸€äº†ç”Ÿæˆæ¥å£ï¼Œåœ¨è°ƒç”¨ workflow ç”Ÿæˆæ—¶æ— éœ€å…³å¿ƒå…·ä½“çš„äº¤äº’ç»†èŠ‚ï¼Œåªéœ€è¦è°ƒç”¨Â `workflow.arun_episode()`ï¼Œå°±å¯ä»¥æ‹¿åˆ°ä¸€æ‰¹åŒ…å«æ ‡å‡†è®­ç»ƒå­—æ®µçš„Â Trajectoryã€‚

## Rollout

Actor è°ƒç”¨  `prepare_batch`Â è¯·æ±‚å¾—åˆ° rollout éƒ¨åˆ†ç”Ÿæˆçš„ trajectoryã€‚è¿™éƒ¨åˆ†çš„è°ƒç”¨è·¯å¾„ä¸º `FSDPPPOACTOR.prepare_batch` â†’ `DistRolloutCoordinator.prepare_batch` â†’ `RemoteInfEngine.prepare_batch` â†’ `WorkflowExecutor.prepare_batch`ã€‚è¿™é‡Œ`WorkflowExecutor` çš„`prepare_batch` è§¦å‘äº†å¼‚æ­¥ Productor-Consumer æµæ°´çº¿ã€‚

> [!TIP]
> **Producer-Consumer**
>
> ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ ï¼ˆProducer-Consumer Patternï¼‰Â æ˜¯ä¸€ç§ç»å…¸çš„å¹¶å‘è®¾è®¡æ¨¡å¼ï¼Œç”¨äºè§£å†³ä¸¤ä¸ªå¤„ç†é€Ÿç‡ä¸ä¸€è‡´çš„ç»„ä»¶ä¹‹é—´çš„æ•°æ®ä¼ è¾“é—®é¢˜ã€‚
>
> - **æ ¸å¿ƒæ€æƒ³**ï¼šç”Ÿäº§è€… ï¼ˆProducerï¼‰ å’Œæ¶ˆè´¹è€… ï¼ˆConsumerï¼‰å¹¶ä¸ç›´æ¥é€šä¿¡ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªÂ ç¼“å†²åŒº ï¼ˆBuffer/Queueï¼‰è¿›è¡Œè§£è€¦ã€‚
> - **Producer**ï¼šè´Ÿè´£ç”Ÿæˆæ•°æ®ï¼Œå¹¶å°†å…¶æ”¾å…¥ç¼“å†²åŒºã€‚å¦‚æœç¼“å†²åŒºæ»¡äº†ï¼Œç”Ÿäº§è€…å¿…é¡»ç­‰å¾…æˆ–ä¸¢å¼ƒæ•°æ®ã€‚
> - **Consumer**ï¼šè´Ÿè´£ä»ç¼“å†²åŒºå–å‡ºæ•°æ®è¿›è¡Œå¤„ç†ã€‚å¦‚æœç¼“å†²åŒºç©ºäº†ï¼Œæ¶ˆè´¹è€…å¿…é¡»ç­‰å¾…ã€‚
> - **Buffer**ï¼šå¹³æ»‘äº†ç”Ÿäº§å’Œæ¶ˆè´¹çš„é€Ÿç‡æ³¢åŠ¨ï¼Œå…è®¸ä¸¤è€…å¹¶è¡Œå·¥ä½œï¼Œäº’ä¸é˜»å¡ã€‚

ä¸‹å›¾å±•ç¤ºäº† rollout è¿‡ç¨‹ä¸­çš„äº¤äº’ä¸æ•°æ®æµè½¬è¿‡ç¨‹ã€‚

```mermaid
sequenceDiagram
    autonumber
    participant User as Main Thread (Train Loop)
    participant Executor as WorkflowExecutor
    participant Producer as Producer Thread (_commit_loop)
    participant Runner as AsyncTaskRunner
    participant Workflow as RolloutWorkflow (e.g. MathWorkflow)
    participant Engine as RemoteSGLangEngine
    participant SGLang as SGLang Server (HTTP)

    User->>Executor: prepare_batch(dataloader)
    Executor->>Executor: submit(data) -> Enqueue to _pending_inputs

    loop Producer Thread
        Producer->>Executor: Check Staleness & Capacity
        Producer->>Runner: submit(task) if allowed
    end

    loop AsyncTaskRunner (Worker)
        Runner->>Workflow: arun_episode(engine, data)
        activate Workflow
        
        Workflow->>Engine: agenerate(prompts)
        activate Engine
        
        Engine->>SGLang: HTTP POST /generate
        SGLang-->>Engine: Response (Text)
        Engine-->>Workflow: Interaction Objects
        deactivate Engine

        Workflow->>Workflow: Compute Rewards (Local or API)
        Workflow->>Workflow: Pack Trajectory (Tensors)
        
        Workflow-->>Runner: Return Trajectory
        deactivate Workflow
    end
    loop Consumer Thread
        Runner->>Executor: Fetch Result -> _pending_results
    end

    Executor->>User: wait() -> Returns Batch of Trajectories
```


æˆ‘ä»¬è¿›ä¸€æ­¥æ·±å…¥ç†è§£ä¸Šå›¾æåˆ°çš„å…³é”®ç±»ã€‚

### RolloutWorkflow

`RolloutWorkflow` ä½äº `areal/api/workflow_api.py`Â ï¼Œæ˜¯ AReaL ä¸­å®šä¹‰ Agent è¡Œä¸ºçš„æ ¸å¿ƒæŠ½è±¡ï¼ˆå®šä¹‰äºÂ `areal/api/workflow_api.py`ï¼‰ã€‚

<details>
<summary>æŠ½è±¡ç±» RolloutWorkflow ä»£ç  ï¼ˆAReaL/areal/api/workflow_api.pyï¼‰</summary>

```python
class RolloutWorkflow(ABC):
    @abstractmethod
    async def arun_episode(
        self, engine: InferenceEngine, data: dict[str, Any]
    ) -> dict[str, Any] | None | dict[str, InteractionWithTokenLogpReward]:
        raise NotImplementedError()

```

</details>

ä»è¿™æ®µä»£ç ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ`RolloutWorkflow`çš„æ‰€æœ‰å…·ä½“å®ç°ï¼ˆå¦‚Â `RLVRWorkflow`,Â `MultiTurnWorkflow`ï¼Œä½äºÂ `areal/workflow/`ï¼‰éƒ½å¿…é¡»å®ç°`arun_episode()`è¿™ä¸€æ ¸å¿ƒå¼‚æ­¥å‡½æ•°ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä»¥`RLVRWorkflow` ä¸ºä¾‹ï¼Œåˆ†æä¸€ä¸‹`RolloutWorkflow`çš„å…·ä½“å®ç°æ˜¯ä»€ä¹ˆæ ·çš„ã€‚

init ä¸­é™¤äº† rollout å¿…è¦å‚æ•°ä¹‹å¤–ï¼Œ`reward_fn` ä¹Ÿä½œä¸º input è¾“å…¥ï¼Œreward è®¡ç®—ä¹ŸåŒ…å«åœ¨ workflow ä¸­ã€‚

<details>
<summary>RLVRWorkflow.init</summary>

```python
class RLVRWorkflow(RolloutWorkflow):
    """Single-turn reward learning workflow supporting optional thinking tokens."""

    def __init__(
        self,
        reward_fn: Callable[..., Any],
        gconfig: GenerationHyperparameters,
        tokenizer: PreTrainedTokenizerFast,
        enable_thinking: bool = False,
        rollout_stat_scope: str = "rollout",
        dump_dir: str | None = None,
        get_input_ids_fn: Callable[
            [Any, PreTrainedTokenizerFast, bool], list[int]
        ] = default_get_input_ids_fn,
        data_extract_prompt_fn: Callable[
            [dict[str, Any]], Any
        ] = default_data_extract_prompt_fn,
    ):
        self.reward_fn = reward_fn
        self.gconfig = gconfig
        self.tokenizer = tokenizer
        self.enable_thinking = enable_thinking
        self.dump_dir = dump_dir
        self.rollout_stat_scope = rollout_stat_scope
        self.async_reward_fn = AsyncRewardWrapper(reward_fn)
        self.get_input_ids_fn = get_input_ids_fn
        self.data_extract_prompt_fn = data_extract_prompt_fn
        if self.dump_dir is not None and not os.path.exists(self.dump_dir):
            os.makedirs(self.dump_dir, exist_ok=True)
```
</details>


`arun_episode()`ä½œä¸ºå¯¹å¤–çš„ç»Ÿä¸€å…¥å£ï¼Œæ¥æ”¶ InferenceEngine å’Œè¾“å…¥æ•°æ® dataï¼Œè´Ÿè´£æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„ rollout æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š

Prompt æ„é€ ä¸é¢„å¤„ç†ã€å¹¶å‘é‡‡æ ·ã€reward è®¡ç®—ï¼Œä»¥åŠæœ€ç»ˆ Trajectory çš„ç»„è£…ä¸è¿”å›ã€‚

<details>
<summary>RLVRWorkflow.arun_episode()</summary>

```python
RLVRWorkflow._collect_samples()
```
</details>


`arun_episode()` å†…éƒ¨é€šè¿‡è°ƒç”¨ `_collect_samples()` è¿›è¡Œé‡‡æ ·ä¸ reward è®¡ç®—ï¼Œè€Œ `_collect_samples()` åˆ™è¿›ä¸€æ­¥è°ƒç”¨ inference engine çš„ `agenerate()`ï¼Œè¿™æ˜¯å®é™…è§¦å‘æ¨¡å‹æ¨ç†çš„æ¥å£ã€‚

<details>
<summary>RLVRWorkflow._collect_samples()</summary>

```python
prepare_batch()
```
</details>


### **WorkflowExecutor**

`WorkflowExecutor` ä½äº`areal/core/workflow_executor.py`ï¼Œæ˜¯ AReaL æ¡†æ¶ä¸­è´Ÿè´£åè°ƒè®­ç»ƒè¿›ç¨‹ä¸è¿œç¨‹æ¨ç†æœåŠ¡ä¹‹é—´çš„å¼‚æ­¥è°ƒåº¦ä¸å¹¶å‘æ§åˆ¶ç»„ä»¶ã€‚å®ƒé€šè¿‡å°è£…Â Producer-Consumer çº¿ç¨‹æ¨¡å‹ï¼Œå…¶å†…éƒ¨ç»“æ„å¦‚ä¸‹ï¼š

`prepare_batch()`ï¼šå°è£…äº† `submit()` å’Œ `wait()` ï¼Œå°† input data / prompts ä½œä¸ºè¾“å…¥ï¼Œoutput ä½œä¸ºè¾“å‡ºã€‚

<details>
<summary>prepare_batch() ä»£ç </summary>

```python
def prepare_batch(
        self,
        dataloader: StatefulDataLoader,
        workflow: RolloutWorkflow | type[RolloutWorkflow] | str,
        workflow_kwargs: dict[str, Any] | None = None,
        should_accept_fn: Callable[[dict[str, Any]], bool] | str | None = None,
    ):

        manager = self.staleness_manager
        if not hasattr(self, "data_generator"):
            self.data_generator = cycle_dataloader(dataloader)
        assert dataloader.batch_size is not None
        cnt = 0
        results = []
        while True:
            # Submit at least two batches to allow maximum overlap
            if (
                len(self._pending_inputs) < manager.get_pending_limit()
                and self.runner.get_input_queue_size() + dataloader.batch_size
                < self.runner.max_queue_size
            ):
                data = next(self.data_generator)
                perf_tracer.instant(
                    "workflow_executor.prepare_batch",
                    category="scheduler",
                    args={"data": len(data)},
                )
                for item in data:
                    self.submit(
                        item,
                        workflow=workflow,
                        should_accept_fn=should_accept_fn,
                        workflow_kwargs=workflow_kwargs,
                    )
            try:
                res = self.wait(count=1, timeout=1)
                if not res:
                    continue
                cnt += 1
                results.append(res)
                if cnt >= dataloader.batch_size:
                    break
            except (TimeoutError, queue.Full):
                pass
        return concat_padded_tensors(results)
```
</details>


Main threadï¼š main thread ä¸æ¶‰åŠç”Ÿæˆæˆ–è€…æ§åˆ¶ï¼Œåªæ˜¯å•çº¯çš„å°† tasks åŠ å…¥é˜Ÿåˆ—å’Œå°† output è¾“å‡ºã€‚`submit()`å°† tasks åŠ å…¥`_pending_inputs`é˜Ÿåˆ—ï¼Œ`wait()` è½®è¯¢`_pending_results`é˜Ÿåˆ—ï¼š

<details>
<summary>submit() å’Œ wait() ä»£ç </summary>

```python
def submit(
        self,
        data: dict[str, Any],
        workflow: RolloutWorkflow | type[RolloutWorkflow] | str,
        workflow_kwargs: dict[str, Any] | None = None,
        should_accept_fn: Callable[[dict[str, Any]], bool] | str | None = None,
    ) -> None:
        ...

        # Resolve workflow and should_accept to their concrete forms
        resolved_workflow = self._resolve_workflow(workflow, workflow_kwargs)
        resolved_should_accept_fn = self._resolve_should_accept_fn(should_accept_fn)

        task_id = perf_tracer.register_task()
        task_input = _RolloutTaskInput(
            data=data,
            workflow=resolved_workflow,
            should_accept_fn=resolved_should_accept_fn,
            task_id=task_id,
        )

        # Enqueue to thread-safe queue (may block if queue is full)
        self._pending_inputs.append(task_input)

        # Notify staleness manager of enqueued rollout tasks
        self.staleness_manager.on_rollout_enqueued()
        
        ...

    def wait(
        self, count: int, timeout: float | None = None, raise_timeout: bool = True
    ) -> dict[str, Any]:
        
        ...

        # Drain all available requests and sort them by time of creation
        # This prioritizes data submitted earlier.
        results: list[TimedResult[_RolloutResult]] = []
        while True:
            try:
                results.append(self._pending_results.popleft())
            except IndexError:
                break
        # Sort results be create time
        results.sort(key=lambda x: x.create_time)
        results, pending = results[:count], results[count:]
        self._pending_results.extendleft(reversed(pending))

        # Shuffle for randomness (helps with data diversity)
        random.shuffle(results)

        # Concatenate into batch tensor format
        trajectories = [r.data.trajectory for r in results if r.data is not None]
        return concat_padded_tensors(trajectories)
```
</details>


Producer threadï¼ˆ`_commit_loop`ï¼‰ï¼šæ ¹æ® `StalenessManager.get_capacity()` è¿”å›çš„å¯ç”¨å®¹é‡ç¡®å®šæœ¬è½®å¯æäº¤çš„ä»»åŠ¡æ•°é‡ï¼Œå¹¶ä» `_pending_inputs` ä¸­å–å‡ºç›¸åº”æ•°é‡çš„ä»»åŠ¡ï¼Œå°†å…¶äº¤ä»˜ç»™ `AsyncTaskRunner` æ‰§è¡Œç”Ÿæˆæµç¨‹ã€‚

<details>
<summary>_commit_loop() ä»£ç </summary>

```python
def _commit_loop(self) -> None:

  while not self._shutdown_event.is_set():
      try:
            ...

          # Get capacity from staleness manager
          version = self.inference_engine.get_version()
          capacity = self.staleness_manager.get_capacity(version)

          if capacity <= 0:
              time.sleep(_POLL_INTERVAL_SECONDS)
              continue

          # Try to submit up to 'capacity' tasks
          for _ in range(capacity):
              try:
                  task = self._pending_inputs.popleft()
              except IndexError:
                  break

              # Submit to runner (may raise TaskQueueFullError)
              workflow_fn = self._create_workflow_task(task)
              try:
                  self.runner.submit(workflow_fn)

                  self.staleness_manager.on_rollout_submitted()
                  if self.config.enable_rollout_tracing:
                      self.logger.info(f"Submit rollout. {self._rollout_stats()}")
              except TaskQueueFullError:
                  # Put back and retry later
                  self._pending_inputs.appendleft(task)
                  break

          ...
```
</details>


Consumer threadï¼ˆ`_fetch_loop`ï¼‰ï¼šä»`AsyncTaskRunner`æ”¶é›†ç»“æœå¹¶å­˜å…¥`_pending_results`

<details>
<summary>_fetch_loop() ä»£ç </summary>

```python
def _fetch_loop(self) -> None:
        """Consumer thread main loop - continuously collects results from runner.

        This method runs in a background thread and continuously:
        1. Checks for errors from other threads (fail-fast)
        2. Polls AsyncTaskRunner for available results (non-blocking)
        3. Collects results in batches up to 100 with short timeout (0.05s)
        4. Filters out None (rejected) results
        5. Appends accepted TimedResult objects to _pending_results deque

        The loop exits when _shutdown_event is set. Polling interval: 0.5s.
        """
        while not self._shutdown_event.is_set():
            try:
                # Check for errors from other threads (fail-fast)
                self._check_thread_exception()

                # Poll runner for available results (non-blocking)
                output_queue_size = self.runner.get_output_queue_size()

                if output_queue_size == 0:
                    time.sleep(_POLL_INTERVAL_SECONDS)
                    continue

                # Collect all available results at once (batch for efficiency)
                # Limit batch size to avoid blocking too long
                count = min(output_queue_size, _MAX_FETCH_BATCH_SIZE)

                try:
                    # Use short timeout for responsiveness (latency-optimized)
                    results = self.runner.wait(
                        count=count, timeout=0.05, with_timing=True
                    )

                    # Enqueue all results. Filtering will be delayed to
                    # `rollout_batch` or `prepare_batch`.
                    for result in results:
                        self._pending_results.append(result)

                except TimeoutError:
                    # No results ready yet
                    pass

                # Small sleep to avoid busy-waiting (latency-optimized)
                time.sleep(_POLL_INTERVAL_SECONDS)

            except Exception as e:
                self.logger.error("Consumer thread failed", exc_info=True)
                self._set_thread_exception(e)
                break
```
</details>


`AsyncTaskRunner`ï¼š`WorkflowExecutor` å†…éƒ¨çš„é€šç”¨å¼‚æ­¥æ‰§è¡Œå™¨ï¼Œè´Ÿè´£åœ¨åå°çº¿ç¨‹ä¸­ç®¡ç†äº‹ä»¶å¾ªç¯ (Event Loop)ï¼Œå¹¶å‘æ‰§è¡Œé«˜å¯†åº¦çš„ç½‘ç»œ I/O ä»»åŠ¡ï¼ˆå³è°ƒç”¨ Inference Engine çš„ `agenerate`ï¼‰

<details>
<summary>AsyncTaskRunner ä»£ç </summary>

```python
async def _run_async_loop(self):
    """Main async event loop that processes tasks.

    This loop:
    1. Pulls tasks from input_queue when not paused
    2. Creates asyncio.Task instances for each
    3. Waits for task completion
    4. Places results in output_queue
    5. Continues until exiting signal is set
    """
    running_tasks: dict[str, _Task[T]] = {}
    task_id = 0

    try:
        while not self.exiting.is_set():
            # 1. Pull new tasks from input queue
            while not self.paused.is_set() and self.input_queue.qsize() > 0:
                try:
                    task_input = self.input_queue.get_nowait()

                    # 2. Create asyncio task (This is where execution starts)
                    async_task = asyncio.create_task(
                        task_input.async_fn(*task_input.args, **task_input.kwargs),
                        name=str(task_id),
                    )

                    # Store task with metadata
                    running_tasks[str(task_id)] = _Task(
                        create_time=time.monotonic_ns(),
                        task=async_task,
                        task_input=task_input,
                    )
                    task_id += 1
                except queue.Empty:
                    break

            # 3. Wait for any task to complete
            done = []
            if running_tasks:
                tasks = [t.task for t in running_tasks.values()]
                done, _ = await asyncio.wait(
                    tasks,
                    timeout=self.poll_wait_time,
                    return_when=asyncio.FIRST_COMPLETED,
                )

            # 4. Process completed tasks and output results
            for async_task in done:
                tid = async_task.get_name()
                task_obj = running_tasks.pop(tid)
                try:
                    result = await async_task
                    # Place result in output queue
                    self.output_queue.put_nowait(
                        TimedResult(create_time=task_obj.create_time, data=result)
                    )
                except Exception as e:
                    # Error handling...
                    pass

            # Sleep to avoid busy-waiting
            await asyncio.sleep(self.poll_sleep_time)

```
</details>


### StalenessManager

`StalenessManager` ä½äº `areal/core/staleness_manager.py`ï¼Œå®ƒç”¨ä¸¤ä¸ªé™åˆ¶ï¼ˆå¹¶å‘é™åˆ¶ + staleness é™åˆ¶ï¼‰æ¥è®¡ç®—ç°åœ¨è¿˜èƒ½å†æ¥æ”¶å¤šå°‘ä¸ªæ–° rolloutã€‚é€šè¿‡ä¸‰ä¸ªå‚æ•°æ¥æ§åˆ¶ï¼Œåˆ†åˆ«æ˜¯ï¼š

- `max_concurrent_rollouts`ï¼šé™åˆ¶å¹¶å‘ä¸Šé™ï¼Œå³æ­£åœ¨è¿è¡Œçš„ç”Ÿæˆä»»åŠ¡çš„æœ€å¤§æ•°é‡
- `consumer_batch_size`ï¼šconsumer æ‰¹æ¬¡å¤§å°ï¼Œå³ trainer è®­ç»ƒçš„æ‰¹æ¬¡å¤§å°
- `max_staleness`ï¼šæ ·æœ¬å’Œå½“å‰æ¨¡å‹ç‰ˆæœ¬ä¹‹é—´æœ€å¤šå…è®¸ç›¸å·®å¤šå°‘ç‰ˆæœ¬

<details>
<summary>init ä»£ç </summary>

```python
class StalenessManager:
   
    def __init__(
        self,
        max_concurrent_rollouts: int,
        consumer_batch_size: int,
        max_staleness: int,
    ):
        self.max_concurrent_rollouts = max_concurrent_rollouts
        self.consumer_batch_size = consumer_batch_size
        self.max_staleness = max_staleness

        # Thread-safe access to rollout statistics
        self.lock = Lock()
        self.rollout_stat = RolloutStat()
```
</details>


å…¶ä¸­çš„æ ¸å¿ƒç®—æ³•ä½äº `get_capacity()`ï¼Œå…¬å¼å¦‚ä¸‹ã€‚

```python
max_samples = (max_staleness + current_version + 1) * consumer_batch_size
capacity = min(concurrency_limit, max_samples - current_samples)
```

è¿™é‡Œè¯´ä¸€ä¸‹æˆ‘å¯¹è¿™ä¸ªå…¬å¼çš„ç†è§£ï¼šåœ¨å½“å‰æƒé‡ç‰ˆæœ¬ä¸º `current_version` æ—¶ï¼Œæˆ‘ä»¬ä¸ä»…è¦æœåŠ¡ç°åœ¨çš„ trainerï¼Œè¿˜è¦ä¸ºæœªæ¥æœ€å¤šå‰è¿› `max_staleness` ä¸ªç‰ˆæœ¬çš„ trainerï¼ˆå³ç‰ˆæœ¬ä» `current_version` åˆ° `current_version + max_staleness`ï¼‰æå‰å‡†å¤‡æ•°æ®ã€‚è€Œ trainer æ¯ä¸ªç‰ˆæœ¬ä¼šæ¶ˆè€— `consumer_batch_size` ä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆä»ç‰ˆæœ¬ 0 ä¸€ç›´åˆ° `current_version + max_staleness`ï¼Œç†è®ºä¸Šæœ€å¤šå¯ä»¥å®‰å…¨æ¶ˆè´¹çš„æ ·æœ¬æ€»æ•°å°±æ˜¯ï¼š

```python
max_samples = (max_staleness + current_version + 1) * consumer_batch_size
```

å½“å‰ç³»ç»Ÿé‡Œå·²ç»ç´¯ç§¯çš„æ ·æœ¬æ•°è®°ä¸º `current_samples`ï¼ˆåŒ…æ‹¬å·²æ¥å—çš„å’Œæ­£åœ¨è¿è¡Œçš„ rolloutï¼‰ï¼Œé‚£ä¹ˆåœ¨ä¸è®©æœªæ¥çš„æ ·æœ¬â€œè¿‡æœŸâ€çš„å‰æä¸‹ï¼Œç°åœ¨è¿˜èƒ½å†ç”Ÿæˆçš„æ ·æœ¬æ•°é‡å°±æ˜¯ï¼š

```python
staleness_capacity = max_samples - current_samples
```

ä¹Ÿå°±æ˜¯è¯´ï¼Œ`max_staleness` æ§åˆ¶çš„æ˜¯â€œä»ç°åœ¨èµ·å¾€å‰èµ°æœ€å¤š `max_staleness` ä¸ªç‰ˆæœ¬æ—¶ï¼Œè¿™äº›æ ·æœ¬ä»ç„¶ä¸è¿‡æœŸâ€ï¼Œå…¬å¼é€šè¿‡é™åˆ¶æ ·æœ¬æ€»é‡æ¥é—´æ¥ä¿è¯è¿™ä¸€ç‚¹ã€‚

ç†è§£ staleness ç®—æ³•ä¹‹åçœ‹ä»£ç å°±éå¸¸æ¸…æ™°äº†ã€‚é¦–å…ˆè®¡ç®—åœ¨å¹¶å‘é™åˆ¶ä¸‹çš„å®¹é‡ä½™é‡ï¼Œå†è®¡ç®—staleness é™åˆ¶ä¸‹çš„å®¹é‡ä½™é‡ï¼Œä¸¤è€…å–æœ€å°å€¼ï¼Œå°±æ˜¯çœŸæ­£çš„å®¹é‡ä½™é‡ã€‚

<details>
<summary>get_capacity() ä»£ç </summary>

```python
def get_capacity(self, current_version: int) -> int:

    with self.lock:
        # Calculate concurrency-based capacity
        max_concurrent_rollouts = max(1, self.max_concurrent_rollouts)
        concurrency_capacity = max_concurrent_rollouts - self.rollout_stat.running

        # Calculate staleness-based capacity
        ofp = self.max_staleness
        sample_cnt = self.rollout_stat.accepted + self.rollout_stat.running
        consumer_bs = max(1, self.consumer_batch_size)
        staleness_capacity = (ofp + current_version + 1) * consumer_bs - sample_cnt

        # Return the minimum of both constraints
        capacity = min(concurrency_capacity, staleness_capacity)
        return capacity

```
</details>


### DistRolloutCoordinator

`DistRolloutCoordinator`å®ç°äº†ä¸€ä¸ªåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ•°æ®åè°ƒå™¨ï¼ˆCoordinatorï¼‰å®ƒçš„æ ¸å¿ƒä½œç”¨æ˜¯ï¼Œåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œç¡®ä¿æ¯ä¸ª Data Parallel Rank (GPU) æ‹¿åˆ°çš„ Token æ€»æ•°å°½å¯èƒ½ä¸€è‡´ï¼Œè§£å†³ç”Ÿæˆçš„åºåˆ—é•¿åº¦ä¸ä¸€å¯¼è‡´çš„è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ã€‚å¦‚æœç›´æ¥å¹³å‡åˆ†é…æ•°æ®ï¼Œæœ‰çš„ GPU å¯èƒ½åˆ†åˆ°å¾ˆå¤šé•¿æ–‡æœ¬ï¼ˆè®¡ç®—é‡å¤§ï¼‰ï¼Œæœ‰çš„åˆ†åˆ°çŸ­æ–‡æœ¬ï¼Œå¯¼è‡´è®­ç»ƒæ—¶å‡ºç°ç­‰å¾…ï¼ˆStraggler problemï¼‰ã€‚

è€Œ AReal é€šè¿‡ FFDï¼ˆFirst-Fit Decreasingï¼‰ç®—æ³•é‡æ–°åˆ†é…æ•°æ®ï¼Œå®ç° Token çº§åˆ«çš„è´Ÿè½½å‡è¡¡ã€‚

<aside>
ğŸ’¡

FFDï¼ˆFirst-Fit Decreasingï¼‰ç®—æ³•ï¼š

FFD æ˜¯ä¸€ç§ç»å…¸çš„**è´ªå¿ƒç®—æ³•**ï¼Œå¸¸ç”¨äºè§£å†³è£…ç®±é—®é¢˜æˆ–å¤šæœºè°ƒåº¦é—®é¢˜ã€‚å…¶æ ¸å¿ƒç­–ç•¥æ˜¯â€œé™åºæ’åˆ—ï¼Œä¼˜å…ˆå¡«å‘â€œï¼š

1. **Decreasing (é™åº)**ï¼šé¦–å…ˆå°†æ‰€æœ‰ä»»åŠ¡ï¼ˆSequenceï¼‰æŒ‰ç…§è®¡ç®—é‡ï¼ˆToken é•¿åº¦ï¼‰ä»å¤§åˆ°å°æ’åºã€‚
2. **First-Fit (é¦–æ¬¡é€‚åº”)**ï¼šä¾æ¬¡å–å‡ºä»»åŠ¡ï¼Œå°†å…¶åˆ†é…ç»™å½“å‰è´Ÿè½½æœ€å°ï¼ˆæˆ–å‰©ä½™ç©ºé—´æœ€å¤§ï¼‰çš„èŠ‚ç‚¹ã€‚

é€šè¿‡è¿™ç§â€œå…ˆå¤„ç†å¤§çŸ³å—ï¼Œå†ç”¨æ²™å­å¡«ç¼éš™â€çš„æ–¹å¼ï¼ŒFFD èƒ½æœ€å¤§ç¨‹åº¦åœ°ç¡®ä¿æ‰€æœ‰ GPU çš„æ€»è®¡ç®—é‡è¶‹äºä¸€è‡´ï¼Œæ¶ˆé™¤æœ¨æ¡¶æ•ˆåº”ã€‚

</aside>

è¯¥åŠŸèƒ½çš„æ ¸å¿ƒå‡½æ•°ä¸º`redistribute()` ï¼Œä½äº `areal/core/dist_rollout.py` ã€‚è¯¥å‡½æ•°é¦–å…ˆé€šè¿‡`all_gather_tensor_container` å°†æ‰€æœ‰ GPU ä¸Šç”Ÿæˆçš„æ•°æ®æ”¶é›†èµ·æ¥ï¼Œè®©æ¯ä¸ª rank éƒ½æ‹¥æœ‰å…¨å±€çš„å…¨é‡æ•°æ®ã€‚ç„¶åæŠŠå¤§ batch ä¸­çš„æ•°æ®æŒ‰ç…§ granularityï¼ˆé€šå¸¸ä¸º GRPO ä¸­åŒä¸€ä¸ª prompt äº§ç”Ÿæ•°æ®çš„æ•°é‡ï¼‰åˆ‡åˆ†ï¼Œç¡®å®šæœ€å°å•å…ƒã€‚åˆ‡åˆ†å®Œä¹‹åï¼Œæ‹¿åˆ°æ¯ä¸ªsequence çš„ token æ•°ï¼Œä¸º FFD ç®—æ³•ä½œå‡†å¤‡ã€‚

æ¥ä¸‹æ¥æŠŠ sequence ä¸­çš„ padding éƒ½å»æ‰ï¼Œå°† sequence åœ¨è¿›ç¨‹ç»„å†…é€šè¿‡ FFD å‡åŒ€åˆ†é…å·¥ä½œé‡ï¼Œæœ€åè¿”å›ç»“æœã€‚

```python
return RedistributedData(
    all_data=all_data,            # åŸå§‹åˆ‡åˆ†åçš„æ‰€æœ‰å°å—ï¼ˆå¤‡ä»½ç”¨ï¼‰
    data=data,                    # å½“å‰ GPU æœ€ç»ˆè¦ç”¨çš„æ‹¼å¥½çš„æ•°æ®
    rank=dist.get_rank(group=group),
    group_indices=group_indices,  # å…¨å±€çš„åˆ†é…æ–¹æ¡ˆ
    )
```

<details>
<summary>redistribute() ä»£ç </summary>

```python
def redistribute(
    data: dict[str, Any], granularity: int = 1, group=None
) -> RedistributedData:
    """Redistribute a batch across a process group.

    This function only accepts padded data which must have an "attention_mask" field,
    Each tensor should have shape [bs, seqlen, *] or [bs].

    This function will divide the global batch into segments each with consecutive
    `granularity` sequences, and then redistribute the segments (e.g., for GRPO).
    """
    all_gathered = all_gather_tensor_container(data, group=group)

    all_data = []
    for d in all_gathered:
        bs = get_batch_size(d)
        assert bs % granularity == 0
        all_data += [
            _slice_tensor_dict(d, i, i + granularity) for i in range(0, bs, granularity)
        ]

    seqlens = [d["attention_mask"].sum().item() for d in all_data]

    # Remove pad positions
    for d in all_data:
        max_sequence_length = d["attention_mask"].sum(-1).max().item()
        attn_mask_shape = d["attention_mask"].shape
        for k, v in d.items():
            if (
                torch.is_tensor(v)
                and len(v.shape) >= 2
                and v.shape[:2] == attn_mask_shape[:2]
            ):
                d[k] = v[:, :max_sequence_length]

    # No capacity limit leads to balanced partition across this group
    group_indices = ffd_allocate(
        seqlens, capacity=int(1e12), min_groups=dist.get_world_size(group)
    )
    local_indices = group_indices[dist.get_rank(group=group)]

    data = concat_padded_tensors([all_data[i] for i in local_indices])
    return RedistributedData(
        all_data=all_data,
        data=data,
        rank=dist.get_rank(group=group),
        group_indices=group_indices,
    )

```
</details>

é™¤æ­¤ä¹‹å¤–ï¼Œ`DistRolloutCoordinator` ç±»ä¸­æœ€æ ¸å¿ƒçš„é€»è¾‘å…¶å®æ˜¯åˆ†å¸ƒå¼é€šä¿¡æ¨¡å¼ï¼Œå®ç°äº†ä¸€ä¸ªéå¸¸ç»å…¸çš„Â Head-Worker åŒæ­¥æ¨¡å¼ï¼Œè¿™ç‚¹åœ¨ `prepare_batch` å‡½æ•°ä»¥åŠå®ƒè°ƒç”¨çš„ `_broadcast_and_redistribute_batch` ä¸­å¾—ä»¥ä½“ç°ã€‚åœ¨`prepare_batch` ä¸­ï¼Œå‡½æ•°ä¼šåˆ¤æ–­è¯¥ rank æ˜¯å¦ä¸º headï¼Œåªæœ‰ head rank æ‰èƒ½è°ƒç”¨ `prepare_batch` è¿›è¡Œç”Ÿæˆã€‚

ç”Ÿæˆåçš„æ•°æ®é¦–å…ˆä¼šè¢«ç§»åŠ¨åˆ°å½“å‰è®¡ç®—è®¾å¤‡ï¼ˆGPUï¼‰ä¸Šï¼Œéšåè¿›å…¥æ ¸å¿ƒçš„é€šä¿¡ç®¡é“ `_broadcast_and_redistribute_batch`ã€‚

åœ¨è¿™ä¸ªç®¡é“ä¸­ï¼ŒæŒæœ‰æ•°æ®çš„ Head èŠ‚ç‚¹é¦–å…ˆä¼šåœ¨æ•°æ®å¹¶è¡Œç»„å†…è°ƒç”¨ `redistribute`ï¼Œåˆ©ç”¨å‰è¿°çš„ FFD ç®—æ³•å¯¹ Batch è¿›è¡Œåˆ‡åˆ†å’Œé‡æ–°æ‰“åŒ…ï¼Œå®Œæˆè´Ÿè½½å‡è¡¡çš„è®¡ç®—ã€‚

æ¥ç€ï¼Œä»£ç è®¾ç½®äº†ä¸¥æ ¼çš„åŒæ­¥å±éšœ (`dist.barrier`)ã€‚æ‰€æœ‰ Worker èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬é Head èŠ‚ç‚¹ï¼‰éƒ½ä¼šåœ¨æ­¤ç­‰å¾…ï¼Œç›´åˆ° Head èŠ‚ç‚¹å®Œæˆä¸Šè¿°çš„æ•°æ®å‡†å¤‡ä¸é‡åˆ†é…å·¥ä½œï¼Œç¡®ä¿é›†ç¾¤çŠ¶æ€çš„ä¸€è‡´æ€§ã€‚

åŒæ­¥è§£é™¤åï¼Œè¿›å…¥å¹¿æ’­é˜¶æ®µã€‚Head èŠ‚ç‚¹ä½œä¸ºæºå¤´ (`src_rank`)ï¼Œé€šè¿‡ `broadcast_tensor_container` å°†å¤„ç†å®Œæ¯•ã€è´Ÿè½½å‡è¡¡åçš„æ•°æ®åˆ†å‘ç»™ `context_and_model_parallel_group` ä¸­çš„æ‰€æœ‰ Workerã€‚æœ€åå†æ¬¡è¿›è¡ŒåŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰ rank éƒ½å®Œæ•´æ”¶åˆ°äº†æ•°æ®ã€‚è¿™ç§è®¾è®¡æ—¢é¿å…äº†æ‰€æœ‰èŠ‚ç‚¹é‡å¤è¿›è¡Œ Rollout ç”Ÿæˆçš„ç®—åŠ›æµªè´¹ï¼Œåˆä¿è¯äº†æ•°æ®èƒ½å¤Ÿé«˜æ•ˆã€å‡è¡¡åœ°åŒæ­¥åˆ°æ•´ä¸ªåˆ†å¸ƒå¼é›†ç¾¤ä¸­ã€‚

<details>
<summary>prepare_batch() ä»£ç </summary>

```python
def prepare_batch(
        self,
        dataloader: StatefulDataLoader,
        workflow: RolloutWorkflow | type[RolloutWorkflow] | str,
        granularity: int = 1,
        workflow_kwargs: dict[str, Any] | None = None,
        should_accept_fn: Callable[[dict[str, Any]], bool] | str | None = None,
    ) -> dict[str, Any]:

        batch = None
        if self.train_engine.is_data_parallel_head():
            batch = self.rollout_engine.prepare_batch(
                dataloader,
                workflow=workflow,
                workflow_kwargs=workflow_kwargs,
                should_accept_fn=should_accept_fn,
            )
            batch = tensor_container_to(batch, current_platform.current_device())

        return self._broadcast_and_redistribute_batch(batch, granularity=granularity)

```
</details>

<details>
<summary>_broadcast_and_redistribute_batch() ä»£ç </summary>

```python
def _broadcast_and_redistribute_batch(
        self,
        batch: dict[str, Any] | None,
        granularity: int = 1,
    ) -> dict[str, Any]:
        
        if batch is not None:
            redist = redistribute(
                batch,
                granularity=granularity,
                group=self.train_engine.data_parallel_group,
            )
            batch = redist.data

        current_platform.synchronize()
        dist.barrier(group=self.train_engine.cpu_group)

        batch = broadcast_tensor_container(
            batch,
            src_rank=self.train_engine.current_data_parallel_head(),
            group=self.train_engine.context_and_model_parallel_group,
            )

        current_platform.synchronize()
        dist.barrier(group=self.train_engine.cpu_group)

        return batch
```
</details>


## Training

Actor å¾—åˆ° trajectories åï¼Œè¿›å…¥æ ¸å¿ƒè®­ç»ƒé€»è¾‘ï¼š

- Micro-Batchingï¼ˆåˆ‡åˆ†ï¼‰: ä¸ºäº†å¤„ç†è¶…å¤§ Batch (å¦‚ GRPO éœ€è¦çš„å¤§é‡é‡‡æ ·)ï¼ŒActor é¦–å…ˆè°ƒç”¨Â `split_padded_tensor_dict_into_mb_list`Â (è§Â `areal/utils/data.py`ï¼Œåœ¨Â `Actor.train_batch`Â ä¸­ä½¿ç”¨) æ ¹æ® Token æ•°é‡ï¼Œä½¿ç”¨ Dynamic Batching å°†å¤§ Batch åˆ‡åˆ†ä¸ºå¤šä¸ªÂ Micro-Batch ï¼Œæ–¹æ³•çš„è¯¦ç»†åˆ†æå¯ä»¥è§åç»­ Dynamic Batching éƒ¨åˆ†ã€‚
- Gradient Accumulationï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰: é’ˆå¯¹æ¯ä¸ª Micro-Batchï¼ŒActor è°ƒç”¨åº•å±‚çš„Â `TrainEngine`ï¼ˆFSDP / Megatronï¼‰çš„Â `train_batch`Â æ¥å£ (`areal/engine/fsdp_engine.py`)ï¼š
    - æ‰§è¡Œ Forward è®¡ç®—ï¼›
    - è®¡ç®—Â log probsã€KLÂ diverge ä»¥åŠÂ advantageï¼›
    - æ‰§è¡Œ Backward å¹¶ç´¯ç§¯æ¢¯åº¦ï¼›
- Parameter Updateï¼ˆå‚æ•°æ›´æ–°ï¼‰: æ‰€æœ‰ Micro-Batch å¤„ç†å®Œæ¯•åï¼Œæ‰§è¡ŒÂ `optimizer.step()`Â æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

### Micro-Batching

Micro-Batching çš„é€»è¾‘ä¸»è¦åˆ†æ•£åœ¨ `areal/utils/data.py` ä¸­çš„ `split_padded_tensor_dict_into_mb_list` å‡½æ•°ã€‚è¿™é‡Œå°†ä¸€ä¸ªå¤§ Batch åˆ‡åˆ†æˆå¤šä¸ªå° Micro-Batchï¼ŒæŠŠä¸åŒé•¿åº¦çš„ Sequence åˆ†ç»„ï¼Œä½¿å¾—æ¯ä¸ª Micro-Batch çš„æ€» Token æ•°ä¸è¶…è¿‡ `max_tokens_per_mb`ï¼Œä»¥æ­¤æ¥é¿å… OOM (Out of Memory)ã€‚è¿™é‡Œçš„é€»è¾‘ä¸ `DistRolloutCoordinator` é€šè¿‡ FFD ç®—æ³•å®ç°çš„ coordinator ç›¸åŒï¼Œä¸å†åšè¯¦ç»†åˆ†æã€‚

<details>
<summary>æ ¸å¿ƒä»£ç å®ç°</summary>

```python
def split_padded_tensor_dict_into_mb_list(
    data: dict[str, Any],
    mb_spec: MicroBatchSpec,
    group: dist.ProcessGroup | None = None,
) -> MicroBatchList:
    # 1. é•¿åº¦ç»Ÿè®¡ï¼šåŸºäº attention_mask è®¡ç®—æ¯æ¡æ•°æ®çš„æœ‰æ•ˆé•¿åº¦
    seq_lens = data["attention_mask"].sum(1).long().cpu().numpy().tolist()
    
    # 2. åˆ†ç»„èšåˆï¼šè‹¥å­˜åœ¨ granularityï¼ˆå¦‚ GRPOï¼‰ï¼Œå…ˆèšåˆç»„å†… Token æ€»æ•°
    granularity = mb_spec.granularity
    bs = data["attention_mask"].shape[0]
    input_lens = (
        data["attention_mask"]
        .view(bs // granularity, granularity, -1)
        .sum(dim=(1, 2))
        .long()
        .cpu()
        .numpy()
    )

    # 3. è´Ÿè½½å‡è¡¡åˆ†é…ï¼šä½¿ç”¨ FFD (First-Fit Decreasing) ç®—æ³•è¿›è¡Œè£…ç®±è§„åˆ’
    # ä½¿æ¯ä¸ª Micro-batch çš„ Token æ€»æ•°ä¸è¶…è¿‡ max_tokens_per_mb
    group_indices = allocate_balanced_mbs_synced(mb_spec, input_lens, group=group)
    
    # 4. ç´¢å¼•æ˜ å°„ï¼šå°†åˆ†é…ç»“æœæ˜ å°„å›åŸå§‹æ•°æ®ç´¢å¼•
    group_indices = [
        datapack.flat2d(
            [list(range(i * granularity, (i + 1) * granularity)) for i in group_index]
        )
        for group_index in group_indices
    ]

    # 5. ç‰©ç†é‡æ’ä¸åˆ‡åˆ†ï¼šæ ¹æ®è®¡ç®—å¥½çš„ç´¢å¼•é‡ç»„ Tensor
    def _split(tensor):
        unpacked = [tensor[i] for i in range(bs)]
        reordered = reorder_list(unpacked, forward_indices)
        reordered = torch.stack(reordered)
        
        splitted = []
        offset = 0
        for _n_seqs in group_n_seqs:
            splitted.append(reordered[offset : offset + _n_seqs])
            offset += _n_seqs
        return splitted

    to_split = dict_map(to_split, lambda x: _split(x))

    return MicroBatchList(...)
```
</details>


## Update weights

Training è¿‡åï¼Œrollout ç«¯éœ€è¦è¿›è¡Œç›¸åº”çš„æ›´æ–°ã€‚ è¿™ä¸€è¿‡ç¨‹ç”± `WeightUpdateMeta` é…ç½®æ§åˆ¶ï¼Œå®šä¹‰äº† Actorï¼ˆå‘èµ·ç«¯ï¼‰å¦‚ä½•å°†å‚æ•°ä¼ è¾“ç»™ Rollout Engineï¼ˆæ¥æ”¶ç«¯ï¼‰

1. é…ç½®é©±åŠ¨ï¼š
    - `WeightUpdateMeta`Â å®šä¹‰äº†ä¼ è¾“æ–¹å¼ï¼ˆåŸºäº nccl çš„ `_update_weights_from_distributed` æˆ–è€…åŸºäºç›´æ¥å†™å…¥æ–‡ä»¶çš„ `_update_weights_from_disk`ï¼‰å’Œç›®æ ‡åœ°å€ã€‚Actor ä¾æ®æ­¤å…ƒæ•°æ®å†³å®šèµ°å“ªæ¡æ›´æ–°è·¯å¾„ã€‚
2. Actor å‘èµ·æ›´æ–°ï¼š
    - Actor è°ƒç”¨Â `update_weights`ï¼ˆè§ Â `fsdp_engine.py`ï¼‰ã€‚
    - è‹¥ä¸ºÂ ncclÂ æ¨¡å¼ï¼Œæ‰§è¡ŒÂ `_update_weights_from_distributed`ï¼Œé€šè¿‡ NCCL å¹¿æ’­å‚æ•°ï¼›è‹¥ä¸ºÂ diskÂ æ¨¡å¼ï¼Œæ‰§è¡ŒÂ `_update_weights_from_disk`ï¼Œä¿å­˜ Checkpointã€‚
3. RolloutÂ Engine å“åº”ï¼š
    - Actor è°ƒç”¨Â rollout_engineÂ çš„å¯¹åº”æ¥å£ï¼ˆå¦‚Â `update_weights_from_distributed`ï¼Œè§Â sglang_remote.pyï¼‰ã€‚
    - Rollout Engine å‘è¿œç¨‹ SGLang Server å‘é€æºå¸¦ Meta ä¿¡æ¯çš„ HTTP è¯·æ±‚ï¼ŒæŒ‡ç¤º Server æ‰§è¡Œæƒé‡åŒæ­¥æˆ–åŠ è½½ã€‚

## **Ray Placement Group & Launch**

ray ç›¸å…³çš„ä»£ç é›†ä¸­åœ¨ `areal/launcher/ray.py` ä¸­ï¼Œå…¶ä¸­æœ€é‡è¦çš„ç±»æ˜¯ `RayLauncher`ã€‚

### AllocationMode

`AllocationMode`Â æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªÂ é…ç½®ç±»ï¼ˆConfiguration Classï¼‰å’Œè§£æå™¨ï¼ˆParserï¼‰çš„å°è£…ã€‚

- å®ƒåŒ…å«äº†Â `_LLMParallelParser`ï¼Œè´Ÿè´£å°†ç”¨æˆ·è¾“å…¥çš„ DSL å­—ç¬¦ä¸²ï¼ˆå¦‚Â `"sglang:d2+fsdp:d4"`ï¼‰è§£ææˆç»“æ„åŒ–çš„æ•°æ®ã€‚
- å®ƒè¿˜å­˜å‚¨äº†è§£æåçš„ç»“æœï¼ˆModelAllocationÂ å¯¹è±¡åˆ—è¡¨ï¼‰ã€‚ç¨‹åºåç»­æ‰€æœ‰å…³äºâ€œæˆ‘æœ‰å‡ ä¸ªæ¨¡å‹â€ã€â€œæ¯ä¸ªæ¨¡å‹ç”¨å‡ å¼ å¡â€ã€â€œè°å’Œè°å…±ç”¨èµ„æºâ€çš„æŸ¥è¯¢ï¼Œéƒ½é€šè¿‡ç›´æ¥è®¿é—®Â AllocationModeÂ å®ä¾‹çš„å±æ€§æ¥è·å–çš„ã€‚

å¯¹äº colocate å’Œ disaggregateï¼ŒAReal é€šè¿‡ä»¥ä¸‹å‚æ•°æ§åˆ¶ ï¼š

```python
# disaggregation / separationï¼ˆå„éƒ¨åˆ†ä½¿ç”¨â€œ+â€è¿æ¥ï¼‰
allocation_mode="sglang[rollout]:d2+fsdp[actor]:d4+fsdp[critic]:d2"
```

è¿™é‡Œæ’å¸ƒçš„æ„æ€æ˜¯ï¼Œ Rollout: 2 GPUs (SGLangï¼Œ2DP)ï¼ŒActor:Â 4 GPUs (FSDPï¼Œ4DP)ï¼ŒCritic: 2 GPUs (FSDPï¼Œ4DP)ï¼Œæ€»å…±åŠ èµ·æ¥éœ€è¦ 8 GPUs (2+4+2)ï¼š

```python
# Colocation ï¼ˆcolocationéƒ¨åˆ†ä½¿ç”¨â€œ|â€è¿æ¥ï¼‰
allocation_mode="sglang[rollout]:d4+fsdp[actor]:d4|fsdp[critic]:d4"
```

è¿™é‡Œæ’å¸ƒçš„æ„æ€æ˜¯ï¼Œ **Rollout**: 4 GPUs (SGLangï¼Œ4DP)ï¼Œ **Actor & Critic**: å…±ç”¨å¦å¤– 4 GPUs (FSDPï¼Œ4DP)ï¼Œæ€»å…±åŠ èµ·æ¥éœ€è¦ 8 GPUs (4+4)

### RayLauncher / ray_main()

ray_main() æ˜¯ AReaL æ•´ä¸ª RL æµç¨‹çš„ entrypointï¼Œå®ƒçš„æ ¸å¿ƒèŒè´£æ˜¯ï¼šæ ¹æ®Â AllocationModeÂ çš„å†…çš„å‚æ•°è®¾ç½®ï¼Œç”³è¯· Ray èµ„æºï¼Œå¹¶æŒ‰é¡ºåºå¯åŠ¨ Rollout Engine å’Œ Training Actorã€‚ä»¥ä¸‹ä»£ç å±•ç¤ºäº† Launcher å¯åŠ¨ SGLang æ¨ç†æœåŠ¡çš„è¿‡ç¨‹ã€‚ä»ä»£ç ä¸Šå¯ä»¥å¾ˆæ¸…æ¥šçš„çœ‹åˆ°ï¼Œé¦–å…ˆä» `AllocationMode` ä¸­æå– Rollout ä¾§çš„å¹¶è¡Œåº¦å‚æ•°ï¼ˆå¦‚ dp_size, world_sizeï¼‰ï¼Œæ®æ­¤è®¡ç®—å‡ºå¯¹åº”çš„ Ray Placement Group è§„æ ¼ï¼Œæœ€åé€šè¿‡ `launcher.submit_array()` å°†å…·ä½“çš„å¯åŠ¨æŒ‡ä»¤æäº¤ç»™ Ray é›†ç¾¤æ‰§è¡Œã€‚

Trainer çš„ launcher ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚

<details>
<summary>Launch SGLang ä»£ç </summary>

```python
if allocation_mode.gen_backend == "sglang":
    # Launcher should launch SGLang servers according to allocation mode.
    config.sglang = to_structured_cfg(config.sglang, SGLangConfig)
    n_sglang_servers = allocation_mode.gen.dp_size
    n_sglang_nodes = max(1, allocation_mode.gen.world_size // n_gpus_per_node)
    node_group_size = max(1, allocation_mode.gen_instance_size // n_gpus_per_node)
    n_servers_per_node = max(n_sglang_servers // n_sglang_nodes, 1)
    cross_nodes = allocation_mode.gen_instance_size > n_gpus_per_node

    base_seed = config.sglang.random_seed
    sglang_args_list = [
        [
            sys.argv[1:]
            + [f"sglang.random_seed={base_seed + i * n_servers_per_node}"]
        ]
        for i in range(n_sglang_nodes)
    ]
    sglang_entry_point = str(
        pathlib.Path(__file__).resolve().parent.joinpath("sglang_server.py")
    )

    def sglang_env_hook(
        n_tasks: int, task_group_size: int, placement_group: PlacementGroup
    ) -> list[dict]:
        master_addrs = []
        master_ports = []
        for i in range(0, n_tasks, task_group_size):
            host_ip, port = get_placement_group_master_ip_and_port(
                placement_group, i
            )
            master_addrs.append(host_ip)
            master_ports.append(port)

        env_vars = []
        for i in range(n_tasks):
            env_vars.append(
                dict(
                    AREAL_SGLANG_MULTI_NODE_RANK=str(i % task_group_size),
                    AREAL_SGLANG_MULTI_NODE_MASTER_ADDR=master_addrs[
                        i // task_group_size
                    ],
                    AREAL_SGLANG_MULTI_NODE_MASTER_PORT=str(
                        master_ports[i // task_group_size]
                    ),
                )
            )

        return env_vars

    # launch a task to start all sglang servers in one node
    launcher.submit_array(
        job_name="llm_server",
        file_path=sglang_entry_point,
        func_name=DEFAULT_MAIN_FUNC_NAME,
        count=n_sglang_nodes,
        nodes=n_sglang_nodes,
        list_args=sglang_args_list,
        gpus_per_task=n_gpus_per_node,
        cpus_per_task=config.launcher.inference_server_cpus_per_gpu
        * n_gpus_per_node,
        mem_per_task=config.launcher.inference_server_mem_per_gpu * n_gpus_per_node,
        env_vars=get_env_vars(
            config.cluster.cluster_name,
            config.launcher.inference_server_env_vars,
        ),
        env_hook=(
            partial(sglang_env_hook, n_sglang_nodes, node_group_size)
            if cross_nodes
            else None
        ),
    )
    # Get SGLang server addresses via name_resolve
    try:
        sglang_addrs = wait_llm_server_addrs(
            config.experiment_name,
            config.trial_name,
            n_sglang_servers,
        )
    except (TimeoutError, KeyboardInterrupt) as e:
        launcher.stop_all(
            force=False
        )  # force=False will send KeyboardInterrupt to sglang_server.main() to further clean all sglang-related processes
        raise e
```
</details>


### Bundle & **Ray Placement Group**

åœ¨`RayLauncher.submit_array` ä¸­æœ‰å‡ºç° Areal ä¸­ bundles çš„å®šä¹‰ã€‚æ ¹æ®ä»£ç æ¥çœ‹ï¼ŒAReal çš„ bundle å¹¶ä¸å›ºå®šå¤§å°ï¼Œä¸å›ºå®šæ¯ä¸ª bundle å‡ ä¸ª GPU å‡ ä¸ª CPUï¼Œè€Œæ˜¯ç›´æ¥ä»¥nodeä¸ºå•ä½è¿›è¡Œåˆ†é…ã€‚

```python
tasks_per_node = count // nodes
gpus_per_node = gpus_per_task * tasks_per_node
cpus_per_node = cpus_per_task * tasks_per_node
mem_per_node = mem_per_task * tasks_per_node

device_bundles = [
    {
        "CPU": cpus_per_node,
        "GPU": gpus_per_node,
        "memory": mem_per_node * 1024 * 1024,  # Convert MB to bytes
    }
] * nodes

placement_group = ray.util.placement_group(
		bundles=device_bundles, strategy="PACK"
)
```

å…¸å‹çš„è°ƒç”¨æ–¹å¼ï¼ˆæ¯”å¦‚ Trainer é‚£è¾¹ï¼‰æ˜¯ï¼š

```python
count = trainer_n_nodes * n_gpus_per_node
nodes = trainer_n_nodes
gpus_per_task = 1

# å¸¦å…¥ä¸Šé¢çš„å…¬å¼ï¼Œå³
tasks_per_node = n_gpus_per_node
gpus_per_node = 1 * n_gpus_per_node = n_gpus_per_node
```

ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ª bundle éƒ½å¯¹åº”ä¸€ä¸ªnodeçš„æ‰€æœ‰ GPU + å¯¹åº”çš„ CPU/memoryã€‚
è¿™ç§ node çº§ bundle çš„ç²’åº¦ï¼Œä»æºå¤´ä¸Šé¿å…äº†èµ„æºç¢ç‰‡ï¼Œä¹Ÿæœ€å¤§ç¨‹åº¦ä¿è¯äº†åƒ FSDP / Megatron è¿™ç±»é€šä¿¡å¯†é›†å‹å¹¶è¡Œæ–¹æ¡ˆçš„æ‹“æ‰‘å¯æ§å’Œæ€§èƒ½ç¨³å®šã€‚
